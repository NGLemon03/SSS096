# -*- coding: utf-8 -*-
'''
æ¯”è¼ƒä¸‰ç¨®åˆ†ç¾¤æ–¹æ³•ï¼š
1. æ©Ÿå™¨å­¸ç¿’DTWåˆ†ç¾¤
2. æ‰‹å‹•DTWåˆ†ç¾¤ (3å€‹åˆ†ç¾¤)
3. ç´”åƒæ•¸+ç¸¾æ•ˆåˆ†ç¾¤ (ä¸ä½¿ç”¨DTW)
'''
import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def load_and_prepare_data():
    """è¼‰å…¥ä¸¦æº–å‚™æ•¸æ“š"""
    
    # è¼‰å…¥æœ€æ–°çš„çµæœæª”æ¡ˆä½œç‚ºåŸºæº–
    results_dir = Path("results")
    base_file = results_dir / "optuna_results_single_20250701_093224.csv"
    
    if not base_file.exists():
        print("æ‰¾ä¸åˆ°åŸºæº–æª”æ¡ˆï¼Œè«‹å…ˆé‹è¡Œ optuna_16.py")
        return None
    
    df = pd.read_csv(base_file)
    print(f"è¼‰å…¥ {len(df)} å€‹ç­–ç•¥é€²è¡Œåˆ†æ")
    return df

def method1_ml_dtw_clustering(df):
    """æ–¹æ³•1: æ©Ÿå™¨å­¸ç¿’DTWåˆ†ç¾¤ (ç¾æœ‰çµæœ)"""
    print("\n=== æ–¹æ³•1: æ©Ÿå™¨å­¸ç¿’DTWåˆ†ç¾¤ ===")
    
    # ä½¿ç”¨ç¾æœ‰çš„åˆ†ç¾¤çµæœ
    cluster_counts = df['dtw_cluster'].value_counts().sort_index()
    print(f"åˆ†ç¾¤æ•¸: {df['dtw_cluster'].nunique()}")
    print(f"å„åˆ†ç¾¤ç­–ç•¥æ•¸: {dict(cluster_counts)}")
    print(f"å¹³å‡æ¯ç¾¤ç­–ç•¥æ•¸: {len(df)/df['dtw_cluster'].nunique():.1f}")
    
    # åˆ†æåˆ†ç¾¤ç‰¹å¾µ
    cluster_stats = df.groupby('dtw_cluster').agg({
        'score': ['count', 'mean', 'std'],
        'total_return': ['mean', 'std'],
        'sharpe_ratio': ['mean', 'std'],
        'max_drawdown': ['mean', 'std'],
        'profit_factor': ['mean', 'std'],
        'avg_hold_days': ['mean', 'std']
    }).round(3)
    
    print("\nåˆ†ç¾¤çµ±è¨ˆ:")
    print(cluster_stats)
    
    return df['dtw_cluster'], cluster_stats

def method2_manual_dtw_clustering(df):
    """æ–¹æ³•2: æ‰‹å‹•DTWåˆ†ç¾¤ (3å€‹åˆ†ç¾¤)"""
    print("\n=== æ–¹æ³•2: æ‰‹å‹•DTWåˆ†ç¾¤ (3å€‹åˆ†ç¾¤) ===")
    
    # æ¨¡æ“¬æ‰‹å‹•åˆ†ç¾¤çµæœ (åŸºæ–¼åˆ†æ•¸åˆ†çµ„)
    scores = df['score'].values
    n_clusters = 3
    
    # ä½¿ç”¨åˆ†æ•¸åˆ†ä½æ•¸é€²è¡Œåˆ†ç¾¤
    quantiles = np.percentile(scores, [33, 66])
    manual_clusters = np.ones(len(df), dtype=int)
    manual_clusters[scores > quantiles[1]] = 3
    manual_clusters[(scores > quantiles[0]) & (scores <= quantiles[1])] = 2
    
    df_manual = df.copy()
    df_manual['manual_cluster'] = manual_clusters
    
    cluster_counts = df_manual['manual_cluster'].value_counts().sort_index()
    print(f"åˆ†ç¾¤æ•¸: {df_manual['manual_cluster'].nunique()}")
    print(f"å„åˆ†ç¾¤ç­–ç•¥æ•¸: {dict(cluster_counts)}")
    print(f"å¹³å‡æ¯ç¾¤ç­–ç•¥æ•¸: {len(df_manual)/df_manual['manual_cluster'].nunique():.1f}")
    
    # åˆ†æåˆ†ç¾¤ç‰¹å¾µ
    cluster_stats = df_manual.groupby('manual_cluster').agg({
        'score': ['count', 'mean', 'std'],
        'total_return': ['mean', 'std'],
        'sharpe_ratio': ['mean', 'std'],
        'max_drawdown': ['mean', 'std'],
        'profit_factor': ['mean', 'std'],
        'avg_hold_days': ['mean', 'std']
    }).round(3)
    
    print("\nåˆ†ç¾¤çµ±è¨ˆ:")
    print(cluster_stats)
    
    return manual_clusters, cluster_stats

def method3_param_performance_clustering(df):
    """æ–¹æ³•3: ç´”åƒæ•¸+ç¸¾æ•ˆåˆ†ç¾¤ (ä¸ä½¿ç”¨DTW)"""
    print("\n=== æ–¹æ³•3: ç´”åƒæ•¸+ç¸¾æ•ˆåˆ†ç¾¤ ===")
    
    # æº–å‚™ç‰¹å¾µæ•¸æ“š
    feature_data = []
    
    for _, row in df.iterrows():
        # ç¸¾æ•ˆç‰¹å¾µ
        performance_features = [
            row['total_return'],
            row['sharpe_ratio'],
            row['max_drawdown'],
            row['profit_factor'],
            row['avg_hold_days'],
            row['cpcv_oos_mean'],
            row['cpcv_oos_min']
        ]
        
        # åƒæ•¸ç‰¹å¾µ
        params = json.loads(row['parameters'])
        param_features = [
            params['linlen'],
            params['smaalen'], 
            params['devwin'],
            params['buy_mult'],
            params['sell_mult'],
            params['stop_loss']
        ]
        
        # åˆä½µç‰¹å¾µ
        all_features = performance_features + param_features
        feature_data.append(all_features)
    
    # è½‰æ›ç‚ºnumpyé™£åˆ—
    X = np.array(feature_data)
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ä½¿ç”¨PCAé™ç¶­
    pca = PCA(n_components=min(5, X_scaled.shape[1]))
    X_pca = pca.fit_transform(X_scaled)
    
    # KMeansåˆ†ç¾¤
    n_clusters = min(4, len(df)//5)  # ç¢ºä¿æ¯ç¾¤è‡³å°‘æœ‰5å€‹ç­–ç•¥
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    param_clusters = kmeans.fit_predict(X_pca)
    
    df_param = df.copy()
    df_param['param_cluster'] = param_clusters + 1  # å¾1é–‹å§‹ç·¨è™Ÿ
    
    cluster_counts = df_param['param_cluster'].value_counts().sort_index()
    print(f"åˆ†ç¾¤æ•¸: {df_param['param_cluster'].nunique()}")
    print(f"å„åˆ†ç¾¤ç­–ç•¥æ•¸: {dict(cluster_counts)}")
    print(f"å¹³å‡æ¯ç¾¤ç­–ç•¥æ•¸: {len(df_param)/df_param['param_cluster'].nunique():.1f}")
    
    # åˆ†æåˆ†ç¾¤ç‰¹å¾µ
    cluster_stats = df_param.groupby('param_cluster').agg({
        'score': ['count', 'mean', 'std'],
        'total_return': ['mean', 'std'],
        'sharpe_ratio': ['mean', 'std'],
        'max_drawdown': ['mean', 'std'],
        'profit_factor': ['mean', 'std'],
        'avg_hold_days': ['mean', 'std']
    }).round(3)
    
    print("\nåˆ†ç¾¤çµ±è¨ˆ:")
    print(cluster_stats)
    
    # åˆ†æåƒæ•¸åˆ†å¸ƒ
    print("\nåƒæ•¸åˆ†å¸ƒåˆ†æ:")
    for cluster in sorted(df_param['param_cluster'].unique()):
        cluster_data = df_param[df_param['param_cluster'] == cluster]
        print(f"\nåˆ†ç¾¤ {cluster} åƒæ•¸ç‰¹å¾µ:")
        
        # åˆ†æåƒæ•¸ç¯„åœ
        linlens = [json.loads(row['parameters'])['linlen'] for _, row in cluster_data.iterrows()]
        smaalens = [json.loads(row['parameters'])['smaalen'] for _, row in cluster_data.iterrows()]
        buy_mults = [json.loads(row['parameters'])['buy_mult'] for _, row in cluster_data.iterrows()]
        
        print(f"  linlen: {min(linlens)}-{max(linlens)} (å¹³å‡: {np.mean(linlens):.1f})")
        print(f"  smaalen: {min(smaalens)}-{max(smaalens)} (å¹³å‡: {np.mean(smaalens):.1f})")
        print(f"  buy_mult: {min(buy_mults)}-{max(buy_mults)} (å¹³å‡: {np.mean(buy_mults):.2f})")
    
    return param_clusters + 1, cluster_stats

def compare_methods(df):
    """æ¯”è¼ƒä¸‰ç¨®æ–¹æ³•"""
    print("="*60)
    print("ğŸ” ä¸‰ç¨®åˆ†ç¾¤æ–¹æ³•æ¯”è¼ƒ")
    print("="*60)
    
    # åŸ·è¡Œä¸‰ç¨®åˆ†ç¾¤æ–¹æ³•
    ml_clusters, ml_stats = method1_ml_dtw_clustering(df)
    manual_clusters, manual_stats = method2_manual_dtw_clustering(df)
    param_clusters, param_stats = method3_param_performance_clustering(df)
    
    # æ¯”è¼ƒåˆ†æ
    print("\n" + "="*60)
    print("ğŸ“Š æ–¹æ³•æ¯”è¼ƒç¸½çµ")
    print("="*60)
    
    methods = [
        ("æ©Ÿå™¨å­¸ç¿’DTW", ml_clusters, ml_stats),
        ("æ‰‹å‹•DTW(3ç¾¤)", manual_clusters, manual_stats),
        ("åƒæ•¸+ç¸¾æ•ˆ", param_clusters, param_stats)
    ]
    
    comparison_data = []
    
    for method_name, clusters, stats in methods:
        n_clusters = len(np.unique(clusters))
        avg_cluster_size = len(df) / n_clusters
        cluster_sizes = [np.sum(clusters == i) for i in np.unique(clusters)]
        size_range = f"{min(cluster_sizes)}-{max(cluster_sizes)}"
        
        # è¨ˆç®—åˆ†ç¾¤å…§è®Šç•°æ€§ (ä½¿ç”¨scoreçš„æ¨™æº–å·®)
        cluster_vars = []
        for i in np.unique(clusters):
            cluster_scores = df[clusters == i]['score']
            cluster_vars.append(cluster_scores.std())
        avg_variance = np.mean(cluster_vars)
        
        comparison_data.append({
            'æ–¹æ³•': method_name,
            'åˆ†ç¾¤æ•¸': n_clusters,
            'å¹³å‡æ¯ç¾¤ç­–ç•¥æ•¸': f"{avg_cluster_size:.1f}",
            'ç­–ç•¥æ•¸ç¯„åœ': size_range,
            'å¹³å‡åˆ†ç¾¤å…§è®Šç•°æ€§': f"{avg_variance:.2f}"
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))
    
    # è©•ä¼°å»ºè­°
    print("\n" + "="*60)
    print("ğŸ’¡ è©•ä¼°å»ºè­°")
    print("="*60)
    
    for method_name, clusters, stats in methods:
        n_clusters = len(np.unique(clusters))
        avg_cluster_size = len(df) / n_clusters
        
        print(f"\n{method_name}:")
        if avg_cluster_size < 10:
            print("  âš ï¸  æ¯ç¾¤ç­–ç•¥æ•¸éå°‘ï¼Œå¯èƒ½éåº¦åˆ†ç¾¤")
        elif avg_cluster_size > 30:
            print("  âš ï¸  æ¯ç¾¤ç­–ç•¥æ•¸éå¤šï¼Œç„¡æ³•æœ‰æ•ˆæ’é™¤éåº¦ç›¸ä¼¼")
        else:
            print("  âœ… åˆ†ç¾¤æ•¸é‡åˆç†")
        
        if n_clusters < 3:
            print("  âš ï¸  åˆ†ç¾¤æ•¸éå°‘ï¼Œå¯èƒ½ç„¡æ³•å……åˆ†å€åˆ†ä¸åŒç­–ç•¥")
        elif n_clusters > 10:
            print("  âš ï¸  åˆ†ç¾¤æ•¸éå¤šï¼Œç®¡ç†è¤‡é›œ")
        else:
            print("  âœ… åˆ†ç¾¤æ•¸é©ä¸­")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ¯”è¼ƒä¸‰ç¨®åˆ†ç¾¤æ–¹æ³•")
    
    # è¼‰å…¥æ•¸æ“š
    df = load_and_prepare_data()
    if df is None:
        return
    
    # åŸ·è¡Œæ¯”è¼ƒ
    compare_methods(df)
    
    print("\n" + "="*60)
    print("ğŸ¯ çµè«–")
    print("="*60)
    print("1. æ©Ÿå™¨å­¸ç¿’DTW: åŸºæ–¼æ¬Šç›Šæ›²ç·šè¡Œç‚ºåˆ†ç¾¤ï¼Œèƒ½è­˜åˆ¥ç›¸ä¼¼è¡¨ç¾æ¨¡å¼")
    print("2. æ‰‹å‹•DTW: ç°¡å–®ä½†å¯èƒ½ä¸å¤ ç²¾ç¢º")
    print("3. åƒæ•¸+ç¸¾æ•ˆ: åŸºæ–¼ç‰¹å¾µåˆ†ç¾¤ï¼Œå¯èƒ½æ›´ç¬¦åˆåƒæ•¸ç›¸ä¼¼æ€§")
    print("\nå»ºè­°æ ¹æ“šå¯¦éš›éœ€æ±‚é¸æ“‡åˆé©çš„æ–¹æ³•ï¼")

if __name__ == "__main__":
    main() 