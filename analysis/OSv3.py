import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import pandas as pd
import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import pearsonr
import ast
import shutil
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from analysis.data_loader import load_data
from SSSv096 import backtest_unified, param_presets, compute_single, compute_RMA, compute_ssma_turn_combined, compute_dual

# æ–°å¢ Ensemble ç­–ç•¥æ”¯æ´
try:
    from ensemble_wrapper import EnsembleStrategyWrapper
    ENSEMBLE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Šï¼šç„¡æ³•å°å…¥ ensemble_wrapperï¼ŒEnsemble ç­–ç•¥å°‡ä¸å¯ç”¨")
    ENSEMBLE_AVAILABLE = False
from analysis.config import RESULT_DIR, WF_PERIODS, STRESS_PERIODS, CACHE_DIR
from analysis.metrics import calculate_max_drawdown
from analysis.logging_config import setup_logging
import logging


# åˆå§‹åŒ–å¿«å–ç›®éŒ„
# ç§»é™¤è‡ªå‹•æ¸…ç†å¿«å–ç›®éŒ„ï¼Œé¿å…èˆ‡å…¶ä»–ç¨‹å¼è¡çª
# shutil.rmtree(CACHE_DIR, ignore_errors=True)
(CACHE_DIR / "price").mkdir(parents=True, exist_ok=True)
(CACHE_DIR / "smaa").mkdir(parents=True, exist_ok=True)
(CACHE_DIR / "factor").mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ–æ—¥èªŒ
setup_logging()
logger = logging.getLogger("OSv3")

# === èªªæ˜æ°£æ³¡å‡½æ•¸ ===
def get_column_tooltips():
    """
    è¿”å›å„æ¬„ä½çš„èªªæ˜æ°£æ³¡æ–‡å­—
    """
    tooltips = {
        # åŸºæœ¬ç¸¾æ•ˆæŒ‡æ¨™
        'total_return': 'ç¸½å ±é…¬ç‡ï¼šæ•´å€‹å›æ¸¬æœŸé–“çš„ç´¯ç©å ±é…¬ç‡',
        'annual_return': 'å¹´åŒ–å ±é…¬ç‡ï¼šå°‡ç¸½å ±é…¬ç‡è½‰æ›ç‚ºå¹´åŒ–æ¨™æº–',
        'sharpe_ratio': 'å¤æ™®å€¼ï¼šè¶…é¡å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬',
        'max_drawdown': 'æœ€å¤§å›æ’¤ï¼šæ¬Šç›Šæ›²ç·šå¾å³°å€¼åˆ°è°·å€¼çš„æœ€å¤§è·Œå¹…',
        'calmar_ratio': 'å¡ç‘ªå€¼ï¼šå¹´åŒ–å ±é…¬ç‡èˆ‡æœ€å¤§å›æ’¤çš„æ¯”å€¼',
        'num_trades': 'äº¤æ˜“æ¬¡æ•¸ï¼šæ•´å€‹å›æ¸¬æœŸé–“çš„ç¸½äº¤æ˜“æ¬¡æ•¸',
        'win_rate': 'å‹ç‡ï¼šç²åˆ©äº¤æ˜“æ¬¡æ•¸ä½”ç¸½äº¤æ˜“æ¬¡æ•¸çš„æ¯”ä¾‹',
        
        # éæ“¬åˆæª¢æ¸¬æŒ‡æ¨™
        'overfitting_score': 'éæ“¬åˆåˆ†æ•¸ï¼š0-100ï¼Œåˆ†æ•¸è¶Šé«˜è¡¨ç¤ºéæ“¬åˆé¢¨éšªè¶Šå¤§',
        'parameter_sensitivity': 'åƒæ•¸æ•æ„Ÿæ€§ï¼šç­–ç•¥å°åƒæ•¸è®ŠåŒ–çš„æ•æ„Ÿç¨‹åº¦ï¼Œè¶Šé«˜è¶Šå®¹æ˜“éæ“¬åˆ',
        'consistency_score': 'ä¸€è‡´æ€§åˆ†æ•¸ï¼šæ¨£æœ¬å…§å¤–è¡¨ç¾çš„ä¸€è‡´æ€§ï¼Œè¶Šé«˜è¶Šç©©å®š',
        'stability_score': 'ç©©å®šæ€§åˆ†æ•¸ï¼šç­–ç•¥åœ¨ä¸åŒæ™‚é–“æ®µçš„è¡¨ç¾ç©©å®šæ€§',
        'overfitting_risk': 'éæ“¬åˆé¢¨éšªï¼šç¶œåˆéæ“¬åˆé¢¨éšªè©•åˆ†ï¼Œ0-100',
        'pbo_score': 'PBOåˆ†æ•¸ï¼šProbability of Backtest Overfittingï¼Œå›æ¸¬éæ“¬åˆæ©Ÿç‡',
        
        # æ¨£æœ¬å…§å¤–æ¯”è¼ƒ
        'train_sharpe': 'æ¨£æœ¬å…§å¤æ™®å€¼ï¼šè¨“ç·´æœŸé–“çš„å¤æ™®å€¼',
        'test_sharpe': 'æ¨£æœ¬å¤–å¤æ™®å€¼ï¼šæ¸¬è©¦æœŸé–“çš„å¤æ™®å€¼',
        'sharpe_degradation': 'å¤æ™®å€¼é€€åŒ–ï¼šæ¨£æœ¬å…§å¤–å¤æ™®å€¼çš„å·®ç•°',
        'train_return': 'æ¨£æœ¬å…§å ±é…¬ç‡ï¼šè¨“ç·´æœŸé–“çš„å¹´åŒ–å ±é…¬ç‡',
        'test_return': 'æ¨£æœ¬å¤–å ±é…¬ç‡ï¼šæ¸¬è©¦æœŸé–“çš„å¹´åŒ–å ±é…¬ç‡',
        'return_degradation': 'å ±é…¬ç‡é€€åŒ–ï¼šæ¨£æœ¬å…§å¤–å ±é…¬ç‡çš„å·®ç•°',
        
        # ç©©å®šæ€§æŒ‡æ¨™
        'mean_return': 'å¹³å‡å ±é…¬ç‡ï¼šå„æœŸé–“çš„å¹³å‡å ±é…¬ç‡',
        'std_return': 'å ±é…¬ç‡æ¨™æº–å·®ï¼šå„æœŸé–“å ±é…¬ç‡çš„è®Šç•°ç¨‹åº¦',
        'cv': 'è®Šç•°ä¿‚æ•¸ï¼šæ¨™æº–å·®èˆ‡å¹³å‡å€¼çš„æ¯”å€¼ï¼Œè¡¡é‡ç›¸å°è®Šç•°æ€§',
        'positive_periods_ratio': 'æ­£å ±é…¬æœŸé–“æ¯”ä¾‹ï¼šç”¢ç”Ÿæ­£å ±é…¬çš„æœŸé–“ä½”ç¸½æœŸé–“çš„æ¯”ä¾‹',
        'rank_stability': 'æ’åç©©å®šæ€§ï¼šç­–ç•¥åœ¨ä¸åŒæœŸé–“çš„æ’åä¸€è‡´æ€§',
        
        # å…¶ä»–æŒ‡æ¨™
        'avg_hold_days': 'å¹³å‡æŒå€‰å¤©æ•¸ï¼šå¹³å‡æ¯æ¬¡äº¤æ˜“çš„æŒå€‰æ™‚é–“',
        'profit_factor': 'ç²åˆ©å› å­ï¼šç¸½ç²åˆ©èˆ‡ç¸½è™§æçš„æ¯”å€¼',
        'cpcv_oos_mean': 'CPCVæ¨£æœ¬å¤–å¹³å‡ï¼šäº¤å‰é©—è­‰æ¨£æœ¬å¤–çš„å¹³å‡å ±é…¬ç‡',
        'cpcv_oos_min': 'CPCVæ¨£æœ¬å¤–æœ€å°å€¼ï¼šäº¤å‰é©—è­‰æ¨£æœ¬å¤–çš„æœ€ä½å ±é…¬ç‡',
        'sharpe_var': 'å¤æ™®å€¼è®Šç•°æ•¸ï¼šæ»¾å‹•å¤æ™®å€¼çš„è®Šç•°ç¨‹åº¦',
        
        # é¢¨éšªåˆ†å¸ƒ
        'low_risk': 'ä½é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšªâ‰¤30çš„ç­–ç•¥æ¯”ä¾‹',
        'medium_risk': 'ä¸­é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª31-60çš„ç­–ç•¥æ¯”ä¾‹',
        'high_risk': 'é«˜é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª>60çš„ç­–ç•¥æ¯”ä¾‹',
        'risk_return_correlation': 'é¢¨éšªå ±é…¬ç›¸é—œæ€§ï¼šéæ“¬åˆé¢¨éšªèˆ‡ç¸½å ±é…¬ç‡çš„ç›¸é—œä¿‚æ•¸'
    }
    return tooltips

def create_tooltip_text(column_name):
    """
    ç‚ºæŒ‡å®šæ¬„ä½å‰µå»ºèªªæ˜æ°£æ³¡æ–‡å­—
    """
    tooltips = get_column_tooltips()
    return tooltips.get(column_name, f"æ¬„ä½ï¼š{column_name}")

def display_dataframe_with_tooltips(df, title="", key=""):
    """
    é¡¯ç¤ºå¸¶æœ‰èªªæ˜æ°£æ³¡çš„DataFrame
    """
    if df.empty:
        st.warning("æ²’æœ‰æ•¸æ“šå¯é¡¯ç¤º")
        return
    
    # é¡¯ç¤ºæ¨™é¡Œ
    if title:
        st.subheader(title)
    
    # å‰µå»ºèªªæ˜æ–‡å­—
    tooltip_text = ""
    for col in df.columns:
        if col in get_column_tooltips():
            tooltip_text += f"**{col}**: {get_column_tooltips()[col]}\n\n"
    
    # é¡¯ç¤ºèªªæ˜æ°£æ³¡
    if tooltip_text:
        with st.expander("ğŸ“– æ¬„ä½èªªæ˜", expanded=False):
            st.markdown(tooltip_text)
    
    # é¡¯ç¤ºDataFrame
    st.dataframe(df, use_container_width=True, key=key)

# === åƒæ•¸ç›¸é—œæ€§åˆ†æèˆ‡å¤šæ¨£æ€§éæ¿¾å‡½æ•¸ï¼ˆç§»åˆ°æª”æ¡ˆå‰é¢ï¼‰ ===
def compute_param_correlations(optuna_results_df, strategy, data_source):
    """
    è¨ˆç®—æŒ‡å®šç­–ç•¥å’Œæ•¸æ“šæºçš„åƒæ•¸èˆ‡ç¸¾æ•ˆæŒ‡æ¨™çš„ç›¸é—œæ€§
    
    Args:
        optuna_results_df: OptunaçµæœDataFrame
        strategy: ç­–ç•¥åç¨±
        data_source: æ•¸æ“šæºåç¨±
    
    Returns:
        pd.DataFrame: åƒæ•¸ç›¸é—œæ€§çŸ©é™£
    """
    # ç¯©é¸æŒ‡å®šç­–ç•¥å’Œæ•¸æ“šæºçš„è©¦é©—
    mask = (optuna_results_df['strategy'] == strategy) & (optuna_results_df['data_source'] == data_source)
    strategy_trials = optuna_results_df[mask].copy()
    
    if len(strategy_trials) < 10:
        logger.warning(f"ç­–ç•¥ {strategy} æ•¸æ“šæº {data_source} çš„è©¦é©—æ•¸é‡ä¸è¶³ ({len(strategy_trials)})ï¼Œè·³éç›¸é—œæ€§åˆ†æ")
        return pd.DataFrame()
    
    # è§£æåƒæ•¸
    strategy_trials['parameters'] = strategy_trials['parameters'].apply(
        lambda x: ast.literal_eval(x) if isinstance(x, str) else x
    )
    
    # æå–åƒæ•¸åˆ°ç¨ç«‹æ¬„ä½
    param_keys = set()
    for params in strategy_trials['parameters']:
        if isinstance(params, dict):
            param_keys.update(params.keys())
    
    for param in param_keys:
        strategy_trials[f'param_{param}'] = strategy_trials['parameters'].apply(
            lambda x: x.get(param, np.nan) if isinstance(x, dict) else np.nan
        )
    
    # è¨ˆç®—ç›¸é—œæ€§
    correlations = {}
    metric_keys = ["total_return", "sharpe_ratio"]
    
    for param in param_keys:
        param_col = f'param_{param}'
        if param_col not in strategy_trials.columns:
            continue
            
        correlations[param] = {}
        param_values = pd.to_numeric(strategy_trials[param_col], errors='coerce')
        
        for metric in metric_keys:
            if metric not in strategy_trials.columns:
                continue
                
            metric_values = pd.to_numeric(strategy_trials[metric], errors='coerce')
            
            # ç¢ºä¿æ˜¯pandas Seriesä¸¦ç§»é™¤NaNå€¼
            if not isinstance(param_values, pd.Series):
                param_values = pd.Series(param_values)
            if not isinstance(metric_values, pd.Series):
                metric_values = pd.Series(metric_values)
            
            # ç§»é™¤NaNå€¼
            valid_mask = param_values.notna() & metric_values.notna()
            if valid_mask.sum() < 3:
                correlations[param][metric] = np.nan
                continue
            
            valid_params = param_values[valid_mask]
            valid_metrics = metric_values[valid_mask]
            
            try:
                corr, p_value = pearsonr(valid_params, valid_metrics)
                correlations[param][metric] = corr
            except:
                correlations[param][metric] = np.nan
    
    return pd.DataFrame(correlations).T

def pick_topN_by_diversity(trials, metric_keys, top_n=5):
    """
    åŸºæ–¼ fine_grained_cluster çš„å¤šæ¨£æ€§ top N è©¦é©—é¸æ“‡ï¼Œæ”¯æ´æ¬„ä½è‡ªå‹•é©æ‡‰
    
    Args:
        trials: è©¦é©—åˆ—è¡¨ï¼Œæ¯å€‹è©¦é©—åŒ…å« score å’Œ fine_grained_cluster
        metric_keys: ç”¨æ–¼åˆ†çµ„çš„æŒ‡æ¨™éµï¼ˆæ”¯æ´ num_tradesã€excess_return_stressã€avg_hold_daysï¼‰
        top_n: æœ€çµ‚é¸å–çš„è©¦é©—æ•¸é‡
    
    Returns:
        List: ç¯©é¸å¾Œçš„è©¦é©—åˆ—è¡¨
    """
    logger.info(f"é–‹å§‹å¤šæ¨£æ€§ç¯©é¸: {len(trials)} å€‹è©¦é©—, ç›®æ¨™é¸å– {top_n} å€‹")
    
    # è½‰æ›ç‚º DataFrame
    df = pd.DataFrame(trials)
    
    # æª¢æŸ¥ DataFrame æ˜¯å¦ç‚ºç©º
    if df.empty:
        logger.error("è©¦é©—æ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•é€²è¡Œç¯©é¸")
        return []
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if 'score' not in df.columns:
        logger.error("ç¼ºå°‘å¿…è¦æ¬„ä½ 'score'ï¼Œç„¡æ³•é€²è¡Œç¯©é¸")
        return []
    
    # å„ªå…ˆä½¿ç”¨ fine_grained_cluster é€²è¡Œåˆ†çµ„
    if 'fine_grained_cluster' in df.columns:
        logger.info("ä½¿ç”¨ fine_grained_cluster é€²è¡Œåˆ†çµ„")
        
        # æª¢æŸ¥ fine_grained_cluster æ¬„ä½æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
        valid_clusters = df['fine_grained_cluster'].notna() & (df['fine_grained_cluster'] >= 0)
        if valid_clusters.sum() > 0:
            # ä½¿ç”¨ fine_grained_cluster é€²è¡Œåˆ†çµ„
            df['group'] = df['fine_grained_cluster'].astype(str)
            logger.info(f"fine_grained_cluster åˆ†çµ„: å…± {df['fine_grained_cluster'].nunique()} å€‹ç¾¤çµ„")
            
            # çµ±è¨ˆå„ç¾¤çµ„çš„è©¦é©—æ•¸é‡
            cluster_counts = df['fine_grained_cluster'].value_counts().sort_index()
            logger.info("å„ç¾¤çµ„è©¦é©—æ•¸é‡:")
            for cluster_id, count in cluster_counts.items():
                logger.info(f"  ç¾¤çµ„ {cluster_id}: {count} å€‹è©¦é©—")
        else:
            logger.warning("fine_grained_cluster æ¬„ä½ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œä½¿ç”¨å‚™ç”¨åˆ†çµ„æ–¹æ³•")
            df['group'] = 'default'
    else:
        logger.warning("ç¼ºå°‘ fine_grained_cluster æ¬„ä½ï¼Œä½¿ç”¨å‚™ç”¨åˆ†çµ„æ–¹æ³•")
        
        # å®šç¾©é—œéµæŒ‡æ¨™ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
        key_metrics = ['num_trades', 'excess_return_stress', 'avg_hold_days']
        logger.info(f"æœŸæœ›çš„é—œéµæŒ‡æ¨™: {key_metrics}")
        
        # æª¢æŸ¥å“ªäº›æŒ‡æ¨™å¯¦éš›å­˜åœ¨
        available_metrics = []
        for metric in key_metrics:
            if metric in df.columns:
                # æª¢æŸ¥æ¬„ä½æ˜¯å¦æœ‰æœ‰æ•ˆæ•¸æ“š
                valid_data = df[metric].notna() & (df[metric] != np.inf) & (df[metric] != -np.inf)
                if valid_data.sum() > 0:
                    available_metrics.append(metric)
                    logger.info(f"âœ“ æŒ‡æ¨™ {metric} å¯ç”¨ï¼Œæœ‰æ•ˆæ•¸æ“š: {valid_data.sum()}/{len(df)}")
                else:
                    logger.warning(f"âœ— æŒ‡æ¨™ {metric} å­˜åœ¨ä½†ç„¡æœ‰æ•ˆæ•¸æ“š")
            else:
                logger.warning(f"âœ— æŒ‡æ¨™ {metric} ä¸å­˜åœ¨")
        
        # å¦‚æœæ²’æœ‰å¯ç”¨çš„é—œéµæŒ‡æ¨™ï¼Œä½¿ç”¨åŸºæœ¬ç¯©é¸
        if not available_metrics:
            logger.warning("æ²’æœ‰å¯ç”¨çš„é—œéµæŒ‡æ¨™ï¼Œä½¿ç”¨åŸºæœ¬åˆ†æ•¸æ’åºç¯©é¸")
            df_sorted = df.sort_values(by='score', ascending=False)
            chosen_trials = df_sorted.head(top_n).to_dict('records')
            logger.info(f"åŸºæœ¬ç¯©é¸å®Œæˆï¼Œé¸å– {len(chosen_trials)} å€‹è©¦é©—")
            return chosen_trials
        
        logger.info(f"ä½¿ç”¨å¯ç”¨æŒ‡æ¨™: {available_metrics}")
        
        # é‡å°ä¸åŒæŒ‡æ¨™ç‰¹æ€§çš„åˆ†çµ„è™•ç†
        for metric in available_metrics:
            if metric not in df.columns:
                continue
                
            # è™•ç† NaN å€¼
            df[metric] = df[metric].fillna(df[metric].median())
                
            if metric == 'num_trades':
                # num_trades: åˆ†ç´šè™•ç†ï¼Œæ¯5æ¬¡ç‚ºä¸€çµ„ï¼Œé¿å…éæ–¼ç´°ç¢
                df[f'grouped_{metric}'] = (df[metric] // 5) * 5
                logger.info(f"æŒ‡æ¨™ {metric}: åŸå§‹å€¼ç¯„åœ [{df[metric].min()}, {df[metric].max()}], åˆ†ç´šå¾Œç¯„åœ [{df[f'grouped_{metric}'].min()}, {df[f'grouped_{metric}'].max()}]")
                
            elif metric == 'excess_return_stress':
                # excess_return_stress: å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œä¸€ä½
                df[f'grouped_{metric}'] = df[metric].round(1)
                logger.info(f"æŒ‡æ¨™ {metric}: åŸå§‹å€¼ç¯„åœ [{df[metric].min():.3f}, {df[metric].max():.3f}], å››æ¨äº”å…¥å¾Œç¯„åœ [{df[f'grouped_{metric}'].min():.1f}, {df[f'grouped_{metric}'].max():.1f}]")
                
            elif metric == 'avg_hold_days':
                # avg_hold_days: å››æ¨äº”å…¥åˆ°å°æ•¸é»å¾Œä¸€ä½
                df[f'grouped_{metric}'] = df[metric].round(1)
                logger.info(f"æŒ‡æ¨™ {metric}: åŸå§‹å€¼ç¯„åœ [{df[metric].min():.3f}, {df[metric].max():.3f}], å››æ¨äº”å…¥å¾Œç¯„åœ [{df[f'grouped_{metric}'].min():.1f}, {df[f'grouped_{metric}'].max():.1f}]")
        
        # æŒ‰åˆ†çµ„å¾Œçš„æŒ‡æ¨™å‰µå»ºçµ„åˆ¥æ¨™è­˜
        group_cols = [f'grouped_{metric}' for metric in available_metrics]
        df['group'] = df[group_cols].astype(str).agg('_'.join, axis=1)
    
    # çµ±è¨ˆåˆ†çµ„æƒ…æ³
    group_counts = df['group'].value_counts()
    logger.info(f"åˆ†çµ„çµ±è¨ˆ: å…± {len(group_counts)} å€‹ä¸åŒçµ„åˆ¥")
    
    # é¡¯ç¤ºå‰å¹¾å€‹çµ„åˆ¥çš„è©³ç´°ä¿¡æ¯
    for i, (group, count) in enumerate(group_counts.head(5).items()):
        group_str = str(group)[:50] if group else "ç©ºçµ„åˆ¥"
        logger.info(f"çµ„åˆ¥ {i+1}: {count} å€‹è©¦é©—, çµ„åˆ¥æ¨™è­˜: {group_str}...")
    
    # æŒ‰åˆ†æ•¸æ’åºä¸¦é¸æ“‡æ¯å€‹åˆ†çµ„ä¸­åˆ†æ•¸æœ€é«˜çš„è©¦é©—
    df_sorted = df.sort_values(by='score', ascending=False)
    chosen_trials = []
    seen_groups = set()
    
    logger.info("é–‹å§‹é¸å–è©¦é©—...")
    
    for idx, row in df_sorted.iterrows():
        group = row['group']
        trial_num = row.get('trial_number', f'trial_{idx}')
        score = row['score']
        
        if group not in seen_groups:
            chosen_trials.append(row.to_dict())
            seen_groups.add(group)
            logger.info(f"é¸å–è©¦é©— {trial_num}: score={score:.3f}, çµ„åˆ¥={group[:50]}...")
        else:
            logger.debug(f"è·³éè©¦é©— {trial_num}: score={score:.3f}, çµ„åˆ¥å·²å­˜åœ¨")
        
        if len(chosen_trials) >= top_n:
            logger.info(f"å·²é¸å– {len(chosen_trials)} å€‹è©¦é©—ï¼Œé”åˆ°ç›®æ¨™æ•¸é‡")
            break
    
    if len(chosen_trials) < top_n:
        logger.warning(f"åªé¸å–äº† {len(chosen_trials)} å€‹è©¦é©—ï¼Œå°‘æ–¼ç›®æ¨™ {top_n} å€‹")
        # å¦‚æœåˆ†çµ„ç¯©é¸ä¸è¶³ï¼Œè£œå……é«˜åˆ†è©¦é©—
        remaining_needed = top_n - len(chosen_trials)
        remaining_trials = df_sorted[~df_sorted.index.isin([t.get('index', i) for i, t in enumerate(chosen_trials)])]
        if len(remaining_trials) > 0:
            # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è½‰æ› DataFrame åˆ°å­—å…¸åˆ—è¡¨
            additional_trials = []
            for _, row in remaining_trials.head(remaining_needed).iterrows():
                additional_trials.append(row.to_dict())
            chosen_trials.extend(additional_trials)
            logger.info(f"è£œå…… {len(additional_trials)} å€‹é«˜åˆ†è©¦é©—")
    
    # é¡¯ç¤ºæœ€çµ‚é¸å–çš„è©¦é©—ä¿¡æ¯
    logger.info("æœ€çµ‚é¸å–çš„è©¦é©—:")
    for i, trial in enumerate(chosen_trials):
        trial_num = trial.get('trial_number', f'trial_{i}')
        score = trial.get('score', -np.inf)
        logger.info(f"  {i+1}. è©¦é©— {trial_num}: score={score:.3f}")
        # é¡¯ç¤ºé—œéµæŒ‡æ¨™ï¼ˆå®‰å…¨è™•ç†ï¼‰
        if 'fine_grained_cluster' in df.columns:
            cluster_id = trial.get('fine_grained_cluster', 'N/A')
            logger.info(f"     ç¾¤çµ„: {cluster_id}")
        else:
            # åªæœ‰åœ¨ä½¿ç”¨å‚™ç”¨åˆ†çµ„æ–¹æ³•æ™‚æ‰é¡¯ç¤ºæŒ‡æ¨™
            try:
                key_metrics_values = {k: trial.get(k, 'N/A') for k in available_metrics}
                logger.info(f"     é—œéµæŒ‡æ¨™: {key_metrics_values}")
            except NameError:
                logger.info("     ä½¿ç”¨ fine_grained_cluster åˆ†çµ„")
    
    return chosen_trials

# Streamlit UI é…ç½®
st.set_page_config(layout="wide", page_title="00631L ç­–ç•¥å›æ¸¬èˆ‡èµ°æŸ¥åˆ†æ")

# è‡ªè¨‚ multiselect æ¨™ç±¤é¡è‰²ç‚ºæ°´è—è‰²
st.markdown(
    '''
    <style>
    .stMultiSelect [data-baseweb="tag"] {
        background-color: #00BFFF !important;
        color: #222 !important;
    }
    </style>
    ''',
    unsafe_allow_html=True
)

st.title("00631L ç­–ç•¥åˆ†æèˆ‡å›æ¸¬")

# å‹•æ…‹ç”Ÿæˆèµ°æŸ¥å€é–“
def generate_walk_forward_periods(data_index, n_splits, min_days=30):
    """æ ¹æ“šæ•¸æ“šæ—¥æœŸç¯„åœç”Ÿæˆå¹³åˆ†çš„èµ°æŸ¥å€é–“"""
    if data_index.empty:
        st.error("æ•¸æ“šç´¢å¼•ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆèµ°æŸ¥å€é–“")
        logger.error("æ•¸æ“šç´¢å¼•ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆèµ°æŸ¥å€é–“")
        return []
    start_date = data_index.min()
    end_date = data_index.max()
    total_days = (end_date - start_date).days
    if total_days < min_days:
        st.error(f"æ•¸æ“šç¯„åœéçŸ­ï¼ˆ{total_days} å¤©ï¼‰ï¼Œç„¡æ³•ç”Ÿæˆèµ°æŸ¥å€é–“")
        logger.error(f"æ•¸æ“šç¯„åœéçŸ­ï¼ˆ{total_days} å¤©ï¼‰ï¼Œç„¡æ³•ç”Ÿæˆèµ°æŸ¥å€é–“")
        return []
    if total_days < min_days * n_splits:
        n_splits = max(1, total_days // min_days)
        logger.warning(f"æ•¸æ“šç¯„åœéçŸ­ï¼Œèª¿æ•´åˆ†æ®µæ•¸ç‚º {n_splits}")
    days_per_split = total_days // n_splits
    periods = []
    current_start = start_date
    for i in range(n_splits):
        if i == n_splits - 1:
            current_end = end_date
        else:
            current_end = current_start + pd.Timedelta(days=days_per_split - 1)
            end_candidates = data_index[data_index <= current_end]
            if end_candidates.empty:
                break
            current_end = end_candidates[-1]
        if (current_end - current_start).days >= min_days:
            periods.append((current_start, current_end))
        current_start = current_end + pd.Timedelta(days=1)
        start_candidates = data_index[data_index >= current_start]
        if start_candidates.empty:
            break
        current_start = start_candidates[0]
    return periods

# è¨ˆç®— PR å€¼
def calculate_pr_values(series, is_mdd, is_initial_period, hedge_mask):
    """
    å ±é…¬ç‡é‡åŒ–åˆ†æ•¸ï¼ˆRelative Strength Scoreï¼‰æˆ– MDD é‡åŒ–åˆ†æ•¸ï¼ˆRisk-Adjusted MDD Scoreï¼‰
    :param series: è¦è¨ˆç®—åˆ†æ•¸çš„ç³»åˆ—ï¼ˆå ±é…¬ç‡æˆ– MDDï¼‰
    :param is_mdd: æ˜¯å¦ç‚º MDDï¼ˆTrueï¼‰æˆ–å ±é…¬ç‡ï¼ˆFalseï¼‰
    :param is_initial_period: æ˜¯å¦ç‚ºåˆå§‹æ™‚æ®µ
    :param hedge_mask: é¿éšªæ©ç¢¼ï¼ŒTrue è¡¨ç¤ºè©²ç­–ç•¥åœ¨è©²æ™‚æ®µé¿éšª
    :return: åˆ†æ•¸ç³»åˆ—
    """
    score = pd.Series(index=series.index, dtype=float)
    if is_initial_period:
        # åˆå§‹æ™‚æ®µï¼šæ’é™¤é¿éšªç­–ç•¥
        score[hedge_mask] = np.nan
        valid = ~hedge_mask
        valid_series = series[valid]
        if is_mdd:
            # MDD ä¿®æ­£ï¼šMDDè¶Šå°ï¼ˆè¶Šæ¥è¿‘0ï¼‰åˆ†æ•¸è¶Šé«˜
            # MDDé€šå¸¸æ˜¯è² å€¼ï¼Œæˆ‘å€‘å¸Œæœ›å®ƒè¶Šæ¥è¿‘0è¶Šå¥½
            benchmark = abs(valid_series.mean())
            if benchmark == 0 or np.isnan(benchmark):
                score[valid] = 0
            else:
                # ä¿®æ­£å…¬å¼ï¼šMDDè¶Šå°åˆ†æ•¸è¶Šé«˜
                # ä½¿ç”¨ (1 + valid_series/benchmark) è€Œä¸æ˜¯ (1 - valid_series/benchmark)
                # å› ç‚ºvalid_seriesæ˜¯è² å€¼ï¼Œæ‰€ä»¥åŠ è™Ÿæœƒè®“MDDè¶Šå°åˆ†æ•¸è¶Šé«˜
                score[valid] = 100 * (1 + valid_series / benchmark)
        else:
            # å ±é…¬ç‡ï¼šå ±é…¬ç‡è¶Šé«˜åˆ†æ•¸è¶Šé«˜
            benchmark = valid_series.mean()
            std = valid_series.std()
            if std == 0 or np.isnan(std):
                score[valid] = 0
            else:
                score[valid] = 100 * (valid_series - benchmark) / std
    else:
        # éåˆå§‹æ™‚æ®µï¼šé¿éšªç­–ç•¥çµ¦ 100 åˆ†
        score[hedge_mask] = 100
        valid = ~hedge_mask
        valid_series = series[valid]
        if is_mdd:
            # MDD ä¿®æ­£ï¼šMDDè¶Šå°ï¼ˆè¶Šæ¥è¿‘0ï¼‰åˆ†æ•¸è¶Šé«˜
            benchmark = abs(valid_series.mean())
            if benchmark == 0 or np.isnan(benchmark):
                score[valid] = 0
            else:
                # ä¿®æ­£å…¬å¼ï¼šMDDè¶Šå°åˆ†æ•¸è¶Šé«˜
                score[valid] = 100 * (1 + valid_series / benchmark)
        else:
            # å ±é…¬ç‡ï¼šå ±é…¬ç‡è¶Šé«˜åˆ†æ•¸è¶Šé«˜
            benchmark = valid_series.mean()
            std = valid_series.std()
            if std == 0 or np.isnan(std):
                score[valid] = 0
            else:
                score[valid] = 100 * (valid_series - benchmark) / std
    return score

# åœ¨æ–‡ä»¶é–‹é ­æ·»åŠ æ–°çš„éæ“¬åˆæª¢æ¸¬å‡½æ•¸
def calculate_overfitting_metrics(train_returns, test_returns, strategy_name):
    """
    è¨ˆç®—éæ“¬åˆæŒ‡æ¨™ - å¢å¼·ç‰ˆ
    """
    if len(train_returns) == 0 or len(test_returns) == 0:
        return {}
    
    # è¨ˆç®—æ¨£æœ¬å…§å¤–è¡¨ç¾å·®ç•°
    train_sharpe = train_returns.mean() / train_returns.std() if train_returns.std() > 0 else 0
    test_sharpe = test_returns.mean() / test_returns.std() if test_returns.std() > 0 else 0
    
    # æ¨£æœ¬å…§å¤–å¤æ™®å€¼å·®ç•°
    sharpe_degradation = train_sharpe - test_sharpe
    
    # æ¨£æœ¬å…§å¤–å ±é…¬ç‡å·®ç•°
    return_degradation = train_returns.mean() - test_returns.mean()
    
    # ç©©å®šæ€§æŒ‡æ¨™ï¼ˆè®Šç•°ä¿‚æ•¸ï¼‰
    train_cv = train_returns.std() / abs(train_returns.mean()) if train_returns.mean() != 0 else float('inf')
    test_cv = test_returns.std() / abs(test_returns.mean()) if test_returns.mean() != 0 else float('inf')
    
    # éæ“¬åˆåˆ†æ•¸ï¼ˆ0-100ï¼Œè¶Šé«˜è¶Šéæ“¬åˆï¼‰
    # ä¿®æ­£è¨ˆç®—æ–¹å¼ï¼šè€ƒæ…®å¤æ™®å€¼å’Œå ±é…¬ç‡çš„ç›¸å°é‡è¦æ€§
    sharpe_weight = 0.6
    return_weight = 0.4
    overfitting_score = min(100, max(0, 
        abs(sharpe_degradation) * 50 * sharpe_weight + 
        abs(return_degradation) * 200 * return_weight
    ))
    
    return {
        'train_sharpe': train_sharpe,
        'test_sharpe': test_sharpe,
        'train_return': train_returns.mean(),
        'test_return': test_returns.mean(),
        'sharpe_degradation': sharpe_degradation,
        'return_degradation': return_degradation,
        'overfitting_score': overfitting_score
    }

def calculate_enhanced_overfitting_analysis(optuna_results_df, strategy_name, data_source_name):
    """
    è¨ˆç®—å¢å¼·çš„éæ“¬åˆåˆ†æ - åŸºæ–¼Optunaçµæœä¸­çš„æ–°æŒ‡æ¨™
    """
    # ç¯©é¸æŒ‡å®šç­–ç•¥å’Œæ•¸æ“šæºçš„è©¦é©—
    mask = (optuna_results_df['strategy'] == strategy_name) & (optuna_results_df['data_source'] == data_source_name)
    strategy_trials = optuna_results_df[mask].copy()
    
    if len(strategy_trials) < 10:
        return {}
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„éæ“¬åˆæŒ‡æ¨™
    new_metrics = ['parameter_sensitivity', 'consistency_score', 'stability_score', 'overfitting_risk']
    available_metrics = [metric for metric in new_metrics if metric in strategy_trials.columns]
    
    if not available_metrics:
        return {}
    
    # è¨ˆç®—çµ±è¨ˆæ‘˜è¦
    analysis_results = {}
    
    for metric in available_metrics:
        values = pd.to_numeric(strategy_trials[metric], errors='coerce')
        # ä½¿ç”¨pandasæ–¹æ³•è™•ç†NaNå€¼
        valid_values = values.dropna()
        
        if len(valid_values) > 0:
            analysis_results[f'{metric}_mean'] = np.mean(valid_values)
            analysis_results[f'{metric}_std'] = np.std(valid_values)
            analysis_results[f'{metric}_min'] = np.min(valid_values)
            analysis_results[f'{metric}_max'] = np.max(valid_values)
            analysis_results[f'{metric}_median'] = np.median(valid_values)
            
            # è¨ˆç®—é¢¨éšªç­‰ç´š
            if metric == 'overfitting_risk':
                low_risk = np.sum(valid_values <= 30) / len(valid_values)
                medium_risk = np.sum((valid_values > 30) & (valid_values <= 60)) / len(valid_values)
                high_risk = np.sum(valid_values > 60) / len(valid_values)
                analysis_results[f'{metric}_risk_distribution'] = {
                    'low_risk': low_risk,
                    'medium_risk': medium_risk,
                    'high_risk': high_risk
                }
    
    # è¨ˆç®—ç›¸é—œæ€§åˆ†æ
    if 'overfitting_risk' in available_metrics and 'total_return' in strategy_trials.columns:
        risk_values = pd.to_numeric(strategy_trials['overfitting_risk'], errors='coerce')
        return_values = pd.to_numeric(strategy_trials['total_return'], errors='coerce')
        
        valid_mask = risk_values.notna() & return_values.notna()
        if valid_mask.sum() > 5:
            correlation = risk_values[valid_mask].corr(return_values[valid_mask])
            analysis_results['risk_return_correlation'] = correlation
    
    return analysis_results

def calculate_strategy_stability(period_returns_dict):
    """
    è¨ˆç®—ç­–ç•¥ç©©å®šæ€§æŒ‡æ¨™
    """
    if not period_returns_dict:
        return {}
    
    # ç¢ºä¿æ‰€æœ‰ç­–ç•¥çš„æœŸé–“å ±é…¬ç‡éƒ½æœ‰ç›¸åŒçš„ç´¢å¼•
    all_periods = set()
    for returns in period_returns_dict.values():
        all_periods.update(returns.index)
    
    # å‰µå»ºçµ±ä¸€çš„DataFrame
    returns_df = pd.DataFrame(index=sorted(all_periods))
    for name, returns in period_returns_dict.items():
        returns_df[name] = returns
    
    stability_metrics = {}
    
    for col in returns_df.columns:
        returns = returns_df[col].dropna()
        if len(returns) < 2:
            continue
            
        # è¨ˆç®—å„æœŸé–“è¡¨ç¾çš„ä¸€è‡´æ€§ï¼ˆæ³¨æ„ï¼šperiod_returnså·²ç¶“æ˜¯ç™¾åˆ†æ¯”ï¼‰
        mean_return = returns.mean() / 100  # è½‰æ›ç‚ºå°æ•¸
        std_return = returns.std() / 100    # è½‰æ›ç‚ºå°æ•¸
        
        # ä¿®æ­£è®Šç•°ä¿‚æ•¸è¨ˆç®—ï¼Œé¿å…ç„¡é™å€¼
        if abs(mean_return) > 1e-10:  # é¿å…é™¤ä»¥æ¥è¿‘é›¶çš„å€¼
            cv = std_return / abs(mean_return)
        else:
            cv = float('inf') if std_return > 0 else 0.0
        
        # è¨ˆç®—æ­£å ±é…¬æœŸé–“æ¯”ä¾‹
        positive_periods = (returns > 0).sum() / len(returns)
        
        # è¨ˆç®—è¡¨ç¾æ’åç©©å®šæ€§ï¼ˆå¦‚æœæœ‰å¤šå€‹ç­–ç•¥ï¼‰
        if len(returns_df.columns) > 1:
            # è¨ˆç®—è©²ç­–ç•¥åœ¨ä¸åŒæœŸé–“çš„æ’åç©©å®šæ€§
            # å°æ¯å€‹æœŸé–“è¨ˆç®—è©²ç­–ç•¥çš„æ’åï¼Œç„¶å¾Œè¨ˆç®—æ’åçš„ä¸€è‡´æ€§
            strategy_rankings = []
            
            # å°æ¯å€‹æœŸé–“è¨ˆç®—è©²ç­–ç•¥çš„æ’å
            for period in returns_df.index:
                period_returns = returns_df.loc[period].dropna()
                if len(period_returns) > 1 and col in period_returns.index:
                    # è¨ˆç®—æ’åï¼ˆ1ç‚ºæœ€å¥½ï¼Œnç‚ºæœ€å·®ï¼‰
                    rankings = period_returns.rank(ascending=False)
                    # ç²å–è©²ç­–ç•¥çš„æ’å
                    strategy_rank = rankings[col]
                    strategy_rankings.append(strategy_rank)
            
            if len(strategy_rankings) > 1:
                # è¨ˆç®—è©²ç­–ç•¥æ’åçš„è®Šç•°ä¿‚æ•¸ï¼ˆæ¨™æº–å·®/å¹³å‡å€¼ï¼‰
                strategy_rankings_series = pd.Series(strategy_rankings)
                rank_mean = strategy_rankings_series.mean()
                rank_std = strategy_rankings_series.std()
                
                if rank_mean > 0:
                    # æ’åç©©å®šæ€§ = 1 - (æ’åæ¨™æº–å·® / æ’åå¹³å‡å€¼)
                    # é€™æ¨£æ’åè¶Šç©©å®šï¼Œå€¼è¶Šæ¥è¿‘1
                    rank_stability = max(0, 1 - (rank_std / rank_mean))
                else:
                    rank_stability = 1.0
                
                logger.info(f"ç­–ç•¥ {col} æ’åç©©å®šæ€§è¨ˆç®—: {len(strategy_rankings)} å€‹æœŸé–“, å¹³å‡æ’å: {rank_mean:.2f}, æ’åæ¨™æº–å·®: {rank_std:.2f}, ç©©å®šæ€§: {rank_stability:.3f}")
            else:
                rank_stability = 1.0
                logger.info(f"ç­–ç•¥ {col} æ’åç©©å®šæ€§: åªæœ‰ä¸€å€‹æœŸé–“ï¼Œè¨­ç‚º 1.0")
        else:
            rank_stability = 1.0
            logger.info(f"ç­–ç•¥ {col} æ’åç©©å®šæ€§: åªæœ‰ä¸€å€‹ç­–ç•¥ï¼Œè¨­ç‚º 1.0")
            
        stability_metrics[col] = {
            'mean_return': mean_return,
            'std_return': std_return,
            'cv': cv,
            'positive_periods_ratio': positive_periods,
            'rank_stability': rank_stability
        }
    
    return stability_metrics

def calculate_risk_adjusted_metrics(equity_curve, strategy_name):
    """
    è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œçš„å ±é…¬ç‡æŒ‡æ¨™
    """
    if equity_curve.empty or len(equity_curve) < 2:
        return {}
    
    # è¨ˆç®—æ—¥å ±é…¬ç‡
    daily_returns = equity_curve.pct_change().dropna()
    
    # åŸºæœ¬æŒ‡æ¨™
    total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0] - 1)
    annual_return = total_return * (252 / len(daily_returns))
    volatility = daily_returns.std() * np.sqrt(252)
    sharpe_ratio = annual_return / volatility if volatility > 0 else 0
    
    # æœ€å¤§å›æ’¤
    max_drawdown = calculate_max_drawdown(equity_curve)
    calmar_ratio = annual_return / max_drawdown if max_drawdown > 0 else 0
    
    # ç´¢æè«¾æ¯”ç‡ï¼ˆåªè€ƒæ…®ä¸‹è¡Œé¢¨éšªï¼‰
    downside_returns = daily_returns[daily_returns < 0]
    downside_deviation = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0
    sortino_ratio = annual_return / downside_deviation if downside_deviation > 0 else 0
    
    # å¡ç‘ªå€¼ï¼ˆå¹´åŒ–å ±é…¬ç‡/æœ€å¤§å›æ’¤ï¼‰
    kama_ratio = annual_return / max_drawdown if max_drawdown > 0 else 0
    
    # ä¸€è‡´æ€§è©•åˆ†ï¼ˆæ­£å ±é…¬æœˆä»½æ¯”ä¾‹ï¼‰
    monthly_returns = equity_curve.resample('M').last().pct_change().dropna()
    positive_months = (monthly_returns > 0).sum() / len(monthly_returns)
    
    # ç¶œåˆè©•åˆ†ï¼ˆçµåˆå ±é…¬ç‡å’Œé¢¨éšªï¼‰
    composite_score = (sharpe_ratio * 0.3 + sortino_ratio * 0.3 + positive_months * 0.2 + (1 - max_drawdown) * 0.2)
    
    return {
        'total_return': total_return,
        'annual_return': annual_return,
        'volatility': volatility,
        'sharpe_ratio': sharpe_ratio,
        'sortino_ratio': sortino_ratio,
        'calmar_ratio': calmar_ratio,
        'kama_ratio': kama_ratio,
        'max_drawdown': max_drawdown,
        'positive_months_ratio': positive_months,
        'composite_score': composite_score
    }


def run_ensemble_strategy(params, ticker="00631L.TW"):
    """é‹è¡Œ Ensemble ç­–ç•¥"""
    if not ENSEMBLE_AVAILABLE:
        st.error("Ensemble ç­–ç•¥ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ ensemble_wrapper æ˜¯å¦æ­£ç¢ºå®‰è£")
        # è¿”å›ç©ºç»“æœé¿å…UIè¯¯æŠ¥
        empty_equity = pd.Series(1.0, index=pd.date_range('2020-01-01', periods=100, freq='D'))
        empty_trades = pd.DataFrame(columns=['date', 'action', 'weight', 'price'])
        empty_stats = {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_ratio': 0.0, 'num_trades': 0}
        return empty_equity, empty_trades, empty_stats
    
    try:
        # å‰µå»º Ensemble ç­–ç•¥åŒ…è£å™¨
        wrapper = EnsembleStrategyWrapper()
        
        # é‹è¡Œç­–ç•¥
        equity_curve, trades, stats, method_name = wrapper.ensemble_strategy(
            method=params['method'],
            params=params,
            ticker=ticker
        )
        
        # è¨ˆç®—é¡å¤–çš„æŒ‡æ¨™
        additional_metrics = calculate_risk_adjusted_metrics(equity_curve, method_name)
        
        # åˆä½µæŒ‡æ¨™
        combined_stats = {**stats, **additional_metrics}
        
        return equity_curve, trades, combined_stats
        
    except Exception as e:
        st.error(f"é‹è¡Œ Ensemble ç­–ç•¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"Ensemble ç­–ç•¥éŒ¯èª¤: {e}")
        # è¿”å›ç©ºç»“æœé¿å…UIè¯¯æŠ¥
        empty_equity = pd.Series(1.0, index=pd.date_range('2020-01-01', periods=100, freq='D'))
        empty_trades = pd.DataFrame(columns=['date', 'action', 'weight', 'price'])
        empty_stats = {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_ratio': 0.0, 'num_trades': 0}
        return empty_equity, empty_trades, empty_stats


def get_ensemble_strategy_info():
    """ç²å– Ensemble ç­–ç•¥ä¿¡æ¯"""
    if not ENSEMBLE_AVAILABLE:
        return None
    
    try:
        wrapper = EnsembleStrategyWrapper()
        return wrapper.get_strategy_info()
    except Exception as e:
        logger.error(f"ç²å– Ensemble ç­–ç•¥ä¿¡æ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# è§£ææ–‡ä»¶åä»¥æå–ç­–ç•¥å’Œæ•¸æ“šæºä¿¡æ¯
def parse_optuna_filename(filename):
    """è§£æoptunaçµæœæ–‡ä»¶åï¼Œæå–ç­–ç•¥å’Œæ•¸æ“šæºä¿¡æ¯"""
    name = Path(filename).stem  # ç§»é™¤.csvå¾Œç¶´
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯fine-grained processedæ–‡ä»¶
    if name.endswith('_fine_grained_processed'):
        # æ–°æ ¼å¼ï¼š{strategy}_{data_source}_fine_grained_processed
        name = name.replace('_fine_grained_processed', '')
        
        # æŸ¥æ‰¾ç­–ç•¥åç¨±
        strategy = None
        if name.startswith('ssma_turn_'):
            strategy = 'ssma_turn'
            name = name.replace('ssma_turn_', '')
        elif name.startswith('single_'):
            strategy = 'single'
            name = name.replace('single_', '')
        elif name.startswith('dual_'):
            strategy = 'dual'
            name = name.replace('dual_', '')
        elif name.startswith('RMA_'):
            strategy = 'RMA'
            name = name.replace('RMA_', '')
        
        if not strategy:
            return None
        
        # æŸ¥æ‰¾æ•¸æ“šæº - æ”¯æ´æ–°çš„æ ¼å¼
        data_source = None
        
        # æ–°æ ¼å¼ï¼šFactor_TWII__2412.TW æˆ– Factor_TWII__2414.TW
        if name.startswith('Factor_TWII__2412.TW'):
            data_source = 'Factor (^TWII / 2412.TW)'
        elif name.startswith('Factor_TWII__2414.TW'):
            data_source = 'Factor (^TWII / 2414.TW)'
        elif name == 'Self':
            data_source = 'Self'
        
        if not data_source:
            return None
        
        return {
            'strategy': strategy,
            'data_source': data_source,
            'timestamp': 'fine_grained_processed',  # æ¨™è¨˜ç‚ºæ–°æ ¼å¼
            'filename': filename
        }
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯optuna_resultsæ–‡ä»¶ï¼ˆèˆŠæ ¼å¼ï¼‰
    if not name.startswith('optuna_results_'):
        return None
    
    # ç§»é™¤å‰ç¶´
    name = name.replace('optuna_results_', '')
    
    # æŸ¥æ‰¾ç­–ç•¥åç¨±
    strategy = None
    if name.startswith('ssma_turn_'):
        strategy = 'ssma_turn'
        name = name.replace('ssma_turn_', '')
    elif name.startswith('single_'):
        strategy = 'single'
        name = name.replace('single_', '')
    elif name.startswith('dual_'):
        strategy = 'dual'
        name = name.replace('dual_', '')
    elif name.startswith('RMA_'):
        strategy = 'RMA'
        name = name.replace('RMA_', '')
    
    if not strategy:
        return None
    
    # æŸ¥æ‰¾æ•¸æ“šæº
    data_source = None
    if name.startswith('Self_'):
        data_source = 'Self'
        name = name.replace('Self_', '')
    elif name.startswith('Factor_TWII__2412.TW_'):
        data_source = 'Factor (^TWII / 2412.TW)'
        name = name.replace('Factor_TWII__2412.TW_', '')
    elif name.startswith('Factor_TWII__2414.TW_'):
        data_source = 'Factor (^TWII / 2414.TW)'
        name = name.replace('Factor_TWII__2414.TW_', '')
    elif name.startswith('Mixed_'):
        data_source = 'Mixed'
        name = name.replace('Mixed_', '')
    
    if not data_source:
        return None
    
    # æå–æ™‚é–“æˆ³
    timestamp = name
    
    return {
        'strategy': strategy,
        'data_source': data_source,
        'timestamp': timestamp,
        'filename': filename
    }

# === æ–°å¢ï¼šUI é¸æ“‡ Optuna ç‰ˆæœ¬å’Œè³‡æ–™å¤¾ ===
st.sidebar.header("ğŸ”§ è³‡æ–™ä¾†æºè¨­å®š")

# å®šç¾©å¯èƒ½çš„çµæœè³‡æ–™å¤¾
result_folders = {
    "Fine-grained Processed (æ–°)": Path("../results/fine_grained_processed"),
    "Optuna 13 (é è¨­)": Path("../results"),
    "Optuna 15": Path("../results_op15"),
    "Optuna 13 (å‚™ç”¨)": Path("../results_op13"),
}

# æª¢æŸ¥å“ªäº›è³‡æ–™å¤¾å­˜åœ¨ä¸”æœ‰æª”æ¡ˆ
available_folders = {}
for folder_name, folder_path in result_folders.items():
    if folder_path.exists():
        csv_files = list(folder_path.glob("*.csv"))
        if csv_files:
            available_folders[folder_name] = {
                'path': folder_path,
                'file_count': len(csv_files)
            }

if not available_folders:
    st.error("æ‰¾ä¸åˆ°ä»»ä½•åŒ…å« optuna çµæœçš„è³‡æ–™å¤¾ï¼")
    st.stop()

# UI é¸æ“‡è³‡æ–™å¤¾
folder_options = [f"{name} ({info['file_count']} å€‹æª”æ¡ˆ)" for name, info in available_folders.items()]
selected_folder_name = st.sidebar.selectbox(
    "é¸æ“‡ Optuna çµæœä¾†æº",
    options=list(available_folders.keys()),
    index=0,
    help="é¸æ“‡åŒ…å« optuna_results_*.csv æª”æ¡ˆçš„è³‡æ–™å¤¾"
)

# ç²å–é¸ä¸­çš„è³‡æ–™å¤¾è·¯å¾‘
RESULT_DIR = available_folders[selected_folder_name]['path']
st.sidebar.success(f"å·²é¸æ“‡: {selected_folder_name}")

# é¡¯ç¤ºè³‡æ–™å¤¾è³‡è¨Š
with st.sidebar.expander("ğŸ“ è³‡æ–™å¤¾è³‡è¨Š", expanded=False):
    st.info(f"è³‡æ–™å¤¾è·¯å¾‘: {RESULT_DIR}")
    st.info(f"CSV æª”æ¡ˆæ•¸é‡: {available_folders[selected_folder_name]['file_count']}")
    
    # é¡¯ç¤ºè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆåˆ—è¡¨
    csv_files = list(RESULT_DIR.glob("*.csv"))
    st.subheader("æª”æ¡ˆåˆ—è¡¨:")
    for file_path in csv_files[:10]:  # åªé¡¯ç¤ºå‰10å€‹
        st.text(f"â€¢ {file_path.name}")
    if len(csv_files) > 10:
        st.text(f"... é‚„æœ‰ {len(csv_files) - 10} å€‹æª”æ¡ˆ")

# === è¼‰å…¥æ‰€æœ‰optunaçµæœæ–‡ä»¶ï¼ˆåŠ å…¥æ¬„ä½è‡ªå‹•é©æ‡‰ï¼‰ ===
optuna_files = list(RESULT_DIR.glob("*.csv"))

# è§£ææ–‡ä»¶åä¸¦é¡¯ç¤º
file_info_list = []
for file_path in optuna_files:
    try:
        info = parse_optuna_filename(file_path.name)
        if info:
            file_info_list.append(info)
    except Exception as e:
        logger.warning(f"ç„¡æ³•è§£ææ–‡ä»¶å {file_path.name}: {e}")

# è¼‰å…¥æ‰€æœ‰optunaçµæœï¼ˆåŠ å…¥æ¬„ä½è‡ªå‹•é©æ‡‰è™•ç†ï¼‰
all_optuna_results = []
for file_info in file_info_list:
    file_path = RESULT_DIR / file_info['filename']
    if file_path.exists():
        try:
            df = pd.read_csv(file_path)
            
            # æ¬„ä½è‡ªå‹•é©æ‡‰ï¼šè™•ç†åƒæ•¸æ¬„ä½
            # æª¢æŸ¥æ˜¯å¦æ˜¯ fine-grained processed æ–‡ä»¶
            if file_info['timestamp'] == 'fine_grained_processed':
                # æ–°æ ¼å¼ï¼šå·²ç¶“æœ‰ fine_grained_cluster æ¬„ä½ï¼Œåƒæ•¸æ¬„ä½è™•ç†
                logger.info(f"æª”æ¡ˆ {file_info['filename']} æ˜¯ fine-grained processed æ ¼å¼")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ parameters æ¬„ä½ï¼ˆJSON æ ¼å¼ï¼‰
                if 'parameters' in df.columns:
                    # å¦‚æœæœ‰ parameters æ¬„ä½ï¼Œå˜—è©¦è§£æ JSON
                    try:
                        import json
                        df['parameters'] = df['parameters'].apply(lambda x: json.loads(x) if isinstance(x, str) and x.strip() else {})
                        logger.info(f"æª”æ¡ˆ {file_info['filename']} æˆåŠŸè§£æ parameters æ¬„ä½")
                    except Exception as e:
                        logger.warning(f"ç„¡æ³•è§£æ parameters æ¬„ä½: {e}")
                        df['parameters'] = df['parameters'].apply(lambda x: {} if pd.isna(x) else {})
                else:
                    # å¦‚æœæ²’æœ‰ parameters æ¬„ä½ï¼Œå‰µå»ºä¸€å€‹ç©ºçš„
                    logger.warning(f"æª”æ¡ˆ {file_info['filename']} æ²’æœ‰æ‰¾åˆ° parameters æ¬„ä½ï¼Œä½¿ç”¨é è¨­å€¼")
                    df['parameters'] = [{}] * len(df)
                
                # ç¢ºä¿æœ‰ fine_grained_cluster æ¬„ä½
                if 'fine_grained_cluster' not in df.columns:
                    logger.warning(f"æª”æ¡ˆ {file_info['filename']} ç¼ºå°‘ fine_grained_cluster æ¬„ä½")
                    df['fine_grained_cluster'] = 0
                
            else:
                # èˆŠæ ¼å¼ï¼šOptuna 13/15 æ ¼å¼
                # æª¢æŸ¥æ˜¯å¦æœ‰ param_* æ¬„ä½ï¼ˆOptuna 13/15 æ ¼å¼ï¼‰
                param_columns = [col for col in df.columns if col.startswith('param_')]
                
                if param_columns:
                    # å¦‚æœæœ‰ param_* æ¬„ä½ï¼Œå¾é€™äº›æ¬„ä½æ§‹å»º parameters å­—å…¸
                    logger.info(f"æª”æ¡ˆ {file_info['filename']} ä½¿ç”¨ param_* æ¬„ä½æ ¼å¼ï¼Œæ‰¾åˆ° {len(param_columns)} å€‹åƒæ•¸æ¬„ä½")
                    
                    # æ§‹å»º parameters å­—å…¸
                    parameters_list = []
                    for _, row in df.iterrows():
                        params_dict = {}
                        for col in param_columns:
                            param_name = col.replace('param_', '')
                            param_value = row[col]
                            # å˜—è©¦è½‰æ›ç‚ºé©ç•¶çš„æ•¸æ“šé¡å‹
                            if param_value is not None and str(param_value) != 'nan':
                                try:
                                    # å˜—è©¦è½‰æ›ç‚ºæ•¸å­—
                                    if isinstance(param_value, str) and '.' in param_value:
                                        params_dict[param_name] = float(param_value)
                                    else:
                                        params_dict[param_name] = int(param_value)
                                except:
                                    params_dict[param_name] = param_value
                            else:
                                params_dict[param_name] = None
                        parameters_list.append(params_dict)
                    
                    df['parameters'] = parameters_list
                    
                elif 'parameters' in df.columns:
                    # å¦‚æœæœ‰ parameters æ¬„ä½ï¼ˆJSON æ ¼å¼ï¼‰ï¼Œå˜—è©¦è§£æ
                    try:
                        df['parameters'] = df['parameters'].apply(ast.literal_eval)
                        logger.info(f"æª”æ¡ˆ {file_info['filename']} ä½¿ç”¨ JSON æ ¼å¼ parameters æ¬„ä½")
                    except:
                        logger.warning(f"ç„¡æ³•è§£æ parameters æ¬„ä½ï¼Œä½¿ç”¨é è¨­å€¼: {file_info['filename']}")
                        df['parameters'] = df['parameters'].apply(lambda x: {} if pd.isna(x) else {})
                else:
                    # å¦‚æœæ²’æœ‰åƒæ•¸æ¬„ä½ï¼Œå‰µå»ºä¸€å€‹ç©ºçš„
                    logger.warning(f"æª”æ¡ˆ {file_info['filename']} æ²’æœ‰æ‰¾åˆ°åƒæ•¸æ¬„ä½ï¼Œä½¿ç”¨é è¨­å€¼")
                    df['parameters'] = [{}] * len(df)
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
            df['source_file'] = file_info['filename']
            df['strategy'] = file_info['strategy']
            df['data_source'] = file_info['data_source']
            
            # æ¬„ä½è‡ªå‹•é©æ‡‰ï¼šç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
            required_fields = ['trial_number', 'score']
            for field in required_fields:
                if field not in df.columns:
                    logger.warning(f"æª”æ¡ˆ {file_info['filename']} ç¼ºå°‘æ¬„ä½ {field}ï¼Œä½¿ç”¨é è¨­å€¼")
                    if field == 'trial_number':
                        df[field] = range(len(df))
                    elif field == 'score':
                        df[field] = -np.inf
            
            all_optuna_results.append(df)
        except Exception as e:
            st.sidebar.error(f"è¼‰å…¥å¤±æ•— {file_info['filename']}: {str(e)}")

if not all_optuna_results:
    st.error("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•optunaçµæœæ–‡ä»¶")
    st.stop()

# åˆä½µæ‰€æœ‰çµæœï¼ˆç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½å­˜åœ¨ï¼‰
all_columns = set()
for df in all_optuna_results:
    all_columns.update(df.columns)

# ç‚ºæ¯å€‹ DataFrame æ·»åŠ ç¼ºå¤±çš„æ¬„ä½
for df in all_optuna_results:
    for col in all_columns:
        if col not in df.columns:
            if col in ['num_trades', 'sharpe_ratio', 'max_drawdown', 'profit_factor']:
                df[col] = np.nan
            elif col in ['avg_hold_days', 'excess_return_stress']:
                df[col] = np.nan
            else:
                df[col] = None

optuna_results = pd.concat(all_optuna_results, ignore_index=True)

# æŒ‰ç­–ç•¥å’Œæ•¸æ“šæºåˆ†çµ„ï¼Œæ¯å€‹çµ„åˆé¸å–top5
selected_trials = []

# æ–°å¢åƒæ•¸ç›¸é—œæ€§åˆ†æçµæœå­˜å„²
param_correlations = {}

for strategy in optuna_results['strategy'].unique():
    for data_source in optuna_results['data_source'].unique():
        # ç¯©é¸è©²ç­–ç•¥+æ•¸æ“šæºçš„è©¦é©—
        mask = (optuna_results['strategy'] == strategy) & (optuna_results['data_source'] == data_source)
        strategy_trials_raw = optuna_results[mask]
        
        if len(strategy_trials_raw) == 0:
            continue
            
        logger.info(f"è™•ç†ç­–ç•¥ {strategy} + æ•¸æ“šæº {data_source}: {len(strategy_trials_raw)} å€‹è©¦é©—")
        
        # 1. å…ˆé€²è¡Œåƒæ•¸ç›¸é—œæ€§åˆ†æ
        corr_df = compute_param_correlations(optuna_results, strategy, data_source)
        if not corr_df.empty:
            param_correlations[f"{strategy}_{data_source}"] = corr_df
            
            # æ ¹æ“šç›¸é—œæ€§å¼·åº¦é¸æ“‡ä¸»è¦åƒæ•¸ï¼ˆçµ•å°å€¼>0.1çš„åƒæ•¸ï¼‰
            important_params = []
            for param in corr_df.index:
                max_corr = corr_df.loc[param].abs().max()
                if max_corr > 0.1:  # ç›¸é—œæ€§é–¾å€¼
                    important_params.append(param)
            
            # å¦‚æœæ²’æœ‰é‡è¦åƒæ•¸ï¼Œä½¿ç”¨é è¨­åƒæ•¸
            if not important_params:
                if strategy == 'single':
                    important_params = ['linlen', 'smaalen', 'buy_mult']
                elif strategy == 'dual':
                    important_params = ['linlen', 'smaalen', 'short_win', 'long_win']
                elif strategy == 'RMA':
                    important_params = ['linlen', 'smaalen', 'rma_len', 'dev_len']
                elif strategy == 'ssma_turn':
                    important_params = ['linlen', 'smaalen', 'prom_factor', 'buy_mult']
        else:
            # ä½¿ç”¨é è¨­åƒæ•¸
            if strategy == 'single':
                important_params = ['linlen', 'smaalen', 'buy_mult']
            elif strategy == 'dual':
                important_params = ['linlen', 'smaalen', 'short_win', 'long_win']
            elif strategy == 'RMA':
                important_params = ['linlen', 'smaalen', 'rma_len', 'dev_len']
            elif strategy == 'ssma_turn':
                important_params = ['linlen', 'smaalen', 'prom_factor', 'buy_mult']
        
        # å®šç¾©æ€§èƒ½æŒ‡æ¨™ç”¨æ–¼å¤šæ¨£æ€§ç¯©é¸ï¼ˆæ¬„ä½è‡ªå‹•é©æ‡‰ï¼‰
        metric_keys = ['num_trades']  # åŸºæœ¬æŒ‡æ¨™
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ Optuna 15 çš„æ–°æ¬„ä½
        if 'excess_return_stress' in strategy_trials_raw.columns:
            metric_keys.append('excess_return_stress')
        if 'avg_hold_days' in strategy_trials_raw.columns:
            metric_keys.append('avg_hold_days')
        
        logger.info(f"ç­–ç•¥ {strategy} ä½¿ç”¨æ€§èƒ½æŒ‡æ¨™: {metric_keys}")
        
        # 2. è½‰æ›ç‚ºå­—å…¸æ ¼å¼
        trials_dict = []
        for _, trial in strategy_trials_raw.iterrows():
            trial_dict = trial.to_dict()
            trials_dict.append(trial_dict)
        
        # 3. ä½¿ç”¨å¤šæ¨£æ€§éæ¿¾é¸æ“‡top5
        logger.info(f"é–‹å§‹ç‚ºç­–ç•¥ {strategy} + {data_source} é€²è¡Œå¤šæ¨£æ€§ç¯©é¸...")
        diverse_trials = pick_topN_by_diversity(
            trials_dict, 
            metric_keys, 
            top_n=5
        )
        
        logger.info(f"ç­–ç•¥ {strategy} + {data_source} ç¯©é¸å®Œæˆï¼Œé¸å– {len(diverse_trials)} å€‹è©¦é©—")
        
        # 4. æ·»åŠ ç­–ç•¥å’Œæ•¸æ“šæºä¿¡æ¯
        for trial in diverse_trials:
            # ç”Ÿæˆç°¡çŸ­åç¨±ï¼ˆä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼ï¼‰
            if '2412' in data_source:
                short_name = f"{strategy}_2412_{trial['trial_number']}"
            elif '2414' in data_source:
                short_name = f"{strategy}_2414_{trial['trial_number']}"
            elif 'Self' in data_source:
                short_name = f"{strategy}_Self_{trial['trial_number']}"
            else:
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è™•ç†data_source
                clean_source = data_source.replace('(', '').replace(')', '').replace('/', '_').replace('^', '').strip()
                short_name = f"{strategy}_{clean_source}_{trial['trial_number']}"
            
            trial['short_name'] = short_name
            selected_trials.append(trial)
            logger.info(f"æ·»åŠ è©¦é©—: {short_name} (score: {trial['score']:.3f})")

logger.info(f"æ‰€æœ‰ç­–ç•¥ç¯©é¸å®Œæˆï¼Œç¸½å…±é¸å– {len(selected_trials)} å€‹è©¦é©—")

# å»ºç«‹ç­–ç•¥æ¸…å–®
optuna_strategies = [
    {
        'name': trial['short_name'],  # ä½¿ç”¨ç°¡çŸ­åç¨±
        'strategy_type': trial['strategy'],
        'params': trial['parameters'],
        'smaa_source': trial['data_source']
    } for trial in selected_trials
]
preset_strategies = [
    {
        'name': key,
        'strategy_type': value['strategy_type'],
        'params': {k: v for k, v in value.items() if k not in ['strategy_type', 'smaa_source']},
        'smaa_source': value['smaa_source']
    } for key, value in param_presets.items()
]
all_strategies = optuna_strategies + preset_strategies

# å´é‚Šæ¬„åªåšè¨Šæ¯å‘ˆç¾
st.sidebar.header("ğŸ“Š ç³»çµ±ç‹€æ…‹")
st.sidebar.info(f"å·²è¼‰å…¥ {len(selected_trials)} å€‹è©¦é©—")
st.sidebar.info(f"å¯ç”¨ç­–ç•¥: {len(all_strategies)} å€‹")

# ç­–ç•¥çµ±è¨ˆ
strategy_counts = {}
for trial in selected_trials:
    strategy = trial['strategy']
    data_source = trial['data_source']
    key = f"{strategy}_{data_source}"
    strategy_counts[key] = strategy_counts.get(key, 0) + 1

st.sidebar.subheader("ç­–ç•¥åˆ†å¸ƒ:")
for key, count in strategy_counts.items():
    st.sidebar.text(f"â€¢ {key}: {count}")

# è¼‰å…¥ä¿¡æ¯ç§»åˆ°å°èˆªæ¬„
with st.sidebar.expander("ğŸ“Š è¼‰å…¥ä¿¡æ¯", expanded=False):
    st.info(f"æ‰¾åˆ° {len(optuna_files)} å€‹optuna resultsæ–‡ä»¶")
    
    # é¡¯ç¤ºæ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
    st.subheader("å¯ç”¨çš„æ–‡ä»¶:")
    for file_info in file_info_list:
        st.text(f"â€¢ {file_info['strategy']} - {file_info['data_source']} ({file_info['timestamp']})")
    
    # é¡¯ç¤ºæ¬„ä½è³‡è¨Š
    st.subheader("ğŸ“‹ æ¬„ä½è³‡è¨Š:")
    if not optuna_results.empty:
        all_columns = list(optuna_results.columns)
        st.text(f"ç¸½æ¬„ä½æ•¸: {len(all_columns)}")
        st.text("ä¸»è¦æ¬„ä½:")
        for col in ['trial_number', 'score', 'strategy', 'data_source', 'parameters', 'num_trades', 'sharpe_ratio', 'max_drawdown']:
            if col in all_columns:
                st.text(f"âœ“ {col}")
            else:
                st.text(f"âœ— {col} (ç¼ºå¤±)")
        
        # æª¢æŸ¥ Optuna 15 ç‰¹æœ‰æ¬„ä½
        optuna15_fields = ['avg_hold_days', 'excess_return_stress', 'pbo_score']
        st.text("Optuna 15 ç‰¹æœ‰æ¬„ä½:")
        for col in optuna15_fields:
            if col in all_columns:
                st.text(f"âœ“ {col}")
            else:
                st.text(f"âœ— {col} (ç¼ºå¤±)")
    
    # é¡¯ç¤ºåƒæ•¸ç›¸é—œæ€§åˆ†æçµæœ
    if param_correlations:
        st.subheader("ğŸ“ˆ åƒæ•¸ç›¸é—œæ€§åˆ†æ")
        st.write("è¨ˆç®—æ¯å€‹åƒæ•¸å° total_return å’Œ sharpe_ratio çš„çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸")
        for key, corr_df in param_correlations.items():
            if not corr_df.empty:
                st.write(f"**{key}**")
                # æ ¼å¼åŒ–é¡¯ç¤ºç›¸é—œæ€§çŸ©é™£
                formatted_corr = corr_df.style.format("{:.3f}")
                st.dataframe(formatted_corr, use_container_width=True)
                
                # é¡¯ç¤ºé‡è¦åƒæ•¸
                important_params = []
                for param in corr_df.index:
                    max_corr = corr_df.loc[param].abs().max()
                    if max_corr > 0.1:
                        important_params.append(f"{param} ({max_corr:.3f})")
                
                if important_params:
                    st.write(f"**é‡è¦åƒæ•¸ (|ç›¸é—œæ€§| > 0.1):** {', '.join(important_params)}")
                st.divider()

# ç­–ç•¥çµ±è¨ˆ
strategy_counts = {}
for trial in selected_trials:
    strategy = trial['strategy']
    data_source = trial['data_source']
    key = f"{strategy}_{data_source}"
    strategy_counts[key] = strategy_counts.get(key, 0) + 1

st.sidebar.subheader("ç­–ç•¥åˆ†å¸ƒ:")
for key, count in strategy_counts.items():
    st.sidebar.text(f"â€¢ {key}: {count}")

# è¼‰å…¥ä¿¡æ¯ç§»åˆ°å°èˆªæ¬„
with st.sidebar.expander("ğŸ“Š è¼‰å…¥ä¿¡æ¯", expanded=False):
    st.info(f"æ‰¾åˆ° {len(optuna_files)} å€‹optuna resultsæ–‡ä»¶")
    
    # é¡¯ç¤ºæ‰€æœ‰æ–‡ä»¶ä¿¡æ¯
    st.subheader("å¯ç”¨çš„æ–‡ä»¶:")
    for file_info in file_info_list:
        st.text(f"â€¢ {file_info['strategy']} - {file_info['data_source']} ({file_info['timestamp']})")



# èµ°æŸ¥è¨­å®šç§»åˆ°ä¸»é é¢
col1, col2 = st.columns(2)
with col1:
    walk_forward_mode = st.selectbox("èµ°æŸ¥æ¨¡å¼", ["å‹•æ…‹å¹³åˆ†å€é–“", "å›ºå®š WF_PERIODS"])
with col2:
    if walk_forward_mode == "å‹•æ…‹å¹³åˆ†å€é–“":
        n_splits = st.number_input("åˆ†æ®µæ•¸", min_value=1, max_value=10, value=6, step=1)

# ç­–ç•¥é¸æ“‡ - æ“´å¤§é¸æ“‡å€å¡Š

# æ·»åŠ  Ensemble ç­–ç•¥å³æ—¶è°ƒå‚é¢æ¿
if ENSEMBLE_AVAILABLE:
    st.subheader("ğŸ¯ Ensemble ç­–ç•¥å³æ—¶è°ƒå‚")
    
    # æ˜¾ç¤ºå»ºè®®å‚æ•°ç»„åˆ
    with st.expander("ğŸ“‹ å°å‹ç½‘æ ¼æ‰«æä¼˜åŒ–åçš„å»ºè®®å‚æ•°", expanded=False):
        st.markdown("""
        **åŸºäºå°å‹ç½‘æ ¼æ‰«æçš„ä¼˜åŒ–ç»“æœï¼š**
        
        **Majorityç­–ç•¥å»ºè®®ï¼š**
        - `delta_cap`: 0.10ï¼ˆæƒé‡å˜åŒ–ä¸Šé™ï¼‰
        - `min_cooldown_days`: 5ï¼ˆå†·å´å¤©æ•°ï¼‰
        - `min_trade_dw`: 0.02ï¼ˆæœ€å°æƒé‡å˜åŒ–é˜ˆå€¼ï¼‰
        
        **Proportionalç­–ç•¥å»ºè®®ï¼š**
        - `delta_cap`: 0.30ï¼ˆæƒé‡å˜åŒ–ä¸Šé™ï¼‰
        - `min_cooldown_days`: 1ï¼ˆå†·å´å¤©æ•°ï¼‰
        - `min_trade_dw`: 0.02ï¼ˆæœ€å°æƒé‡å˜åŒ–é˜ˆå€¼ï¼‰
        
        **ä¼˜åŒ–æ•ˆæœï¼š**
        - è°ƒæ•´æ¬¡æ•°ï¼š9 â†’ 2ï¼ˆå‡å°‘78%ï¼‰
        - Turnoverï¼š0.80 â†’ 0.60ï¼ˆå‡å°‘25%ï¼‰
        - æŠ¥é…¬ç‡ï¼šç»´æŒâ‰¥98%åŸºçº¿æ°´å¹³
        """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        ensemble_method = st.selectbox(
            "é›†æˆæ–¹æ³•",
            ["majority", "proportional"],
            help="Majority: K-of-N å¤šæ•°å†³, Proportional: æŒ‰å¤šå¤´æ¯”ä¾‹åˆ†é…",
            key="ensemble_method"
        )
        
        # æ ¹æ®é€‰æ‹©çš„ensembleæ–¹æ³•è‡ªåŠ¨è°ƒæ•´å‚æ•°é»˜è®¤å€¼
        if ensemble_method == "majority":
            default_delta_cap = 0.10
            default_cooldown = 5
            default_min_trade_dw = 0.02
        else:  # proportional
            default_delta_cap = 0.30
            default_cooldown = 1
            default_min_trade_dw = 0.02
        
        ensemble_floor = st.slider(
            "åº•ä»“æ¯”ä¾‹",
            min_value=0.0,
            max_value=0.5,
            value=0.2,
            step=0.05,
            help="æœ€å°æŒä»“æ¯”ä¾‹ï¼Œé¿å…å®Œå…¨ç©ºä»“",
            key="ensemble_floor"
        )
        
        ensemble_ema_span = st.slider(
            "EMA å¹³æ»‘å¤©æ•°",
            min_value=1,
            max_value=30,
            value=3,
            help="æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘å¤©æ•°ï¼Œå‡å°‘æƒé‡æ³¢åŠ¨",
            key="ensemble_ema_span"
        )
        
        ensemble_delta_cap = st.slider(
            "æƒé‡å˜åŒ–ä¸Šé™",
            min_value=0.05,
            max_value=0.50,
            value=default_delta_cap,  # æ ¹æ®ensembleæ–¹æ³•è‡ªåŠ¨è°ƒæ•´
            step=0.01,
            help=f"æ¯æ—¥æƒé‡å˜åŒ–çš„æœ€å¤§å¹…åº¦ï¼ˆ{ensemble_method.title()}ç­–ç•¥å»ºè®®{default_delta_cap}ï¼‰",
            key="ensemble_delta_cap"
        )
    
    with col2:
        if ensemble_method == "majority":
            ensemble_majority_k = st.slider(
                "å¤šæ•°å†³é—¨æ§› (K)",
                min_value=3,
                max_value=11,
                value=6,
                help="K-of-N ä¸­çš„ K å€¼ï¼Œéœ€è¦è‡³å°‘ K ä¸ªç­–ç•¥çœ‹å¤š",
                key="ensemble_majority_k"
            )
        else:
            ensemble_majority_k = 6  # é»˜è®¤å€¼ï¼Œä¸æ˜¾ç¤º
        
        ensemble_min_cooldown_days = st.slider(
            "æœ€å°å†·å´å¤©æ•°",
            min_value=1,
            max_value=10,
            value=default_cooldown,  # æ ¹æ®ensembleæ–¹æ³•è‡ªåŠ¨è°ƒæ•´
            step=1,
            help=f"é¿å…é¢‘ç¹è°ƒæ•´æƒé‡çš„æœ€å°é—´éš”å¤©æ•°ï¼ˆ{ensemble_method.title()}ç­–ç•¥å»ºè®®{default_cooldown}ï¼‰",
            key="ensemble_min_cooldown_days"
        )
        
        ensemble_min_trade_dw = st.slider(
            "æœ€å°æƒé‡å˜åŒ–é˜ˆå€¼",
            min_value=0.00,
            max_value=0.10,
            value=default_min_trade_dw,  # æ ¹æ®ensembleæ–¹æ³•è‡ªåŠ¨è°ƒæ•´
            step=0.01,
            help=f"å¿½ç•¥å¾®å°çš„æƒé‡å˜åŒ–ï¼Œå‡å°‘äº¤æ˜“æˆæœ¬ï¼ˆ{ensemble_method.title()}ç­–ç•¥å»ºè®®{default_min_trade_dw}ï¼‰",
            key="ensemble_min_trade_dw"
        )
    
    # Ensemble ç­–ç•¥è¿è¡ŒæŒ‰é’®
    if st.button("ğŸš€ è¿è¡Œ Ensemble ç­–ç•¥", type="secondary", key="run_ensemble"):
        with st.spinner("æ­£åœ¨è¿è¡Œ Ensemble ç­–ç•¥..."):
            try:
                # æ„å»ºå‚æ•°
                ensemble_params = {
                    'method': ensemble_method,
                    'floor': ensemble_floor,
                    'ema_span': ensemble_ema_span,
                    'delta_cap': ensemble_delta_cap,
                    'majority_k': ensemble_majority_k,
                    'min_cooldown_days': ensemble_min_cooldown_days,
                    'min_trade_dw': ensemble_min_trade_dw
                }
                
                # è¿è¡Œç­–ç•¥
                equity_curve, trades, stats = run_ensemble_strategy(ensemble_params)
                
                if equity_curve is not None and not equity_curve.empty:
                    st.success("âœ… Ensemble ç­–ç•¥è¿è¡ŒæˆåŠŸï¼")
                    
                    # æ˜¾ç¤ºç»©æ•ˆæŒ‡æ ‡
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("æ€»æŠ¥é…¬ç‡", f"{stats.get('total_return', 0):.2%}")
                    with col2:
                        st.metric("å¹´åŒ–æŠ¥é…¬ç‡", f"{stats.get('annual_return', 0):.2%}")
                    with col3:
                        st.metric("æœ€å¤§å›æ’¤", f"{stats.get('max_drawdown', 0):.2%}")
                    with col4:
                        st.metric("å¤æ™®æ¯”ç‡", f"{stats.get('sharpe_ratio', 0):.3f}")
                    
                    # æ˜¾ç¤ºæƒç›Šæ›²çº¿
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=equity_curve.index,
                        y=equity_curve.values,
                        name='Ensemble ç­–ç•¥',
                        line=dict(color='red', width=2)
                    ))
                    fig.update_layout(
                        title="Ensemble ç­–ç•¥æƒç›Šæ›²çº¿",
                        xaxis_title="æ—¥æœŸ",
                        yaxis_title="æƒç›Š",
                        template="plotly_white"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºäº¤æ˜“è®°å½•
                    if not trades.empty:
                        st.subheader("äº¤æ˜“è®°å½•")
                        st.dataframe(trades, use_container_width=True)
                else:
                    st.warning("âš ï¸ Ensemble ç­–ç•¥è¿è¡Œå®Œæˆï¼Œä½†æœªè¿”å›æœ‰æ•ˆç»“æœ")
                    
            except Exception as e:
                st.error(f"âŒ Ensemble ç­–ç•¥è¿è¡Œå¤±è´¥: {str(e)}")
    
    st.divider()

selected_strategies = st.multiselect(
    "é¸æ“‡è¦åˆ†æçš„ç­–ç•¥", 
    options=[s['name'] for s in all_strategies], 
    default=[s['name'] for s in all_strategies],  # é è¨­å…¨é¸
    key="main_strategy_selector"
)

# å¿«é€Ÿé¸æ“‡æŒ‰éˆ•
col1, col2, col3 = st.columns(3)
with col1:
    if st.button("å…¨é¸"):
        st.session_state.main_strategies = [s['name'] for s in all_strategies]
        st.rerun()
with col2:
    if st.button("æ¸…é™¤"):
        st.session_state.main_strategies = []
        st.rerun()
with col3:
    if st.button("é¸æ“‡Optunaç­–ç•¥"):
        optuna_names = [s['name'] for s in optuna_strategies]
        st.session_state.main_strategies = optuna_names
        st.rerun()

if not selected_strategies:
    st.warning("è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç­–ç•¥é€²è¡Œåˆ†æ")
    st.stop()

st.info(f"å·²é¸æ“‡ {len(selected_strategies)} å€‹ç­–ç•¥é€²è¡Œåˆ†æ")

# ç›¸é—œæ€§çŸ©é™£è¨­å®šç§»åˆ°ä¸»é é¢
corr_range = st.slider("ç›¸é—œæ€§å€¼ç¯„åœ", min_value=-1.0, max_value=1.0, value=(0.8, 1.0), step=0.1, help="è¨­å®šç›¸é—œæ€§çŸ©é™£ç†±åœ–çš„é¡è‰²ç¯„åœ")

# åŸ·è¡Œå›æ¸¬æŒ‰éˆ•
if st.button("ğŸš€ åŸ·è¡Œå›æ¸¬èˆ‡åˆ†æ", type="primary"):
    # éæ¿¾é¸ä¸­çš„ç­–ç•¥
    strategies_to_run = [s for s in all_strategies if s['name'] in selected_strategies]

    with st.spinner("æ­£åœ¨åŸ·è¡Œå›æ¸¬..."):
        # è¼‰å…¥æ•¸æ“š
        smaa_sources = set(strategy['smaa_source'] for strategy in strategies_to_run)
        df_price_dict = {}
        df_factor_dict = {}
        all_indices = []
        
        # å°‡æ•¸æ“šè¼‰å…¥è¨Šæ¯æ”¶é›†åˆ°expanderä¸­
        with st.expander("ğŸ“Š è¼‰å…¥ä¿¡æ¯", expanded=False):
            st.subheader("æ•¸æ“šè¼‰å…¥ç‹€æ…‹:")
            for source in smaa_sources:
                df_price, df_factor = load_data(ticker="00631L.TW", smaa_source=source)
                if not df_price.empty:
                    all_indices.append(df_price.index)
                df_price_dict[source] = df_price
                df_factor_dict[source] = df_factor
                st.text(f"â€¢ {source}: å·²è¼‰å…¥ {len(df_price)} ç­†æ•¸æ“š")

        # åˆä½µæ‰€æœ‰æ™‚é–“è»¸ï¼Œå‰µå»ºä¸€å€‹å…¨åŸŸæ™‚é–“è»¸
        if all_indices:
            global_index = pd.Index([])
            for index in all_indices:
                global_index = global_index.union(index)
        else:
            global_index = pd.Index([])

        # åŸ·è¡Œå›æ¸¬
        results = {}
        initial_equity = 100000.0
        for strategy in strategies_to_run:
            name = strategy['name']
            strategy_type = strategy['strategy_type']
            params = strategy['params']
            smaa_source = strategy['smaa_source']
            df_price = df_price_dict[smaa_source]
            df_factor = df_factor_dict[smaa_source]
            
            logger.info(f"è™•ç†ç­–ç•¥ {name}ï¼Œåƒæ•¸: {params}")
            
            # ä¿®æ­£åƒæ•¸è™•ç†ï¼šæ”¯æ´å¤šç¨®æ ¼å¼
            # æª¢æŸ¥æ˜¯å¦æœ‰ parameters æ¬„ä½ï¼ˆJSON æ ¼å¼ï¼‰
            if isinstance(params, dict) and 'parameters' in params:
                # å¦‚æœæœ‰ parameters æ¬„ä½ï¼Œä½¿ç”¨å®ƒ
                actual_params = params['parameters']
                logger.info(f"ä½¿ç”¨ JSON æ ¼å¼åƒæ•¸: {actual_params}")
            elif isinstance(params, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå˜—è©¦è§£æç‚º JSON
                try:
                    import json
                    actual_params = json.loads(params)
                    logger.info(f"è§£æ JSON å­—ç¬¦ä¸²åƒæ•¸: {actual_params}")
                except json.JSONDecodeError:
                    logger.error(f"ç„¡æ³•è§£æ JSON åƒæ•¸: {params}")
                    continue
            elif isinstance(params, dict) and len(params) == 0:
                # å¦‚æœæ˜¯ç©ºå­—å…¸ï¼Œå˜—è©¦å¾ trial ä¸­ç²å– parameters
                logger.warning(f"ç­–ç•¥ {name} çš„åƒæ•¸ç‚ºç©ºå­—å…¸ï¼Œå˜—è©¦å¾ trial ä¸­ç²å–")
                # é€™è£¡éœ€è¦å¾ selected_trials ä¸­æ‰¾åˆ°å°æ‡‰çš„ trial
                for trial in selected_trials:
                    if trial['short_name'] == name:
                        if isinstance(trial['parameters'], str):
                            try:
                                import json
                                actual_params = json.loads(trial['parameters'])
                                logger.info(f"å¾ trial è§£æ JSON å­—ç¬¦ä¸²åƒæ•¸: {actual_params}")
                            except json.JSONDecodeError:
                                logger.error(f"ç„¡æ³•è§£æ trial çš„ JSON åƒæ•¸: {trial['parameters']}")
                                continue
                        else:
                            actual_params = trial['parameters']
                            logger.info(f"å¾ trial ç²å–åƒæ•¸: {actual_params}")
                        break
                else:
                    logger.error(f"æ‰¾ä¸åˆ°ç­–ç•¥ {name} å°æ‡‰çš„ trial")
                    continue
            else:
                # å¦å‰‡ç›´æ¥ä½¿ç”¨ paramsï¼ˆå·²ç¶“æ˜¯åˆ†é›¢çš„æ¬„ä½ï¼‰
                actual_params = params
                logger.info(f"ä½¿ç”¨åˆ†é›¢æ¬„ä½åƒæ•¸: {actual_params}")
            
            if strategy_type == 'single':
                df_ind = compute_single(df_price, df_factor, actual_params['linlen'], actual_params['factor'], actual_params['smaalen'], actual_params['devwin'], smaa_source=smaa_source)
                logger.info(f"ç­–ç•¥ {name} çš„ df_ind å½¢ç‹€: {df_ind.shape}, æ¬„ä½: {df_ind.columns.tolist()}")
            elif strategy_type == 'dual':
                df_ind = compute_dual(df_price, df_factor, actual_params['linlen'], actual_params['factor'], actual_params['smaalen'], actual_params['short_win'], actual_params['long_win'], smaa_source=smaa_source)
                logger.info(f"ç­–ç•¥ {name} çš„ df_ind å½¢ç‹€: {df_ind.shape}, æ¬„ä½: {df_ind.columns.tolist()}")
            elif strategy_type == 'RMA':
                df_ind = compute_RMA(df_price, df_factor, actual_params['linlen'], actual_params['factor'], actual_params['smaalen'], actual_params['rma_len'], actual_params['dev_len'], smaa_source=smaa_source)
                logger.info(f"ç­–ç•¥ {name} çš„ df_ind å½¢ç‹€: {df_ind.shape}, æ¬„ä½: {df_ind.columns.tolist()}")
            elif strategy_type == 'ssma_turn':
                calc_keys = ['linlen', 'factor', 'smaalen', 'prom_factor', 'min_dist', 'buy_shift', 'exit_shift', 'vol_window', 'quantile_win', 'signal_cooldown_days']
                ssma_params = {k: v for k, v in actual_params.items() if k in calc_keys}
                backtest_params = ssma_params.copy()
                backtest_params['buy_mult'] = actual_params.get('buy_mult', 0.5)
                backtest_params['sell_mult'] = actual_params.get('sell_mult', 0.5)
                backtest_params['stop_loss'] = actual_params.get('stop_loss', 0.0)
                logger.info(f"SSMA_Turn åƒæ•¸: {ssma_params}")
                df_ind, buy_dates, sell_dates = compute_ssma_turn_combined(df_price, df_factor, **ssma_params, smaa_source=smaa_source)
                logger.info(f"ç­–ç•¥ {name} ç”Ÿæˆçš„è²·å…¥ä¿¡è™Ÿæ•¸: {len(buy_dates)}, è³£å‡ºä¿¡è™Ÿæ•¸: {len(sell_dates)}")
                logger.info(f"è²·å…¥ä¿¡è™Ÿ: {buy_dates[:5] if buy_dates else 'ç„¡'}")
                logger.info(f"è³£å‡ºä¿¡è™Ÿ: {sell_dates[:5] if sell_dates else 'ç„¡'}")
                if df_ind.empty:
                    logger.error(f"ç­–ç•¥ {name} çš„ df_ind ç‚ºç©ºï¼Œè·³éå›æ¸¬")
                    st.error(f"ç­–ç•¥ {name} çš„ df_ind ç‚ºç©ºï¼Œè·³éå›æ¸¬")
                    continue
                # ä½¿ç”¨èˆ‡SSSv096ä¸€è‡´çš„é è¨­åƒæ•¸è¨­å®š
                result = backtest_unified(df_ind, strategy_type, actual_params, buy_dates, sell_dates, 
                                         discount=0.30, trade_cooldown_bars=3, bad_holding=False)
                results[name] = result
                continue
            
            required_cols = ['open', 'close', 'smaa', 'base', 'sd']
            if df_ind.empty or not all(col in df_ind.columns for col in required_cols):
                logger.error(f"ç­–ç•¥ {name} çš„ df_ind ç¼ºå°‘å¿…è¦æ¬„ä½: {set(required_cols) - set(df_ind.columns)}ï¼Œè·³éå›æ¸¬")
                st.error(f"ç­–ç•¥ {name} çš„ df_ind ç¼ºå°‘å¿…è¦æ¬„ä½: {set(required_cols) - set(df_ind.columns)}ï¼Œè·³éå›æ¸¬")
                continue
            
            # å°æ–¼ single å’Œ RMAï¼Œç›®å‰çš„å›æ¸¬åƒæ•¸æ˜¯å›ºå®šçš„
            # æ³¨æ„ï¼šå¦‚æœé€™äº›ç­–ç•¥ä¹Ÿéœ€è¦ä¸åŒçš„ discount æˆ– cooldownï¼Œé€™è£¡éœ€è¦ä¿®æ”¹
            if df_ind.empty:
                logger.error(f"ç­–ç•¥ {name} çš„ df_ind ç‚ºç©ºï¼Œè·³éè¨ˆç®—")
                continue
            
            result = backtest_unified(df_ind, strategy_type, actual_params, discount=0.30, trade_cooldown_bars=3, bad_holding=False)
            results[name] = result

        # æå–æ¬Šç›Šæ›²ç·š
        equity_curves = pd.DataFrame({name: result['equity_curve'].reindex(global_index, fill_value=initial_equity) for name, result in results.items()})
        
        # è¨ˆç®—ä¸¦å„²å­˜æŒ‡æ¨™
        for name, result in results.items():
            if 'equity_curve' in result and not result['equity_curve'].empty:
                # è¨ˆç®—åŸºæœ¬æŒ‡æ¨™
                equity_curve = result['equity_curve']
                total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0] - 1)
                annual_return = total_return * (252 / len(equity_curve))
                daily_returns = equity_curve.pct_change().dropna()
                volatility = daily_returns.std() * np.sqrt(252)
                sharpe_ratio = annual_return / volatility if volatility > 0 else 0
                max_drawdown = calculate_max_drawdown(equity_curve)
                calmar_ratio = annual_return / max_drawdown if max_drawdown > 0 else 0
                
                # è¨ˆç®—é¢¨éšªèª¿æ•´æŒ‡æ¨™
                risk_adjusted_metrics = calculate_risk_adjusted_metrics(equity_curve, name)
                
                # ä¿®æ­£å‹ç‡è¨ˆç®— - ç§»åˆ°metricså­—å…¸å‰µå»ºä¹‹å‰
                trades_df = result.get('trades_df', pd.DataFrame())
                logger.info(f"ç­–ç•¥ {name} çš„ trades_df å½¢ç‹€: {trades_df.shape}")
                logger.info(f"ç­–ç•¥ {name} çš„ trades_df æ¬„ä½: {list(trades_df.columns)}")

                # æ·»åŠ æ›´è©³ç´°çš„èª¿è©¦ä¿¡æ¯
                if not trades_df.empty:
                    logger.info(f"ç­–ç•¥ {name} çš„ trades_df å‰5è¡Œ:")
                    logger.info(trades_df.head())
                    logger.info(f"ç­–ç•¥ {name} çš„ trades_df æ•¸æ“šé¡å‹:")
                    logger.info(trades_df.dtypes)
                    if 'ret' in trades_df.columns:
                        logger.info(f"ç­–ç•¥ {name} çš„ 'ret' æ¬„ä½çµ±è¨ˆ:")
                        logger.info(f"  éç©ºå€¼æ•¸é‡: {trades_df['ret'].notna().sum()}")
                        logger.info(f"  ç©ºå€¼æ•¸é‡: {trades_df['ret'].isna().sum()}")
                        logger.info(f"  å”¯ä¸€å€¼: {trades_df['ret'].unique()}")
                        logger.info(f"  å¤§æ–¼0çš„æ•¸é‡: {(trades_df['ret'] > 0).sum()}")
                        logger.info(f"  ç­‰æ–¼0çš„æ•¸é‡: {(trades_df['ret'] == 0).sum()}")
                        logger.info(f"  å°æ–¼0çš„æ•¸é‡: {(trades_df['ret'] < 0).sum()}")

                # è¨ˆç®—å‹ç‡
                win_rate = 0
                if not trades_df.empty and 'ret' in trades_df.columns:
                    winning_trades = trades_df[trades_df['ret'] > 0]
                    total_trades = len(trades_df)
                    winning_count = len(winning_trades)
                    win_rate = winning_count / total_trades if total_trades > 0 else 0
                    logger.info(f"ç­–ç•¥ {name} å‹ç‡è¨ˆç®—: ç¸½äº¤æ˜“æ•¸={total_trades}, ç²åˆ©äº¤æ˜“æ•¸={winning_count}, å‹ç‡={win_rate:.2%}")
                    
                    # æ·»åŠ è©³ç´°çš„äº¤æ˜“è¨˜éŒ„æ—¥èªŒ
                    if total_trades > 0:
                        logger.info(f"ç­–ç•¥ {name} äº¤æ˜“å ±é…¬ç‡çµ±è¨ˆ:")
                        logger.info(f"  æœ€å°å ±é…¬ç‡: {trades_df['ret'].min():.4f}")
                        logger.info(f"  æœ€å¤§å ±é…¬ç‡: {trades_df['ret'].max():.4f}")
                        logger.info(f"  å¹³å‡å ±é…¬ç‡: {trades_df['ret'].mean():.4f}")
                        logger.info(f"  æ­£å ±é…¬äº¤æ˜“: {winning_count} ç­†")
                        logger.info(f"  è² å ±é…¬äº¤æ˜“: {total_trades - winning_count} ç­†")
                else:
                    logger.warning(f"ç­–ç•¥ {name} çš„ trades_df ç‚ºç©ºæˆ–ç¼ºå°‘ 'ret' æ¬„ä½")
                    if not trades_df.empty:
                        logger.warning(f"ç­–ç•¥ {name} çš„ trades_df å¯¦éš›æ¬„ä½: {list(trades_df.columns)}")

                # åˆä½µæ‰€æœ‰æŒ‡æ¨™
                result['metrics'] = {
                    'total_return': total_return,
                    'annual_return': annual_return,
                    'volatility': volatility,
                    'sharpe_ratio': sharpe_ratio,
                    'calmar_ratio': calmar_ratio,
                    'max_drawdown': max_drawdown,
                    'num_trades': len(result.get('trades_df', pd.DataFrame())),
                    'win_rate': win_rate,  # ä½¿ç”¨è¨ˆç®—å‡ºçš„å‹ç‡
                    **risk_adjusted_metrics  # æ·»åŠ é¢¨éšªèª¿æ•´æŒ‡æ¨™
                }
                
                # ä¿®æ­£å¡ç‘ªå€¼è¨ˆç®—
                if result['metrics']['max_drawdown'] != 0:
                    result['metrics']['calmar_ratio'] = result['metrics']['annual_return'] / abs(result['metrics']['max_drawdown'])
                else:
                    result['metrics']['calmar_ratio'] = 0
        
        if equity_curves.empty:
            logger.error("æœªç”Ÿæˆä»»ä½•æ¬Šç›Šæ›²ç·šï¼Œè«‹æª¢æŸ¥å›æ¸¬é‚è¼¯æˆ–æ•¸æ“š")
            st.error("æœªç”Ÿæˆä»»ä½•æ¬Šç›Šæ›²ç·šï¼Œè«‹æª¢æŸ¥å›æ¸¬é‚è¼¯æˆ–æ•¸æ“š")
        else:
            logger.info(f"æ¬Šç›Šæ›²ç·šå½¢ç‹€: {equity_curves.shape}")
            logger.info(equity_curves.head())

            # ä½¿ç”¨æ¨™ç±¤é å‘ˆç¾çµæœ
            tabs = st.tabs(["ç›¸é—œæ€§çŸ©é™£ç†±åœ–", "å ±é…¬ç‡ç›¸é—œ", "æœ€å¤§å›æ’¤ç›¸é—œ", "ç¸½çµ", "éæ“¬åˆæª¢æ¸¬", "Topç­–ç•¥åƒæ•¸"])

            # æ¨™ç±¤é  1: ç›¸é—œæ€§çŸ©é™£ç†±åœ–
            with tabs[0]:
                st.subheader("ç­–ç•¥ç›¸é—œæ€§çŸ©é™£ç†±åœ–")
                
                # é¡¯ç¤ºèªªæ˜æ°£æ³¡
                with st.expander("ğŸ“– ç›¸é—œæ€§åˆ†æèªªæ˜", expanded=False):
                    st.markdown("""
                    **ç›¸é—œæ€§çŸ©é™£**: é¡¯ç¤ºä¸åŒç­–ç•¥ä¹‹é–“çš„ç›¸é—œæ€§
                    
                    **ç›¸é—œä¿‚æ•¸ç¯„åœ**: -1 åˆ° +1
                    - +1: å®Œå…¨æ­£ç›¸é—œï¼ˆå…©å€‹ç­–ç•¥å®Œå…¨åŒæ­¥ï¼‰
                    - 0: ç„¡ç›¸é—œï¼ˆå…©å€‹ç­–ç•¥ç¨ç«‹ï¼‰
                    - -1: å®Œå…¨è² ç›¸é—œï¼ˆå…©å€‹ç­–ç•¥å®Œå…¨ç›¸åï¼‰
                    
                    **æŠ•è³‡çµ„åˆæ„ç¾©**:
                    - ä½ç›¸é—œæ€§ï¼ˆæ¥è¿‘0ï¼‰: é©åˆçµ„åˆæŠ•è³‡ï¼Œåˆ†æ•£é¢¨éšª
                    - é«˜ç›¸é—œæ€§ï¼ˆæ¥è¿‘Â±1ï¼‰: çµ„åˆæ•ˆæœæœ‰é™ï¼Œé¢¨éšªé›†ä¸­
                    """)
                
                # ç›´æ¥ä½¿ç”¨å·²è¨ˆç®—çš„çµæœ
                if equity_curves.empty:
                    st.warning("æ²’æœ‰æ¬Šç›Šæ›²ç·šæ•¸æ“š")
                else:
                    # è¨ˆç®—ç›¸é—œæ€§çŸ©é™£
                    corr_matrix = equity_curves.corr()
                    corr_matrix.to_csv(RESULT_DIR / 'correlation_matrix.csv')
                    
                    # ä½¿ç”¨ä¸»é é¢è¨­å®šçš„ç›¸é—œæ€§ç¯„åœ
                    zmin, zmax = corr_range
                    
                    fig = px.imshow(
                        corr_matrix,
                        color_continuous_scale='RdBu_r',
                        zmin=zmin,
                        zmax=zmax,
                        text_auto=True,
                        title="ç­–ç•¥ç›¸é—œæ€§çŸ©é™£"
                    )
                    fig.update_layout(
                        width=2200,
                        height=1500,
                        xaxis_title="ç­–ç•¥",
                        yaxis_title="ç­–ç•¥",
                        coloraxis_colorbar_title="ç›¸é—œæ€§"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ·»åŠ ç›¸é—œæ€§çµ±è¨ˆä¿¡æ¯
                    st.subheader("ç›¸é—œæ€§çµ±è¨ˆ")
                    # ä½¿ç”¨numpyçš„triu_indicesä¾†ç²å–ä¸Šä¸‰è§’çŸ©é™£çš„å€¼
                    if corr_matrix is not None:
                        upper_triangle = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]
                        corr_stats = {
                            'å¹³å‡ç›¸é—œæ€§': upper_triangle.mean(),
                            'æœ€å¤§ç›¸é—œæ€§': upper_triangle.max(),
                            'æœ€å°ç›¸é—œæ€§': upper_triangle.min(),
                            'ç›¸é—œæ€§æ¨™æº–å·®': upper_triangle.std()
                        }
                        
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("å¹³å‡ç›¸é—œæ€§", f"{corr_stats['å¹³å‡ç›¸é—œæ€§']:.3f}")
                        with col2:
                            st.metric("æœ€å¤§ç›¸é—œæ€§", f"{corr_stats['æœ€å¤§ç›¸é—œæ€§']:.3f}")
                        with col3:
                            st.metric("æœ€å°ç›¸é—œæ€§", f"{corr_stats['æœ€å°ç›¸é—œæ€§']:.3f}")
                        with col4:
                            st.metric("ç›¸é—œæ€§æ¨™æº–å·®", f"{corr_stats['ç›¸é—œæ€§æ¨™æº–å·®']:.3f}")

            # æ¨™ç±¤é  2: å ±é…¬ç‡ç›¸é—œ
            with tabs[1]:
                st.subheader("èµ°æŸ¥æ™‚æ®µå ±é…¬ç‡åˆ†æ•¸ (%)")
                period_returns = {}
                period_mdd = {}
                period_pr = {}
                period_mdd_pr = {}
                period_hedge_counts = {}
                periods = WF_PERIODS if walk_forward_mode == "å›ºå®š WF_PERIODS" else generate_walk_forward_periods(equity_curves.index, n_splits)

                for i, period in enumerate([(p['test'] if isinstance(p, dict) else p) for p in periods]):
                    start = pd.to_datetime(period[0])
                    end = pd.to_datetime(period[1])
                    valid_dates = pd.to_datetime(equity_curves.index)
                    if start < valid_dates[0]:
                        adjusted_start = valid_dates[0]
                        logger.warning(f"èµ·å§‹æ—¥æœŸ {start.strftime('%Y-%m-%d')} æ—©æ–¼æ•¸æ“šé–‹å§‹ï¼Œèª¿æ•´ç‚º {adjusted_start.strftime('%Y-%m-%d')}")
                    else:
                        adjusted_start = valid_dates[valid_dates >= start][0]
                    if end > valid_dates[-1]:
                        adjusted_end = valid_dates[-1]
                        logger.warning(f"çµæŸæ—¥æœŸ {end.strftime('%Y-%m-%d')} æ™šæ–¼æ•¸æ“šçµæŸï¼Œèª¿æ•´ç‚º {adjusted_end.strftime('%Y-%m-%d')}")
                    else:
                        adjusted_end = valid_dates[valid_dates <= end][-1]
                    if (adjusted_end - adjusted_start).days < 30:
                        logger.warning(f"èµ°æŸ¥å€é–“ {adjusted_start.strftime('%Y-%m-%d')} è‡³ {adjusted_end.strftime('%Y-%m-%d')} éçŸ­ï¼Œè·³é")
                        continue
                    period_equity = equity_curves.loc[adjusted_start:adjusted_end]
                    for col in period_equity.columns:
                        if pd.isna(period_equity[col].iloc[0]):
                            period_equity[col].iloc[0] = initial_equity
                    # è¨ˆç®—å ±é…¬ç‡å’Œ MDD
                    period_return = (period_equity.iloc[-1] / period_equity.iloc[0] - 1) * 100
                    period_mdd_value = period_equity.apply(calculate_max_drawdown) * 100
                    # è¨ˆç®—é¿éšªæ©ç¢¼
                    hedge_mask = (period_return == 0) & (period_mdd_value == 0)
                    # è¨˜éŒ„é¿éšªæ¬¡æ•¸
                    if i > 0:  # éåˆå§‹æ™‚æ®µ
                        period_hedge_counts[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = hedge_mask.astype(int)
                    else:  # åˆå§‹æ™‚æ®µ
                        period_hedge_counts[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = pd.Series(0, index=hedge_mask.index)
                    # è¨ˆç®—å ±é…¬ç‡ PR å€¼
                    pr_values_ret = calculate_pr_values(period_return, is_mdd=False, is_initial_period=(i == 0), hedge_mask=hedge_mask)
                    period_pr[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = pr_values_ret
                    # å„²å­˜å ±é…¬ç‡
                    period_returns[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = period_return

                period_returns_df = pd.DataFrame(period_returns).T
                period_returns_df.to_csv(RESULT_DIR / 'period_returns.csv')
                st.dataframe(period_returns_df.style.format("{:.2f}%"))
                period_pr_df = pd.DataFrame(period_pr).T
                period_pr_df.to_csv(RESULT_DIR / 'period_pr.csv')
                st.subheader("èµ°æŸ¥æ™‚æ®µå ±é…¬ç‡åˆ†æ•¸ (%)")
                st.dataframe(period_pr_df.style.format("{:.2f}"))
                period_hedge_counts_df = pd.DataFrame(period_hedge_counts).T
                period_hedge_counts_df.to_csv(RESULT_DIR / 'period_hedge_counts.csv')
                st.subheader("èµ°æŸ¥æ™‚æ®µé¿éšªæ¬¡æ•¸")
                st.dataframe(period_hedge_counts_df)
                stress_returns = {}
                stress_pr = {}
                stress_hedge_counts = {}
                for i, (start, end) in enumerate(STRESS_PERIODS):
                    start = pd.to_datetime(start)
                    end = pd.to_datetime(end)
                    if start in equity_curves.index and end in equity_curves.index:
                        period_equity = equity_curves.loc[start:end]
                        for col in period_equity.columns:
                            if pd.isna(period_equity[col].iloc[0]):
                                period_equity[col].iloc[0] = initial_equity
                        # è¨ˆç®—å ±é…¬ç‡å’Œ MDD
                        period_return = (period_equity.iloc[-1] / period_equity.iloc[0] - 1) * 100
                        period_mdd_value = period_equity.apply(calculate_max_drawdown) * 100
                        # è¨ˆç®—é¿éšªæ©ç¢¼
                        hedge_mask = (period_return == 0) & (period_mdd_value == 0)
                        # è¨˜éŒ„é¿éšªæ¬¡æ•¸ï¼ˆå£“åŠ›æ™‚æ®µç¬¬ä¸€å€‹ç‚ºåˆå§‹æ™‚æ®µï¼‰
                        if i > 0:  # éåˆå§‹æ™‚æ®µ
                            stress_hedge_counts[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = hedge_mask.astype(int)
                        else:  # åˆå§‹æ™‚æ®µ
                            stress_hedge_counts[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = pd.Series(0, index=hedge_mask.index)
                        # è¨ˆç®—å ±é…¬ç‡ PR å€¼
                        pr_values_ret = calculate_pr_values(period_return, is_mdd=False, is_initial_period=(i == 0), hedge_mask=hedge_mask)
                        stress_pr[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = pr_values_ret
                        # å„²å­˜å ±é…¬ç‡
                        stress_returns[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = period_return

                stress_returns_df = pd.DataFrame(stress_returns).T
                stress_returns_df.to_csv(RESULT_DIR / 'stress_returns.csv')
                st.subheader("å£“åŠ›æ™‚æ®µå ±é…¬ç‡ (%)")
                st.dataframe(stress_returns_df.style.format("{:.2f}%"))
                stress_pr_df = pd.DataFrame(stress_pr).T
                stress_pr_df.to_csv(RESULT_DIR / 'stress_pr.csv')
                st.subheader("å£“åŠ›æ™‚æ®µå ±é…¬ç‡åˆ†æ•¸ (%)")
                st.dataframe(stress_pr_df.style.format("{:.2f}"))
                stress_hedge_counts_df = pd.DataFrame(stress_hedge_counts).T
                stress_hedge_counts_df.to_csv(RESULT_DIR / 'stress_hedge_counts.csv')
                st.subheader("å£“åŠ›æ™‚æ®µé¿éšªæ¬¡æ•¸")
                st.dataframe(stress_hedge_counts_df)

            # æ¨™ç±¤é  3: æœ€å¤§å›æ’¤ç›¸é—œ
            with tabs[2]:
                st.subheader("èµ°æŸ¥æ™‚æ®µæœ€å¤§å›æ’¤åˆ†æ•¸ (%)")
                period_mdd = {}
                period_mdd_pr = {}
                period_mdd_hedge_counts = {}
                periods = WF_PERIODS if walk_forward_mode == "å›ºå®š WF_PERIODS" else generate_walk_forward_periods(equity_curves.index, n_splits)
                for i, period in enumerate([(p['test'] if isinstance(p, dict) else p) for p in periods]):
                    start = pd.to_datetime(period[0])
                    end = pd.to_datetime(period[1])
                    valid_dates = pd.to_datetime(equity_curves.index)
                    if start < valid_dates[0]:
                        adjusted_start = valid_dates[0]
                    else:
                        adjusted_start = valid_dates[valid_dates >= start][0]
                    if end > valid_dates[-1]:
                        adjusted_end = valid_dates[-1]
                    else:
                        adjusted_end = valid_dates[valid_dates <= end][-1]
                    if (adjusted_end - adjusted_start).days < 30:
                        continue
                    period_equity = equity_curves.loc[adjusted_start:adjusted_end]
                    for col in period_equity.columns:
                        if pd.isna(period_equity[col].iloc[0]):
                            period_equity[col].iloc[0] = initial_equity
                    # è¨ˆç®—å ±é…¬ç‡ï¼ˆç”¨æ–¼é¿éšªæ©ç¢¼ï¼‰
                    period_return = (period_equity.iloc[-1] / period_equity.iloc[0] - 1) * 100
                    # è¨ˆç®— MDD
                    period_mdd_value = period_equity.apply(calculate_max_drawdown) * 100
                    # è¨ˆç®—é¿éšªæ©ç¢¼
                    hedge_mask = (period_return == 0) & (period_mdd_value == 0)
                    # è¨˜éŒ„é¿éšªæ¬¡æ•¸
                    if i > 0:  # éåˆå§‹æ™‚æ®µ
                        period_mdd_hedge_counts[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = hedge_mask.astype(int)
                    else:  # åˆå§‹æ™‚æ®µ
                        period_mdd_hedge_counts[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = pd.Series(0, index=hedge_mask.index)
                    # è¨ˆç®— MDD åˆ†æ•¸
                    pr_values_mdd = calculate_pr_values(period_mdd_value, is_mdd=True, is_initial_period=(i == 0), hedge_mask=hedge_mask)
                    period_mdd_pr[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = pr_values_mdd
                    # å„²å­˜ MDD
                    period_mdd[(adjusted_start.strftime('%Y-%m-%d'), adjusted_end.strftime('%Y-%m-%d'))] = period_mdd_value

                period_mdd_df = pd.DataFrame(period_mdd).T
                period_mdd_df.to_csv(RESULT_DIR / 'period_mdd.csv')
                st.dataframe(period_mdd_df.style.format("{:.2f}%"))
                period_mdd_pr_df = pd.DataFrame(period_mdd_pr).T
                period_mdd_pr_df.to_csv(RESULT_DIR / 'period_mdd_pr.csv')
                st.subheader("èµ°æŸ¥æ™‚æ®µæœ€å¤§å›æ’¤åˆ†æ•¸ (%)")
                st.dataframe(period_mdd_pr_df.style.format("{:.2f}"))
                period_mdd_hedge_counts_df = pd.DataFrame(period_mdd_hedge_counts).T
                period_mdd_hedge_counts_df.to_csv(RESULT_DIR / 'period_mdd_hedge_counts.csv')
                st.subheader("èµ°æŸ¥æ™‚æ®µæœ€å¤§å›æ’¤é¿éšªæ¬¡æ•¸")
                st.dataframe(period_mdd_hedge_counts_df)


                stress_mdd = {}
                stress_mdd_pr = {}
                stress_mdd_hedge_counts = {}
                for i, (start, end) in enumerate(STRESS_PERIODS):
                    start = pd.to_datetime(start)
                    end = pd.to_datetime(end)
                    if start in equity_curves.index and end in equity_curves.index:
                        period_equity = equity_curves.loc[start:end]
                        for col in period_equity.columns:
                            if pd.isna(period_equity[col].iloc[0]):
                                period_equity[col].iloc[0] = initial_equity
                        # è¨ˆç®—å ±é…¬ç‡å’Œ MDD
                        period_return = (period_equity.iloc[-1] / period_equity.iloc[0] - 1) * 100
                        period_mdd_value = period_equity.apply(calculate_max_drawdown) * 100
                        # è¨ˆç®—é¿éšªæ©ç¢¼
                        hedge_mask = (period_return == 0) & (period_mdd_value == 0)
                        # è¨˜éŒ„é¿éšªæ¬¡æ•¸
                        if i > 0:  # éåˆå§‹æ™‚æ®µ
                            stress_mdd_hedge_counts[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = hedge_mask.astype(int)
                        else:  # åˆå§‹æ™‚æ®µ
                            stress_mdd_hedge_counts[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = pd.Series(0, index=hedge_mask.index)
                        # è¨ˆç®— MDD åˆ†æ•¸
                        pr_values_mdd = calculate_pr_values(period_mdd_value, is_mdd=True, is_initial_period=(i == 0), hedge_mask=hedge_mask)
                        stress_mdd_pr[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = pr_values_mdd
                        # å„²å­˜ MDD
                        stress_mdd[(start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d'))] = period_mdd_value

                stress_mdd_df = pd.DataFrame(stress_mdd).T
                stress_mdd_df.to_csv(RESULT_DIR / 'stress_mdd.csv')
                st.subheader("å£“åŠ›æ™‚æ®µæœ€å¤§å›æ’¤ (%)")
                st.dataframe(stress_mdd_df.style.format("{:.2f}%"))
                stress_mdd_pr_df = pd.DataFrame(stress_mdd_pr).T
                stress_mdd_pr_df.to_csv(RESULT_DIR / 'stress_mdd_pr.csv')
                st.subheader("å£“åŠ›æ™‚æ®µæœ€å¤§å›æ’¤åˆ†æ•¸ (%)")
                st.dataframe(stress_mdd_pr_df.style.format("{:.2f}"))
                stress_mdd_hedge_counts_df = pd.DataFrame(stress_mdd_hedge_counts).T
                stress_mdd_hedge_counts_df.to_csv(RESULT_DIR / 'stress_mdd_hedge_counts.csv')
                st.subheader("å£“åŠ›æ™‚æ®µæœ€å¤§å›æ’¤é¿éšªæ¬¡æ•¸")
                st.dataframe(stress_mdd_hedge_counts_df)

                mdd = {}
                for name, equity in equity_curves.items():
                    mdd[name] = calculate_max_drawdown(equity) * 100
                mdd_df = pd.Series(mdd)
                mdd_df.to_csv(RESULT_DIR / 'mdd.csv')

            # æ¨™ç±¤é  4: ç¸½çµ
            with tabs[3]:
                st.subheader("ç­–ç•¥åŒ¯ç¸½åˆ†æ")
                
                # é¡¯ç¤ºèªªæ˜æ°£æ³¡
                with st.expander("ğŸ“– åŒ¯ç¸½åˆ†æèªªæ˜", expanded=False):
                    st.markdown("""
                    **ç­–ç•¥åŒ¯ç¸½åˆ†æ**: ç¶œåˆè©•ä¼°å„ç­–ç•¥çš„è¡¨ç¾                
                    - é¢¨éšªèª¿æ•´å ±é…¬: å¤æ™®æ¯”*ç¸½å ±é…¬ç‡
                    - ç¶œåˆè©•åˆ†: sqrt(ç¸½å ±é…¬ç‡)*10*0.3+å¤æ™®å€¼*0.25+æœ€å¤§å›æ’¤*0.2+å‹ç‡*0.15+é¢¨éšªèª¿æ•´å ±é…¬*0.1-éæ“¬åˆåˆ†æ•¸*0.5
                    

                    """)
                
                # å‰µå»ºç­–ç•¥åŒ¯ç¸½è¡¨
                summary_data = []
                for name, result in results.items():
                    if 'metrics' in result and result['metrics']:
                        metrics = result['metrics']
                        
                        # è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œå ±é…¬ç‡
                        risk_adjusted_return = metrics.get('sharpe_ratio', 0) * metrics.get('total_return', 0)
                        
                        summary_data.append({
                            "ç­–ç•¥": name,
                            "ç¸½å ±é…¬ç‡": metrics.get('total_return', 0),
                            "å¹´åŒ–å ±é…¬ç‡": metrics.get('annual_return', 0),
                            "æœ€å¤§å›æ’¤": metrics.get('max_drawdown', 0),
                            "å¤æ™®å€¼": metrics.get('sharpe_ratio', 0),
                            "å¡ç‘ªå€¼": metrics.get('calmar_ratio', 0),
                            "äº¤æ˜“æ¬¡æ•¸": metrics.get('num_trades', 0),
                            "å‹ç‡": metrics.get('win_rate', 0),
                            "é¢¨éšªèª¿æ•´å ±é…¬": risk_adjusted_return
                        })
                
                if summary_data:
                    summary_df = pd.DataFrame(summary_data).set_index("ç­–ç•¥")
                    
                    # è¨ˆç®—ç¶œåˆè©•åˆ†ï¼ˆåŠ å…¥éæ“¬åˆæ‡²ç½°ï¼‰
                    # é¦–å…ˆè¨ˆç®—éæ“¬åˆåˆ†æ•¸
                    overfitting_scores = {}
                    for name, result in results.items():
                        if 'equity_curve' in result and not result['equity_curve'].empty:
                            equity_curve = result['equity_curve']
                            
                            # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æœŸé–“
                            split_point = int(len(equity_curve) * 0.7)
                            train_equity = equity_curve.iloc[:split_point]
                            test_equity = equity_curve.iloc[split_point:]
                            
                            if len(train_equity) > 30 and len(test_equity) > 30:
                                train_returns = train_equity.pct_change().dropna()
                                test_returns = test_equity.pct_change().dropna()
                                
                                # è¨ˆç®—å¹´åŒ–å ±é…¬ç‡
                                train_annual_return = train_returns.mean() * 252
                                test_annual_return = test_returns.mean() * 252
                                
                                # è¨ˆç®—å¤æ™®å€¼
                                train_sharpe = train_annual_return / (train_returns.std() * np.sqrt(252)) if train_returns.std() > 0 else 0
                                test_sharpe = test_annual_return / (test_returns.std() * np.sqrt(252)) if test_returns.std() > 0 else 0
                                
                                # è¨ˆç®—éæ“¬åˆæŒ‡æ¨™
                                sharpe_degradation = train_sharpe - test_sharpe
                                return_degradation = train_annual_return - test_annual_return
                                
                                # éæ“¬åˆåˆ†æ•¸ï¼ˆ0-100ï¼Œè¶Šé«˜è¶Šéæ“¬åˆï¼‰
                                sharpe_weight = 0.6
                                return_weight = 0.4
                                overfitting_score = min(100, max(0, 
                                    abs(sharpe_degradation) * 50 * sharpe_weight + 
                                    abs(return_degradation) * 200 * return_weight
                                ))
                                overfitting_scores[name] = overfitting_score
                            else:
                                overfitting_scores[name] = 50  # é è¨­ä¸­ç­‰é¢¨éšª
                        else:
                            overfitting_scores[name] = 50  # é è¨­ä¸­ç­‰é¢¨éšª
                    
                    # è¨ˆç®—ç¶œåˆè©•åˆ†ï¼ˆæ‰£æ‰éæ“¬åˆåˆ†æ•¸*0.5ï¼‰
                    summary_df['ç¶œåˆè©•åˆ†'] = (
                        np.sqrt(summary_df['ç¸½å ±é…¬ç‡']) * 10 * 0.3 +
                        summary_df['å¤æ™®å€¼'] * 0.25 +
                        (1 + summary_df['æœ€å¤§å›æ’¤']) * 0.2 +
                        summary_df['å‹ç‡'] * 0.1 +
                        np.sqrt(summary_df['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05
                    )
                    print(f"ç¸½å ±é…¬ç‡åˆ†æ•¸: {np.sqrt(summary_df['ç¸½å ±é…¬ç‡']) * 10 * 0.3},å¤æ™®å€¼åˆ†æ•¸: {summary_df['å¤æ™®å€¼'] * 0.25},æœ€å¤§å›æ’¤åˆ†æ•¸: {(1 + summary_df['æœ€å¤§å›æ’¤']) * 0.2},å‹ç‡åˆ†æ•¸: {summary_df['å‹ç‡'] * 0.1},é¢¨éšªèª¿æ•´å ±é…¬åˆ†æ•¸: {np.sqrt(summary_df['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05}")
                    # æ‰£æ‰éæ“¬åˆåˆ†æ•¸æ‡²ç½°
                    for name in summary_df.index:
                        if name in overfitting_scores:
                            print(f"æ‰£æ‰éæ“¬åˆåˆ†æ•¸: {name} {overfitting_scores[name]}")
                            summary_df.loc[name, 'ç¶œåˆè©•åˆ†'] -= overfitting_scores[name] * 0.5
                    
                    # æŒ‰ç¶œåˆè©•åˆ†æ’åº
                    summary_df = summary_df.sort_values('ç¶œåˆè©•åˆ†', ascending=False)
                    
                    # é¡¯ç¤ºåŒ¯ç¸½è¡¨
                    st.subheader("ç­–ç•¥ç¸¾æ•ˆåŒ¯ç¸½è¡¨")
                    st.dataframe(summary_df.style.format({
                        'ç¸½å ±é…¬ç‡': '{:.2%}',
                        'å¹´åŒ–å ±é…¬ç‡': '{:.2%}',
                        'æœ€å¤§å›æ’¤': '{:.2%}',
                        'å¤æ™®å€¼': '{:.3f}',
                        'å¡ç‘ªå€¼': '{:.3f}',
                        'äº¤æ˜“æ¬¡æ•¸': '{:d}',
                        'å‹ç‡': '{:.2%}',
                        'é¢¨éšªèª¿æ•´å ±é…¬': '{:.3f}',
                        'ç¶œåˆè©•åˆ†': '{:.3f}'
                    }))
                    
                    # é¡¯ç¤ºæœ€ä½³ç­–ç•¥
                    if not summary_df.empty:
                        best_strategy = summary_df.index[0]
                        st.subheader(f"ğŸ† æœ€ä½³ç­–ç•¥: {best_strategy}")
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("å¹´åŒ–å ±é…¬ç‡", f"{summary_df.loc[best_strategy, 'å¹´åŒ–å ±é…¬ç‡']:.2%}")
                        with col2:
                            st.metric("å¤æ™®å€¼", f"{summary_df.loc[best_strategy, 'å¤æ™®å€¼']:.3f}")
                        with col3:
                            st.metric("æœ€å¤§å›æ’¤", f"{summary_df.loc[best_strategy, 'æœ€å¤§å›æ’¤']:.2%}")
                        
                        # ç­–ç•¥æ¯”è¼ƒåœ–
                        st.subheader("ç­–ç•¥æ¯”è¼ƒ")
                        
                        # å ±é…¬ç‡ vs é¢¨éšªæ•£é»åœ–
                        fig_scatter = px.scatter(
                            summary_df.reset_index(),
                            x='æœ€å¤§å›æ’¤',
                            y='å¹´åŒ–å ±é…¬ç‡',
                            size='å¤æ™®å€¼',
                            color='ç¶œåˆè©•åˆ†',
                            hover_name='ç­–ç•¥',
                            title="å ±é…¬ç‡ vs é¢¨éšªæ•£é»åœ–",
                            labels={'æœ€å¤§å›æ’¤': 'æœ€å¤§å›æ’¤ (%)', 'å¹´åŒ–å ±é…¬ç‡': 'å¹´åŒ–å ±é…¬ç‡ (%)'}
                        )
                        st.plotly_chart(fig_scatter, use_container_width=True)
                        
                        # ç­–ç•¥æ’ååœ–
                        fig_ranking = px.bar(
                            summary_df.reset_index(),
                            x='ç­–ç•¥',
                            y='ç¶œåˆè©•åˆ†',
                            title="ç­–ç•¥ç¶œåˆè©•åˆ†æ’å",
                            color='ç¶œåˆè©•åˆ†',
                            color_continuous_scale='viridis'
                        )
                        fig_ranking.update_xaxes(tickangle=45)
                        st.plotly_chart(fig_ranking, use_container_width=True)
                else:
                    st.warning("æ²’æœ‰å¯ç”¨çš„ç­–ç•¥æ•¸æ“šé€²è¡ŒåŒ¯ç¸½åˆ†æ")

            # æ¨™ç±¤é  5: éæ“¬åˆæª¢æ¸¬
            with tabs[4]:             
                # è¨ˆç®—éæ“¬åˆæŒ‡æ¨™
                overfitting_metrics = {}
                for name, result in results.items():
                    if 'equity_curve' in result and not result['equity_curve'].empty:
                        equity_curve = result['equity_curve']
                        
                        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æœŸé–“
                        split_point = int(len(equity_curve) * 0.7)
                        train_equity = equity_curve.iloc[:split_point]
                        test_equity = equity_curve.iloc[split_point:]
                        
                        if len(train_equity) > 30 and len(test_equity) > 30:
                            train_returns = train_equity.pct_change().dropna()
                            test_returns = test_equity.pct_change().dropna()
                            
                            # è¨ˆç®—å¹´åŒ–å ±é…¬ç‡
                            train_annual_return = train_returns.mean() * 252
                            test_annual_return = test_returns.mean() * 252
                            
                            # è¨ˆç®—å¤æ™®å€¼
                            train_sharpe = train_annual_return / (train_returns.std() * np.sqrt(252)) if train_returns.std() > 0 else 0
                            test_sharpe = test_annual_return / (test_returns.std() * np.sqrt(252)) if test_returns.std() > 0 else 0
                            
                            # è¨ˆç®—éæ“¬åˆæŒ‡æ¨™
                            sharpe_degradation = train_sharpe - test_sharpe
                            return_degradation = train_annual_return - test_annual_return
                            
                            # éæ“¬åˆåˆ†æ•¸ï¼ˆ0-100ï¼Œè¶Šé«˜è¶Šéæ“¬åˆï¼‰
                            # ä¿®æ­£è¨ˆç®—æ–¹å¼ï¼šè€ƒæ…®å¤æ™®å€¼å’Œå ±é…¬ç‡çš„ç›¸å°é‡è¦æ€§
                            sharpe_weight = 0.6
                            return_weight = 0.4
                            overfitting_score = min(100, max(0, 
                                abs(sharpe_degradation) * 50 * sharpe_weight + 
                                abs(return_degradation) * 200 * return_weight
                            ))
                            
                            overfitting_metrics[name] = {
                                'train_sharpe': train_sharpe,
                                'test_sharpe': test_sharpe,
                                'train_return': train_annual_return,
                                'test_return': test_annual_return,
                                'sharpe_degradation': sharpe_degradation,
                                'return_degradation': return_degradation,
                                'overfitting_score': overfitting_score
                            }
                
                if overfitting_metrics:
                    # éæ“¬åˆåˆ†æ•¸æ’å
                    overfitting_df = pd.DataFrame(overfitting_metrics).T
                    overfitting_df = overfitting_df.sort_values('overfitting_score')
                    
                    st.subheader("éæ“¬åˆåˆ†æ•¸æ’åï¼ˆè¶Šä½è¶Šå¥½ï¼‰")
                    
                    # ä½¿ç”¨èªªæ˜æ°£æ³¡é¡¯ç¤ºéæ“¬åˆæŒ‡æ¨™
                    display_df = overfitting_df[['overfitting_score', 'sharpe_degradation', 'return_degradation']].copy()
                    display_df.columns = ['éæ“¬åˆåˆ†æ•¸', 'å¤æ™®å€¼é€€åŒ–', 'å ±é…¬ç‡é€€åŒ–']
                    
                    # é¡¯ç¤ºèªªæ˜æ°£æ³¡
                    with st.expander("ğŸ“– éæ“¬åˆæŒ‡æ¨™èªªæ˜", expanded=False):
                        st.markdown("""
                        **éæ“¬åˆåˆ†æ•¸**: 0-100ï¼Œåˆ†æ•¸è¶Šé«˜è¡¨ç¤ºéæ“¬åˆé¢¨éšªè¶Šå¤§
                        - 0-30: ä½é¢¨éšªï¼Œç­–ç•¥è¼ƒç©©å®š
                        - 31-60: ä¸­é¢¨éšªï¼Œéœ€è¦é—œæ³¨
                        - 61-100: é«˜é¢¨éšªï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆå•é¡Œ
                        
                        **å¤æ™®å€¼é€€åŒ–**: æ¨£æœ¬å…§å¤–å¤æ™®å€¼çš„å·®ç•°ï¼Œè² å€¼è¡¨ç¤ºæ¨£æœ¬å¤–è¡¨ç¾æ›´å¥½
                        
                        **å ±é…¬ç‡é€€åŒ–**: æ¨£æœ¬å…§å¤–å¹´åŒ–å ±é…¬ç‡çš„å·®ç•°ï¼Œè² å€¼è¡¨ç¤ºæ¨£æœ¬å¤–è¡¨ç¾æ›´å¥½
                        """)
                    
                    st.dataframe(display_df.style.format({
                        'éæ“¬åˆåˆ†æ•¸': '{:.1f}',
                        'å¤æ™®å€¼é€€åŒ–': '{:.3f}',
                        'å ±é…¬ç‡é€€åŒ–': '{:.3f}'
                    }))
                    
                    # éæ“¬åˆåˆ†æ•¸åˆ†å¸ƒåœ–
                    fig_overfitting = px.bar(
                        overfitting_df.reset_index(), 
                        x='index', 
                        y='overfitting_score',
                        title="ç­–ç•¥éæ“¬åˆåˆ†æ•¸åˆ†å¸ƒ",
                        labels={'index': 'ç­–ç•¥', 'overfitting_score': 'éæ“¬åˆåˆ†æ•¸'}
                    )
                    fig_overfitting.add_hline(y=30, line_dash="dash", line_color="orange", annotation_text="è¼•å¾®éæ“¬åˆ")
                    fig_overfitting.add_hline(y=50, line_dash="dash", line_color="red", annotation_text="åš´é‡éæ“¬åˆ")
                    st.plotly_chart(fig_overfitting, use_container_width=True)
                    
                    # æ¨£æœ¬å…§å¤–è¡¨ç¾å°æ¯”
                    st.subheader("æ¨£æœ¬å…§å¤–è¡¨ç¾å°æ¯”")
                    
                    # å¤æ™®å€¼å°æ¯”
                    comparison_sharpe_df = overfitting_df[['train_sharpe', 'test_sharpe']].reset_index()
                    comparison_sharpe_df.columns = ['ç­–ç•¥', 'æ¨£æœ¬å…§å¤æ™®å€¼', 'æ¨£æœ¬å¤–å¤æ™®å€¼']
                    
                    with st.expander("ğŸ“– æ¨£æœ¬å…§å¤–æ¯”è¼ƒèªªæ˜", expanded=False):
                        st.markdown("""
                        **æ¨£æœ¬å…§å¤æ™®å€¼**: è¨“ç·´æœŸé–“ï¼ˆå‰70%æ•¸æ“šï¼‰çš„å¤æ™®å€¼
                        
                        **æ¨£æœ¬å¤–å¤æ™®å€¼**: æ¸¬è©¦æœŸé–“ï¼ˆå¾Œ30%æ•¸æ“šï¼‰çš„å¤æ™®å€¼
                        
                        ç†æƒ³æƒ…æ³ä¸‹ï¼Œæ¨£æœ¬å…§å¤–è¡¨ç¾æ‡‰è©²ç›¸è¿‘ï¼Œå·®ç•°éå¤§å¯èƒ½è¡¨ç¤ºéæ“¬åˆã€‚
                        """)
                    
                    fig_comparison_sharpe = px.bar(
                        comparison_sharpe_df,
                        x='ç­–ç•¥',
                        y=['æ¨£æœ¬å…§å¤æ™®å€¼', 'æ¨£æœ¬å¤–å¤æ™®å€¼'],
                        title="æ¨£æœ¬å…§å¤–å¤æ™®å€¼å°æ¯”",
                        barmode='group'
                    )
                    st.plotly_chart(fig_comparison_sharpe, use_container_width=True)
                    
                    # å ±é…¬ç‡å°æ¯”
                    comparison_return_df = overfitting_df[['train_return', 'test_return']].reset_index()
                    comparison_return_df.columns = ['ç­–ç•¥', 'æ¨£æœ¬å…§å¹´åŒ–å ±é…¬ç‡', 'æ¨£æœ¬å¤–å¹´åŒ–å ±é…¬ç‡']
                    
                    fig_comparison_return = px.bar(
                        comparison_return_df,
                        x='ç­–ç•¥',
                        y=['æ¨£æœ¬å…§å¹´åŒ–å ±é…¬ç‡', 'æ¨£æœ¬å¤–å¹´åŒ–å ±é…¬ç‡'],
                        title="æ¨£æœ¬å…§å¤–å¹´åŒ–å ±é…¬ç‡å°æ¯”",
                        barmode='group'
                    )
                    st.plotly_chart(fig_comparison_return, use_container_width=True)
                else:
                    st.warning("ç„¡æ³•è¨ˆç®—éæ“¬åˆæŒ‡æ¨™ï¼Œè«‹ç¢ºä¿æœ‰è¶³å¤ çš„æ•¸æ“šé€²è¡Œæ¨£æœ¬å…§å¤–åˆ†å‰²")
                
                # ç­–ç•¥ç©©å®šæ€§åˆ†æ
                st.subheader("ç­–ç•¥ç©©å®šæ€§åˆ†æ")
                # å¾æ¬Šç›Šæ›²ç·šè¨ˆç®—æœŸé–“å ±é…¬ç‡
                period_returns = {}
                for name, result in results.items():
                    if 'equity_curve' in result and not result['equity_curve'].empty:
                        equity_curve = result['equity_curve']
                        # è¨ˆç®—æœˆåº¦å ±é…¬ç‡
                        monthly_returns = equity_curve.resample('M').last().pct_change().dropna() * 100
                        period_returns[name] = monthly_returns
                        logger.info(f"ç­–ç•¥ {name} æœˆåº¦å ±é…¬ç‡: {len(monthly_returns)} å€‹æœŸé–“")
                
                logger.info(f"é–‹å§‹è¨ˆç®—ç©©å®šæ€§æŒ‡æ¨™ï¼Œå…± {len(period_returns)} å€‹ç­–ç•¥")
                
                # é¡¯ç¤ºæœŸé–“å ±é…¬ç‡çš„åŸºæœ¬ä¿¡æ¯
                for name, returns in period_returns.items():
                    logger.info(f"ç­–ç•¥ {name}: {len(returns)} å€‹æœŸé–“, ç¯„åœ: {returns.index.min()} åˆ° {returns.index.max()}")
                
                stability_metrics = calculate_strategy_stability(period_returns)
                
                if stability_metrics:
                    stability_df = pd.DataFrame(stability_metrics).T
                    
                    # é‡å‘½åæ¬„ä½ç‚ºä¸­æ–‡
                    stability_df.columns = ['å¹³å‡å ±é…¬ç‡', 'å ±é…¬ç‡æ¨™æº–å·®', 'è®Šç•°ä¿‚æ•¸', 'æ­£å ±é…¬æœŸé–“æ¯”ä¾‹', 'æ’åç©©å®šæ€§']
                    
                    logger.info(f"ç©©å®šæ€§æŒ‡æ¨™è¨ˆç®—å®Œæˆï¼Œå…± {len(stability_df)} å€‹ç­–ç•¥")
                    
                    with st.expander("ğŸ“– ç©©å®šæ€§æŒ‡æ¨™èªªæ˜", expanded=False):
                        st.markdown("""
                        **å¹³å‡å ±é…¬ç‡**: å„æœŸé–“çš„å¹³å‡å ±é…¬ç‡
                        
                        **å ±é…¬ç‡æ¨™æº–å·®**: å„æœŸé–“å ±é…¬ç‡çš„è®Šç•°ç¨‹åº¦ï¼Œè¶Šå°è¶Šç©©å®š
                        
                        **è®Šç•°ä¿‚æ•¸**: æ¨™æº–å·®èˆ‡å¹³å‡å€¼çš„æ¯”å€¼ï¼Œè¡¡é‡ç›¸å°è®Šç•°æ€§ï¼Œè¶Šå°è¶Šç©©å®š
                        
                        **æ­£å ±é…¬æœŸé–“æ¯”ä¾‹**: ç”¢ç”Ÿæ­£å ±é…¬çš„æœŸé–“ä½”ç¸½æœŸé–“çš„æ¯”ä¾‹ï¼Œè¶Šé«˜è¶Šå¥½
                        
                        **æ’åç©©å®šæ€§**: ç­–ç•¥åœ¨ä¸åŒæœŸé–“çš„æ’åç©©å®šæ€§ï¼ŒåŸºæ–¼æ’åè®Šç•°ä¿‚æ•¸è¨ˆç®—ï¼Œè¶Šæ¥è¿‘1è¡¨ç¤ºæ’åè¶Šç©©å®š
                        """)
                    
                    # è™•ç†ç„¡é™å€¼å’Œç•°å¸¸å€¼
                    def format_stability_value(val, format_type):
                        if pd.isna(val) or val == float('inf') or val == float('-inf'):
                            return 'N/A'
                        if format_type == 'percent':
                            return f"{val:.2%}"
                        elif format_type == 'decimal':
                            return f"{val:.2f}"
                        elif format_type == 'correlation':
                            return f"{val:.3f}"
                        else:
                            return str(val)
                    
                    # å‰µå»ºæ ¼å¼åŒ–çš„DataFrame
                    formatted_df = stability_df.copy()
                    formatted_df['å¹³å‡å ±é…¬ç‡'] = formatted_df['å¹³å‡å ±é…¬ç‡'].apply(lambda x: format_stability_value(x, 'percent'))
                    formatted_df['å ±é…¬ç‡æ¨™æº–å·®'] = formatted_df['å ±é…¬ç‡æ¨™æº–å·®'].apply(lambda x: format_stability_value(x, 'percent'))
                    formatted_df['è®Šç•°ä¿‚æ•¸'] = formatted_df['è®Šç•°ä¿‚æ•¸'].apply(lambda x: format_stability_value(x, 'decimal'))
                    formatted_df['æ­£å ±é…¬æœŸé–“æ¯”ä¾‹'] = formatted_df['æ­£å ±é…¬æœŸé–“æ¯”ä¾‹'].apply(lambda x: format_stability_value(x, 'percent'))
                    formatted_df['æ’åç©©å®šæ€§'] = formatted_df['æ’åç©©å®šæ€§'].apply(lambda x: format_stability_value(x, 'correlation'))
                    
                    st.dataframe(formatted_df)
                    
                    # ç©©å®šæ€§ç†±åŠ›åœ–ï¼ˆåªé¡¯ç¤ºæœ‰æ•ˆçš„æ•¸å€¼ï¼‰
                    if len(stability_df) > 1:
                        # éæ¿¾æ‰ç„¡é™å€¼å’ŒNaNå€¼
                        heatmap_data = stability_df[['è®Šç•°ä¿‚æ•¸', 'æ­£å ±é…¬æœŸé–“æ¯”ä¾‹', 'æ’åç©©å®šæ€§']].copy()
                        heatmap_data = heatmap_data.replace([float('inf'), float('-inf')], np.nan)
                        heatmap_data = heatmap_data.dropna(how='all')
                        
                        if not heatmap_data.empty:
                            fig_stability = px.imshow(
                                heatmap_data.T,
                                title="ç­–ç•¥ç©©å®šæ€§æŒ‡æ¨™ç†±åŠ›åœ–",
                                aspect="auto"
                            )
                            st.plotly_chart(fig_stability, use_container_width=True)
                        else:
                            st.warning("ç„¡æ³•ç”Ÿæˆç©©å®šæ€§ç†±åŠ›åœ–ï¼Œæ•¸æ“šåŒ…å«éå¤šç„¡æ•ˆå€¼")
                else:
                    st.warning("ç„¡æ³•è¨ˆç®—ç©©å®šæ€§æŒ‡æ¨™ï¼Œè«‹ç¢ºä¿æœ‰è¶³å¤ çš„æœŸé–“æ•¸æ“š")

            # é¡¯ç¤ºç¸½é«”å›æ¸¬çµæœ
            st.subheader("ç­–ç•¥æ¬Šç›Šæ›²ç·š")

            equity_curves_list = []
            for name, result in results.items():
                if 'equity_curve' in result and not result['equity_curve'].empty:
                    equity_curve = result['equity_curve'].rename(name)
                    equity_curves_list.append(equity_curve)

            if not equity_curves_list:
                st.warning("æ²’æœ‰å¯ç”¨çš„æ¬Šç›Šæ›²ç·šæ•¸æ“šé€²è¡Œç¹ªè£½ã€‚")
                logger.warning("æ²’æœ‰å¯ç”¨çš„æ¬Šç›Šæ›²ç·šæ•¸æ“šé€²è¡Œç¹ªè£½ã€‚")
            else:
                all_equity_curves = pd.concat(equity_curves_list, axis=1)
                all_equity_curves = all_equity_curves.reindex(global_index).ffill()
                all_equity_curves.ffill(inplace=True)
                all_equity_curves.fillna(initial_equity, inplace=True)

                df_plot = all_equity_curves.reset_index().melt(id_vars='index', var_name='ç­–ç•¥', value_name='æ¬Šç›Š')
                df_plot.rename(columns={'index': 'æ—¥æœŸ'}, inplace=True)
                
                fig_equity = px.line(df_plot, x='æ—¥æœŸ', y='æ¬Šç›Š', color='ç­–ç•¥', title="ç­–ç•¥æ¬Šç›Šæ›²ç·š")
                fig_equity.update_layout(legend_title_text='variable')
                st.plotly_chart(fig_equity, use_container_width=True, key="total_equity_curve")

            # é¡¯ç¤ºåŒ¯ç¸½æŒ‡æ¨™
            st.subheader("ç­–ç•¥åŒ¯ç¸½æŒ‡æ¨™")
            summary_data = []
            for name, result in results.items():
                if 'metrics' in result and result['metrics']:
                    metrics = result['metrics']
                    
                    # æ·»åŠ èª¿è©¦ä¿¡æ¯
                    win_rate_value = metrics.get('win_rate', 0)
                    logger.info(f"ç­–ç•¥ {name} çš„å‹ç‡å€¼: {win_rate_value}, é¡å‹: {type(win_rate_value)}")
                    
                    row = {
                        "ç­–ç•¥": name,
                        "ç¸½å ±é…¬ç‡": metrics.get('total_return', 0),
                        "å¹´åŒ–å ±é…¬ç‡": metrics.get('annual_return', 0),
                        "æœ€å¤§å›æ’¤": metrics.get('max_drawdown', 0),
                        "å¤æ™®å€¼": metrics.get('sharpe_ratio', 0),
                        "å¡ç‘ªå€¼": metrics.get('calmar_ratio', 0),
                        "äº¤æ˜“æ¬¡æ•¸": metrics.get('num_trades', 0),
                        "å‹ç‡": win_rate_value
                    }
                    summary_data.append(row)
            
            if summary_data:
                summary_df = pd.DataFrame(summary_data).set_index("ç­–ç•¥")
                
                # é¡¯ç¤ºèªªæ˜æ°£æ³¡
                with st.expander("ğŸ“– ç¸¾æ•ˆæŒ‡æ¨™èªªæ˜", expanded=False):
                    st.markdown("""
                    **ç¸½å ±é…¬ç‡**: æ•´å€‹å›æ¸¬æœŸé–“çš„ç´¯ç©å ±é…¬ç‡
                    
                    **å¹´åŒ–å ±é…¬ç‡**: å°‡ç¸½å ±é…¬ç‡è½‰æ›ç‚ºå¹´åŒ–æ¨™æº–ï¼Œä¾¿æ–¼æ¯”è¼ƒä¸åŒæœŸé–“çš„ç­–ç•¥
                    
                    **æœ€å¤§å›æ’¤**: æ¬Šç›Šæ›²ç·šå¾å³°å€¼åˆ°è°·å€¼çš„æœ€å¤§è·Œå¹…ï¼Œè¡¡é‡ä¸‹è¡Œé¢¨éšª
                    
                    **å¤æ™®å€¼**: è¶…é¡å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬
                    
                    **å¡ç‘ªå€¼**: å¹´åŒ–å ±é…¬ç‡èˆ‡æœ€å¤§å›æ’¤çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬
                    
                    **äº¤æ˜“æ¬¡æ•¸**: æ•´å€‹å›æ¸¬æœŸé–“çš„ç¸½äº¤æ˜“æ¬¡æ•¸
                    
                    **å‹ç‡**: ç²åˆ©äº¤æ˜“æ¬¡æ•¸ä½”ç¸½äº¤æ˜“æ¬¡æ•¸çš„æ¯”ä¾‹
                    """)
                
                # é¡¯ç¤ºè¡¨æ ¼
                st.dataframe(summary_df.style.format({
                    "ç¸½å ±é…¬ç‡": "{:.2%}", "å¹´åŒ–å ±é…¬ç‡": "{:.2%}", "æœ€å¤§å›æ’¤": "{:.2%}",
                    "å¤æ™®å€¼": "{:.2f}", "å¡ç‘ªå€¼": "{:.2f}", "å‹ç‡": "{:.2%}"
                }))
            else:
                st.warning("æ²’æœ‰å¯ç”¨çš„åŒ¯ç¸½æŒ‡æ¨™æ•¸æ“šã€‚")

            # è’™åœ°å¡ç¾…æ¸¬è©¦å»ºè­°
            st.info("è’™åœ°å¡ç¾…æ¸¬è©¦ï¼šè«‹åƒè€ƒ Optuna_12.py ä¸­çš„ compute_pbo_score å’Œ compute_simplified_sra å‡½æ•¸å¯¦ç¾ PBO åˆ†æ•¸èˆ‡ SRA p å€¼è¨ˆç®—")

            # è¼‰å…¥å’ŒåŸ·è¡Œä¿¡æ¯
            with st.expander("ğŸ“‹ åŸ·è¡Œä¿¡æ¯", expanded=False):            
                st.info(f"è¼‰å…¥ {len(strategies_to_run)} å€‹ç­–ç•¥ï¼Œå…± {len(results)} å€‹è©¦é©—")
                
                # é¡¯ç¤ºé¸ä¸­çš„ç­–ç•¥ä¿¡æ¯
                st.subheader("å·²é¸æ“‡çš„ç­–ç•¥:")
                for strategy in strategies_to_run:
                    st.text(f"â€¢ {strategy['name']} ({strategy['strategy_type']})")
                
                # é¡¯ç¤ºç­–ç•¥çµ±è¨ˆ
                strategy_counts = {}
                for strategy in strategies_to_run:
                    strategy_type = strategy['strategy_type']
                    data_source = strategy['smaa_source']
                    key = f"{strategy_type}_{data_source}"
                    strategy_counts[key] = strategy_counts.get(key, 0) + 1
                
                st.subheader("ç­–ç•¥åˆ†å¸ƒ:")
                for key, count in strategy_counts.items():
                    st.text(f"â€¢ {key}: {count} å€‹ç­–ç•¥")
                
                # é¡¯ç¤ºåŸ·è¡Œä¿¡æ¯
                st.subheader("åŸ·è¡Œä¿¡æ¯")
                st.text(f"â€¢ é¸ä¸­ç­–ç•¥æ•¸é‡: {len(strategies_to_run)}")
                st.text(f"â€¢ å¯¦éš›åŸ·è¡Œç­–ç•¥: {len(results)}")
                st.text(f"â€¢ æ¬Šç›Šæ›²ç·šæ•¸æ“šé»: {len(equity_curves) if not equity_curves.empty else 0}")

            # æ¨™ç±¤é  6: Topç­–ç•¥åƒæ•¸
            with tabs[5]:
                st.subheader("Topç­–ç•¥åƒæ•¸è©³æƒ… (åƒ…Optunaç­–ç•¥)")
                
                # è¨­å®šé¡¯ç¤ºçš„Top Nç­–ç•¥æ•¸é‡
                top_n = st.slider("å„ç­–ç•¥é¡å‹é¡¯ç¤ºå‰Nå", min_value=1, max_value=20, value=10, help="é¸æ“‡æ¯å€‹ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆè¦é¡¯ç¤ºçš„Top Nç­–ç•¥æ•¸é‡")
                
                # ç›´æ¥å¾optuna_resultsä¸­ç²å–æ‰€æœ‰ç­–ç•¥ï¼ŒæŒ‰ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºåˆ†çµ„
                if not optuna_results.empty:
                    # è¨ˆç®—ç¶œåˆè©•åˆ†ï¼ˆå¦‚æœé‚„æ²’è¨ˆç®—çš„è©±ï¼‰
                    if 'ç¶œåˆè©•åˆ†' not in optuna_results.columns:
                        # è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œå ±é…¬ç‡
                        optuna_results['é¢¨éšªèª¿æ•´å ±é…¬'] = optuna_results['sharpe_ratio'] * optuna_results['total_return']
                        
                        # è¨ˆç®—ç¶œåˆè©•åˆ†ï¼ˆè™•ç†ç¼ºå¤±æ¬„ä½ï¼‰
                        optuna_results['ç¶œåˆè©•åˆ†'] = (
                            np.sqrt(optuna_results['total_return']) * 10 * 0.3 +
                            optuna_results['sharpe_ratio'] * 0.25 +
                            (1 + optuna_results['max_drawdown']) * 0.2 +
                            (optuna_results.get('win_rate', 0.5) * 0.1) +  # å¦‚æœæ²’æœ‰å‹ç‡ï¼Œä½¿ç”¨0.5ä½œç‚ºé è¨­å€¼
                            np.sqrt(optuna_results['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05
                        )
                    
                    # æŒ‰ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºåˆ†çµ„ä¸¦å–å„çµ„åˆçš„å‰Nå
                    top_strategies_by_group = {}
                    
                    # ç›´æ¥å¾optuna_resultsç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„çµ„åˆï¼ˆä½¿ç”¨tupleï¼‰
                    strategy_groups = set()
                    for _, row in optuna_results.iterrows():
                        strategy_type = row['strategy']
                        data_source = row['data_source']
                        group_key = (strategy_type, data_source)
                        strategy_groups.add(group_key)
                    
                    st.write(f"**å¾optuna_resultsç”Ÿæˆçš„çµ„åˆ:** {list(strategy_groups)}")
                    
                    # ç‰¹åˆ¥æ’æŸ¥ssma_turn
                    st.subheader("ğŸ” ssma_turn è©³ç´°æ’æŸ¥")
                    ssma_turn_data = optuna_results[optuna_results['strategy'] == 'ssma_turn']
                    st.write(f"**ssma_turnç¸½æ•¸æ“šé‡:** {len(ssma_turn_data)}")
                    
                    if not ssma_turn_data.empty:
                        st.write("**ssma_turnæ•¸æ“šæºåˆ†å¸ƒ:**")
                        ssma_sources = ssma_turn_data['data_source'].value_counts()
                        for source, count in ssma_sources.items():
                            st.write(f"â€¢ {source}: {count} å€‹ç­–ç•¥")
                        
                        # æª¢æŸ¥ssma_turnçš„æ¬„ä½
                        st.write("**ssma_turnæ¬„ä½æª¢æŸ¥:**")
                        required_fields = ['total_return', 'sharpe_ratio', 'max_drawdown', 'ç¶œåˆè©•åˆ†']
                        for field in required_fields:
                            if field in ssma_turn_data.columns:
                                valid_count = ssma_turn_data[field].notna().sum()
                                st.write(f"â€¢ {field}: {valid_count}/{len(ssma_turn_data)} æœ‰æ•ˆå€¼")
                            else:
                                st.write(f"â€¢ {field}: æ¬„ä½ä¸å­˜åœ¨")
                        
                        # æª¢æŸ¥ssma_turnçš„ç¶œåˆè©•åˆ†
                        if 'ç¶œåˆè©•åˆ†' in ssma_turn_data.columns:
                            st.write("**ssma_turnç¶œåˆè©•åˆ†çµ±è¨ˆ:**")
                            st.write(f"â€¢ æœ€å°å€¼: {ssma_turn_data['ç¶œåˆè©•åˆ†'].min():.3f}")
                            st.write(f"â€¢ æœ€å¤§å€¼: {ssma_turn_data['ç¶œåˆè©•åˆ†'].max():.3f}")
                            st.write(f"â€¢ å¹³å‡å€¼: {ssma_turn_data['ç¶œåˆè©•åˆ†'].mean():.3f}")
                    
                    # ç‚ºæ¯å€‹ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆæ‰¾åˆ°å°æ‡‰çš„ç­–ç•¥
                    for group_key in strategy_groups:
                        strategy_type, data_source = group_key
                        
                        # ç›´æ¥å¾optuna_resultsä¸­ç¯©é¸è©²çµ„åˆçš„æ‰€æœ‰ç­–ç•¥
                        mask = (optuna_results['strategy'] == strategy_type) & (optuna_results['data_source'] == data_source)
                        group_df = optuna_results[mask].copy()
                        
                        st.write(f"**èª¿è©¦ {strategy_type}_{data_source}:** æ‰¾åˆ° {len(group_df)} å€‹ç­–ç•¥")
                        
                        # ç‰¹åˆ¥èª¿è©¦ssma_turnçš„ç¯©é¸
                        if 'ssma_turn' in strategy_type:
                            st.write(f"  **ssma_turnèª¿è©¦:**")
                            st.write(f"  - ç¯©é¸æ¢ä»¶: strategy == '{strategy_type}' AND data_source == '{data_source}'")
                            
                            # æª¢æŸ¥strategyæ¬„ä½çš„å”¯ä¸€å€¼
                            unique_strategies = optuna_results['strategy'].unique()
                            st.write(f"  - optuna_resultsä¸­strategyæ¬„ä½çš„å”¯ä¸€å€¼: {list(unique_strategies)}")
                            
                            # æª¢æŸ¥data_sourceæ¬„ä½çš„å”¯ä¸€å€¼
                            unique_sources = optuna_results['data_source'].unique()
                            st.write(f"  - optuna_resultsä¸­data_sourceæ¬„ä½çš„å”¯ä¸€å€¼: {list(unique_sources)}")
                            
                            # åˆ†åˆ¥æª¢æŸ¥å…©å€‹æ¢ä»¶
                            strategy_mask = optuna_results['strategy'] == strategy_type
                            source_mask = optuna_results['data_source'] == data_source
                            st.write(f"  - strategy == '{strategy_type}' çš„çµæœ: {strategy_mask.sum()} å€‹")
                            st.write(f"  - data_source == '{data_source}' çš„çµæœ: {source_mask.sum()} å€‹")
                            st.write(f"  - å…©å€‹æ¢ä»¶ANDçš„çµæœ: {(strategy_mask & source_mask).sum()} å€‹")
                        
                        if not group_df.empty:
                            # æª¢æŸ¥æ˜¯å¦æœ‰fine_grained_clusteræ¬„ä½
                            if 'fine_grained_cluster' in group_df.columns:
                                st.write(f"  **ä½¿ç”¨fine_grained_clusteré€²è¡Œå¤šæ¨£æ€§åˆ†çµ„**")
                                
                                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä¾›pick_topN_by_diversityä½¿ç”¨
                                trials_dict = group_df.to_dict('records')
                                
                                # ä½¿ç”¨pick_topN_by_diversityé€²è¡Œå¤šæ¨£æ€§ç¯©é¸
                                diverse_trials = pick_topN_by_diversity(
                                    trials_dict, 
                                    metric_keys=['num_trades'],  # ä½¿ç”¨åŸºæœ¬æŒ‡æ¨™
                                    top_n=top_n
                                )
                                
                                # è½‰æ›å›DataFrame
                                if diverse_trials:
                                    top_group_strategies = pd.DataFrame(diverse_trials)
                                    st.write(f"  â†’ ä½¿ç”¨fine_grained_clusteré¸å–å‰ {len(top_group_strategies)} å")
                                else:
                                    # å¦‚æœå¤šæ¨£æ€§ç¯©é¸å¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬æ’åº
                                    top_group_strategies = group_df.sort_values('ç¶œåˆè©•åˆ†', ascending=False).head(top_n)
                                    st.write(f"  â†’ å¤šæ¨£æ€§ç¯©é¸å¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬æ’åºé¸å–å‰ {len(top_group_strategies)} å")
                            else:
                                # æ²’æœ‰fine_grained_clusteræ¬„ä½ï¼Œä½¿ç”¨åŸºæœ¬æ’åº
                                top_group_strategies = group_df.sort_values('ç¶œåˆè©•åˆ†', ascending=False).head(top_n)
                                st.write(f"  â†’ ç„¡fine_grained_clusteræ¬„ä½ï¼Œä½¿ç”¨åŸºæœ¬æ’åºé¸å–å‰ {len(top_group_strategies)} å")
                            
                            top_strategies_by_group[group_key] = top_group_strategies
                        else:
                            st.write(f"  â†’ æ²’æœ‰æ‰¾åˆ°ç­–ç•¥")

                    
                    st.info(f"é¡¯ç¤ºå„ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆå‰ {top_n} åçš„Optunaç­–ç•¥åƒæ•¸")
                    
                    # é¡¯ç¤ºç­–ç•¥åˆ†å¸ƒèª¿è©¦ä¿¡æ¯
                    st.subheader("ğŸ“Š ç­–ç•¥åˆ†å¸ƒèª¿è©¦ä¿¡æ¯")
                    strategy_counts = optuna_results['strategy'].value_counts()
                    st.write("**ç­–ç•¥é¡å‹åˆ†å¸ƒ:**")
                    for strategy, count in strategy_counts.items():
                        st.write(f"â€¢ {strategy}: {count} å€‹ç­–ç•¥")
                    
                    data_source_counts = optuna_results['data_source'].value_counts()
                    st.write("**æ•¸æ“šæºåˆ†å¸ƒ:**")
                    for data_source, count in data_source_counts.items():
                        st.write(f"â€¢ {data_source}: {count} å€‹ç­–ç•¥")
                    
                    # é¡¯ç¤ºç­–ç•¥+æ•¸æ“šæºçµ„åˆåˆ†å¸ƒ
                    st.write("**ç­–ç•¥+æ•¸æ“šæºçµ„åˆåˆ†å¸ƒ:**")
                    group_counts = optuna_results.groupby(['strategy', 'data_source']).size()
                    for (strategy, data_source), count in group_counts.items():
                        st.write(f"â€¢ {strategy}_{data_source}: {count} å€‹ç­–ç•¥")
                    
                    # é¡¯ç¤ºå„ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆçš„ç¸¾æ•ˆæ‘˜è¦
                    for group_key, top_group_strategies in top_strategies_by_group.items():
                        strategy_type, data_source = group_key
                        st.subheader(f"{strategy_type} - {data_source} ç­–ç•¥ç¸¾æ•ˆæ‘˜è¦ (å‰{top_n}å)")
                        
                        # æº–å‚™ç¸¾æ•ˆæ‘˜è¦æ•¸æ“š
                        performance_summary = top_group_strategies[['total_return', 'sharpe_ratio', 'max_drawdown', 'ç¶œåˆè©•åˆ†']].copy()
                        performance_summary.columns = ['ç¸½å ±é…¬ç‡', 'å¤æ™®å€¼', 'æœ€å¤§å›æ’¤', 'ç¶œåˆè©•åˆ†']
                        
                        # æ·»åŠ å…¶ä»–ç¸¾æ•ˆæŒ‡æ¨™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if 'num_trades' in top_group_strategies.columns:
                            performance_summary['äº¤æ˜“æ¬¡æ•¸'] = top_group_strategies['num_trades']
                        if 'win_rate' in top_group_strategies.columns:
                            performance_summary['å‹ç‡'] = top_group_strategies['win_rate']
                        if 'profit_factor' in top_group_strategies.columns:
                            performance_summary['ç²åˆ©å› å­'] = top_group_strategies['profit_factor']
                        
                        st.dataframe(performance_summary.style.format({
                            'ç¸½å ±é…¬ç‡': '{:.2%}',
                            'å¤æ™®å€¼': '{:.3f}',
                            'æœ€å¤§å›æ’¤': '{:.2%}',
                            'ç¶œåˆè©•åˆ†': '{:.3f}',
                            'å‹ç‡': '{:.2%}',
                            'ç²åˆ©å› å­': '{:.3f}'
                        }))
                    
                    # ç”Ÿæˆåƒæ•¸å­—å…¸æ ¼å¼
                    st.subheader("åƒæ•¸å­—å…¸æ ¼å¼ï¼ˆå¯è¤‡è£½åˆ°SSSçš„param_presetsï¼‰")
                    
                    # å‰µå»ºåƒæ•¸å­—å…¸
                    param_dict = {}
                    
                    # éæ­·æ‰€æœ‰ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆ
                    for group_key, top_group_strategies in top_strategies_by_group.items():
                        for _, row in top_group_strategies.iterrows():
                            # ç²å–ç­–ç•¥ä¿¡æ¯
                            strategy_type = row['strategy']
                            data_source = row['data_source']
                            trial_number = row['trial_number']
                            
                            # ç”Ÿæˆç­–ç•¥åç¨±
                            strategy_name = f"{strategy_type}_{data_source}_{trial_number}"
                            
                            # ç²å–åƒæ•¸
                            params = row['parameters']
                            
                            # è™•ç†åƒæ•¸æ ¼å¼
                            if isinstance(params, dict) and 'parameters' in params:
                                actual_params = params['parameters']
                            elif isinstance(params, str):
                                try:
                                    import json
                                    actual_params = json.loads(params)
                                except json.JSONDecodeError:
                                    actual_params = {}
                            else:
                                actual_params = params
                            
                            # æ·»åŠ ç­–ç•¥é¡å‹å’Œæ•°æ®æº
                            if isinstance(actual_params, dict):
                                actual_params['strategy_type'] = strategy_type
                                actual_params['smaa_source'] = data_source
                                
                                # ç”Ÿæˆç­–ç•¥åç¨±ï¼ˆåŒ…å«ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºä¿¡æ¯ï¼‰
                                clean_name = f"{strategy_type}_{strategy_name.replace('_', '').replace('-', '')}"
                                param_dict[clean_name] = actual_params
                    
                    # é¡¯ç¤ºåƒæ•¸å­—å…¸
                    if param_dict:
                        # æ ¼å¼åŒ–ç‚ºPythonå­—å…¸æ ¼å¼
                        param_str = "param_presets = {\n"
                        for name, params in param_dict.items():
                            param_str += f'    "{name}": {params},\n'
                        param_str += "}"
                        
                        # é¡¯ç¤ºå¯è¤‡è£½çš„ä»£ç¢¼
                        st.code(param_str, language='python')
                        
                        # æä¾›ä¸‹è¼‰æŒ‰éˆ•
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è¼‰åƒæ•¸å­—å…¸",
                            data=param_str,
                            file_name=f"top_{top_n}_optuna_strategies_params.py",
                            mime="text/plain"
                        )
                        
                        # é¡¯ç¤ºåƒæ•¸è©³æƒ…è¡¨æ ¼ï¼ˆæŒ‰ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºåˆ†çµ„ï¼‰
                        st.subheader("åƒæ•¸è©³æƒ…è¡¨æ ¼ï¼ˆæŒ‰ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºåˆ†çµ„ï¼‰")
                        
                        # ç‚ºæ¯å€‹ç­–ç•¥é¡å‹å’Œæ•¸æ“šæºçµ„åˆå‰µå»ºåƒæ•¸è©³æƒ…è¡¨æ ¼
                        for group_key, top_group_strategies in top_strategies_by_group.items():
                            strategy_type, data_source = group_key
                            st.subheader(f"{strategy_type} - {data_source} ç­–ç•¥åƒæ•¸è©³æƒ…")
                            

                            
                            # å‰µå»ºè©²çµ„åˆçš„åƒæ•¸è©³æƒ…DataFrame
                            param_details = []
                            for _, row in top_group_strategies.iterrows():
                                # ç²å–ç­–ç•¥ä¿¡æ¯
                                strategy_type = row['strategy']
                                data_source = row['data_source']
                                trial_number = row['trial_number']
                                strategy_name = f"{strategy_type}_{data_source}_{trial_number}"
                                
                                # ç²å–åƒæ•¸
                                params = row['parameters']
                                
                                # è™•ç†åƒæ•¸æ ¼å¼
                                if isinstance(params, dict) and 'parameters' in params:
                                    actual_params = params['parameters']
                                elif isinstance(params, str):
                                    try:
                                        import json
                                        actual_params = json.loads(params)
                                    except json.JSONDecodeError:
                                        actual_params = {}
                                else:
                                    actual_params = params
                                
                                if isinstance(actual_params, dict):
                                    # å‰µå»ºåŒ…å«ç¸¾æ•ˆä¿¡æ¯çš„è¡Œ
                                    detail_row = {
                                        'ç­–ç•¥åç¨±': strategy_name,
                                        'ç¸½å ±é…¬ç‡': f"{row['total_return']:.2%}",
                                        'å¤æ™®å€¼': f"{row['sharpe_ratio']:.3f}",
                                        'æœ€å¤§å›æ’¤': f"{row['max_drawdown']:.2%}",
                                        'ç¶œåˆè©•åˆ†': f"{row['ç¶œåˆè©•åˆ†']:.3f}"
                                    }
                                    
                                    # æ·»åŠ å…¶ä»–ç¸¾æ•ˆæŒ‡æ¨™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                    if 'num_trades' in row and pd.notna(row['num_trades']):
                                        detail_row['äº¤æ˜“æ¬¡æ•¸'] = row['num_trades']
                                    if 'win_rate' in row and pd.notna(row['win_rate']):
                                        detail_row['å‹ç‡'] = f"{row['win_rate']:.2%}"
                                    if 'profit_factor' in row and pd.notna(row['profit_factor']):
                                        detail_row['ç²åˆ©å› å­'] = f"{row['profit_factor']:.3f}"
                                    
                                    # æ·»åŠ ç­–ç•¥åƒæ•¸
                                    detail_row.update(actual_params)
                                    param_details.append(detail_row)
                            
                            if param_details:
                                param_df = pd.DataFrame(param_details)
                                st.dataframe(param_df, use_container_width=True)

                            else:
                                st.warning(f"ç„¡æ³•ç”Ÿæˆ {strategy_type} - {data_source} ç­–ç•¥çš„åƒæ•¸è©³æƒ…")
                    else:
                        st.warning("ç„¡æ³•ç”Ÿæˆåƒæ•¸å­—å…¸ï¼Œè«‹æª¢æŸ¥Optunaç­–ç•¥é…ç½®")
                else:
                    st.warning("æ²’æœ‰å¯ç”¨çš„Optunaç­–ç•¥æ•¸æ“šä¾†ç”Ÿæˆåƒæ•¸è©³æƒ…")




