# check_data_consistency.py
# ç›´æ¥æª¢æŸ¥å…¨å±€å¥—ç”¨èˆ‡å¢å¼·åˆ†æçš„æ•¸æ“šä¸€è‡´æ€§
# å‰µå»ºæ™‚é–“ï¼š2025-08-21 01:25:00
# è·¯å¾‘ï¼š#æ ¹ç›®éŒ„/ä»£ç¢¼

import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
import os

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.getcwd())

def check_data_consistency():
    """ç›´æ¥æª¢æŸ¥å…¨å±€å¥—ç”¨èˆ‡å¢å¼·åˆ†æçš„æ•¸æ“šä¸€è‡´æ€§"""
    print("ğŸ” ç›´æ¥æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§...")
    print("=" * 60)
    
    # æª¢æŸ¥å¿…è¦çš„æ–‡ä»¶
    required_files = [
        "app_dash.py",
        "SSS_EnsembleTab.py"
    ]
    
    for file_path in required_files:
        if not Path(file_path).exists():
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file_path}")
            return False
    
    print("âœ… å¿…è¦æ–‡ä»¶æª¢æŸ¥é€šé")
    
    # æª¢æŸ¥ app_dash.py ä¸­çš„æ•´åˆæƒ…æ³
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("\nğŸ“Š æ–‡ä»¶çµ±è¨ˆ:")
    print(f"   æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦")
    print(f"   å¢å¼·åˆ†æç›¸é—œä»£ç¢¼æ•¸é‡: {content.count('å¢å¼·åˆ†æ')}")
    print(f"   å…¨å±€å¥—ç”¨ç›¸é—œä»£ç¢¼æ•¸é‡: {content.count('å…¨å±€å¥—ç”¨')}")
    print(f"   backtest-store å‡ºç¾æ¬¡æ•¸: {content.count('backtest-store')}")
    
    # æª¢æŸ¥é—œéµæ•´åˆé»
    print("\nğŸ” æª¢æŸ¥é—œéµæ•´åˆé»:")
    
    # 1. æª¢æŸ¥å›èª¿æ˜¯å¦åŒ…å« backtest-store
    if 'State("backtest-store", "data")' in content:
        print("   âœ… å›èª¿åŒ…å« backtest-store")
    else:
        print("   âŒ å›èª¿ä¸åŒ…å« backtest-store")
    
    # 2. æª¢æŸ¥å‡½æ•¸åƒæ•¸æ˜¯å¦åŒ…å« backtest_data
    if 'backtest_data=None' in content:
        print("   âœ… å‡½æ•¸åƒæ•¸åŒ…å« backtest_data")
    else:
        print("   âŒ å‡½æ•¸åƒæ•¸ä¸åŒ…å« backtest_data")
    
    # 3. æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº
    if 'ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº' in content:
        print("   âœ… ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº")
    else:
        print("   âŒ æœªä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº")
    
    # 4. æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
    if 'æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥' in content:
        print("   âœ… æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥")
    else:
        print("   âŒ æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥")
    
    # æª¢æŸ¥å…·é«”çš„æ•¸æ“šè™•ç†é‚è¼¯
    print("\nğŸ” æª¢æŸ¥æ•¸æ“šè™•ç†é‚è¼¯:")
    
    # æŸ¥æ‰¾æ•¸æ“šç²å–é‚è¼¯
    data_source_patterns = [
        'df_from_pack(backtest_data.get("df_raw"))',
        'df_from_pack(cache.get("df_raw"))',
        'daily_state_std',
        'daily_state'
    ]
    
    for pattern in data_source_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    # æª¢æŸ¥é¢¨éšªé–¥é–€åƒæ•¸
    print("\nğŸ” æª¢æŸ¥é¢¨éšªé–¥é–€åƒæ•¸:")
    
    valve_params = [
        'use_slopes: True',
        'slope_method: "polyfit"',
        'atr_cmp: "gt"',
        'atr_win: 20',
        'atr_ref_win: 60'
    ]
    
    for param in valve_params:
        if param in content:
            print(f"   âœ… {param}")
        else:
            print(f"   âŒ {param}")
    
    return True

def check_enhanced_analysis_implementation():
    """æª¢æŸ¥å¢å¼·åˆ†æçš„å…·é«”å¯¦ç¾"""
    print("\nğŸ” æª¢æŸ¥å¢å¼·åˆ†æå¯¦ç¾:")
    print("=" * 60)
    
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ‰¾åˆ°å¢å¼·åˆ†æå‡½æ•¸çš„å®šç¾©
    start_marker = "def _run_rv("
    end_marker = "    if not n_clicks or not cache:"
    
    start_pos = content.find(start_marker)
    if start_pos != -1:
        # æ‰¾åˆ°å‡½æ•¸é–‹å§‹
        func_start = start_pos
        # æ‰¾åˆ°å‡½æ•¸é«”é–‹å§‹
        body_start = content.find(end_marker, func_start)
        
        if body_start != -1:
            # æå–å‡½æ•¸å®šç¾©å’Œåƒæ•¸
            func_def = content[func_start:body_start].strip()
            print("ğŸ“‹ å‡½æ•¸å®šç¾©:")
            print(f"   {func_def}")
            
            # æª¢æŸ¥åƒæ•¸
            if 'backtest_data' in func_def:
                print("   âœ… åŒ…å« backtest_data åƒæ•¸")
            else:
                print("   âŒ ä¸åŒ…å« backtest_data åƒæ•¸")
        else:
            print("   âš ï¸  ç„¡æ³•æ‰¾åˆ°å‡½æ•¸é«”é–‹å§‹")
    else:
        print("   âŒ ç„¡æ³•æ‰¾åˆ° _run_rv å‡½æ•¸å®šç¾©")
    
    # æª¢æŸ¥æ•¸æ“šç²å–é‚è¼¯
    print("\nğŸ” æª¢æŸ¥æ•¸æ“šç²å–é‚è¼¯:")
    
    # æŸ¥æ‰¾æ•¸æ“šæºé¸æ“‡é‚è¼¯
    data_source_logic = [
        'å„ªå…ˆä½¿ç”¨å…¨å±€å¥—ç”¨çš„æ•¸æ“šæº',
        'å¾ backtest-store ç²å–æ•¸æ“š',
        'å›é€€åˆ°å¿«å–æ•¸æ“š',
        'ä½¿ç”¨å¿«å–æ•¸æ“šæº'
    ]
    
    for logic in data_source_logic:
        if logic in content:
            print(f"   âœ… {logic}")
        else:
            print(f"   âŒ {logic}")
    
    # æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥é‚è¼¯
    print("\nğŸ” æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥é‚è¼¯:")
    
    consistency_checks = [
        'æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥',
        'æ•¸æ“šé•·åº¦ä¸€è‡´',
        'æ•¸æ“šé•·åº¦ä¸ä¸€è‡´',
        'daily_state è¼‰å…¥æˆåŠŸ',
        'daily_state è¼‰å…¥å¤±æ•—'
    ]
    
    for check in consistency_checks:
        if check in content:
            print(f"   âœ… {check}")
        else:
            print(f"   âŒ {check}")

def check_global_application_implementation():
    """æª¢æŸ¥å…¨å±€å¥—ç”¨çš„å¯¦ç¾"""
    print("\nğŸ” æª¢æŸ¥å…¨å±€å¥—ç”¨å¯¦ç¾:")
    print("=" * 60)
    
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾å…¨å±€å¥—ç”¨çš„é—œéµé‚è¼¯
    global_patterns = [
        'å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨',
        'èˆ‡å¢å¼·åˆ†æä¸€è‡´',
        'compute_risk_valve_signals',
        'risk_valve_backtest'
    ]
    
    for pattern in global_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    # æª¢æŸ¥å…¨å±€å¥—ç”¨çš„é¢¨éšªé–¥é–€åƒæ•¸
    print("\nğŸ” æª¢æŸ¥å…¨å±€å¥—ç”¨é¢¨éšªé–¥é–€åƒæ•¸:")
    
    global_valve_params = [
        'use_slopes=True',
        'slope_method="polyfit"',
        'atr_cmp="gt"',
        'atr_win=20',
        'atr_ref_win=60'
    ]
    
    for param in global_valve_params:
        count = content.count(param)
        print(f"   {param}: {count} æ¬¡")

def generate_detailed_report():
    """ç”Ÿæˆè©³ç´°çš„æª¢æŸ¥å ±å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆè©³ç´°æª¢æŸ¥å ±å‘Š...")
    
    report = {
        "check_date": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        "check_type": "æ•¸æ“šä¸€è‡´æ€§è©³ç´°æª¢æŸ¥",
        "files_checked": ["app_dash.py", "SSS_EnsembleTab.py"],
        "integration_status": "å·²æ•´åˆ",
        "key_findings": [],
        "recommendations": []
    }
    
    # æª¢æŸ¥æ•´åˆç‹€æ…‹
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æª¢æŸ¥é—œéµæ•´åˆé»
    integration_checks = {
        "å›èª¿åŒ…å« backtest-store": 'State("backtest-store", "data")' in content,
        "å‡½æ•¸åƒæ•¸åŒ…å« backtest_data": "backtest_data=None" in content,
        "ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº": "ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº" in content,
        "æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥": "æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥" in content
    }
    
    report["integration_checks"] = integration_checks
    
    # æª¢æŸ¥æ•¸æ“šè™•ç†é‚è¼¯
    data_logic_checks = {
        "å„ªå…ˆä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº": "å„ªå…ˆä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº" in content,
        "å¾ backtest-store ç²å–æ•¸æ“š": "å¾ backtest-store ç²å–æ•¸æ“š" in content,
        "å›é€€åˆ°å¿«å–æ•¸æ“š": "å›é€€åˆ°å¿«å–æ•¸æ“š" in content,
        "ä½¿ç”¨å¿«å–æ•¸æ“šæº": "ä½¿ç”¨å¿«å–æ•¸æ“šæº" in content
    }
    
    report["data_logic_checks"] = data_logic_checks
    
    # æª¢æŸ¥é¢¨éšªé–¥é–€åƒæ•¸ä¸€è‡´æ€§
    valve_param_checks = {
        "use_slopes: True": "use_slopes: True" in content,
        "slope_method: polyfit": 'slope_method: "polyfit"' in content,
        "atr_cmp: gt": 'atr_cmp: "gt"' in content
    }
    
    report["valve_param_checks"] = valve_param_checks
    
    # ç”Ÿæˆå»ºè­°
    if all(integration_checks.values()):
        report["recommendations"].append("æ‰€æœ‰æ•´åˆé»æª¢æŸ¥é€šéï¼Œå¯ä»¥é€²è¡Œå¯¦éš›æ¸¬è©¦")
    else:
        failed_checks = [k for k, v in integration_checks.items() if not v]
        report["recommendations"].append(f"ä»¥ä¸‹æ•´åˆé»éœ€è¦ä¿®å¾©: {', '.join(failed_checks)}")
    
    if all(data_logic_checks.values()):
        report["recommendations"].append("æ•¸æ“šè™•ç†é‚è¼¯å®Œæ•´ï¼Œæ”¯æŒæ•¸æ“šæºåˆ‡æ›")
    else:
        failed_checks = [k for k, v in data_logic_checks.items() if not v]
        report["recommendations"].append(f"ä»¥ä¸‹æ•¸æ“šé‚è¼¯éœ€è¦ä¿®å¾©: {', '.join(failed_checks)}")
    
    # ä¿å­˜å ±å‘Š
    report_file = f"data_consistency_detailed_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è©³ç´°æª¢æŸ¥å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ç›´æ¥æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§...")
    print("=" * 60)
    
    # åŸ·è¡Œå„é …æª¢æŸ¥
    check_data_consistency()
    check_enhanced_analysis_implementation()
    check_global_application_implementation()
    
    # ç”Ÿæˆè©³ç´°å ±å‘Š
    report = generate_detailed_report()
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ¯ æª¢æŸ¥å®Œæˆï¼")
    
    print("\nğŸ“Š æª¢æŸ¥çµæœ:")
    if report["integration_checks"]["å›èª¿åŒ…å« backtest-store"]:
        print("   âœ… å›èª¿å·²æ•´åˆ backtest-store")
    else:
        print("   âŒ å›èª¿æœªæ•´åˆ backtest-store")
    
    if report["integration_checks"]["å‡½æ•¸åƒæ•¸åŒ…å« backtest_data"]:
        print("   âœ… å‡½æ•¸åƒæ•¸å·²æ·»åŠ  backtest_data")
    else:
        print("   âŒ å‡½æ•¸åƒæ•¸æœªæ·»åŠ  backtest_data")
    
    if report["integration_checks"]["ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº"]:
        print("   âœ… å·²ä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº")
    else:
        print("   âŒ æœªä½¿ç”¨å…¨å±€å¥—ç”¨æ•¸æ“šæº")
    
    if report["integration_checks"]["æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥"]:
        print("   âœ… å·²æ·»åŠ æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥")
    else:
        print("   âŒ æœªæ·»åŠ æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥")
    
    print("\nğŸ’¡ å»ºè­°:")
    for rec in report["recommendations"]:
        print(f"   â€¢ {rec}")
    
    print("\nâœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()
