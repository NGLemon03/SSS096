# app_dash_modules/utils.py / 2025-08-23 03:07
import dash
from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.graph_objects as go
import json
import io
from dash.dependencies import ALL
import shutil
import os
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from analysis import config as cfg
import yfinance as yf
import logging
import numpy as np
from urllib.parse import quote as urlparse

# é…ç½® logger - ä½¿ç”¨çµ±ä¸€æ—¥èªŒç³»çµ±ï¼ˆæŒ‰éœ€åˆå§‹åŒ–ï¼‰
from analysis.logging_config import get_logger, init_logging
import os

# è¨­å®šç’°å¢ƒè®Šæ•¸ï¼ˆä½†ä¸ç«‹å³åˆå§‹åŒ–ï¼‰
os.environ["SSS_CREATE_LOGS"] = "1"

# ç²å–æ—¥èªŒå™¨ï¼ˆæ‡¶åŠ è¼‰ï¼‰
logger = get_logger("SSS.App")

def normalize_daily_state_columns(ds: pd.DataFrame) -> pd.DataFrame:
    """å°‡ä¸åŒä¾†æºçš„ daily_state æ¬„ä½èªæ„çµ±ä¸€ï¼š
    - è‹¥ equity å¯¦ç‚ºå€‰ä½å¸‚å€¼ï¼Œæ”¹åç‚º position_value
    - å»ºç«‹ portfolio_value = position_value + cash
    - ä¿è­‰æœ‰ invested_pct / cash_pct
    """
    if ds is None or ds.empty:
        return ds
    ds = ds.copy()

    # è‹¥å·²ç¶“æœ‰ position_value èˆ‡ cashï¼Œç›´æ¥å»ºç«‹ portfolio_value
    if {'position_value','cash'}.issubset(ds.columns):
        ds['portfolio_value'] = ds['position_value'] + ds['cash']

    # åƒ…æœ‰ equity + cash çš„æƒ…æ³ -> åˆ¤æ–· equity æ˜¯ç¸½è³‡ç”¢é‚„æ˜¯å€‰ä½
    elif {'equity','cash'}.issubset(ds.columns):
        # åˆ¤æ–·è¦å‰‡ï¼šè‹¥ equity/(equity+cash) çš„ä¸­ä½æ•¸é¡¯è‘— < 0.9ï¼Œè¼ƒåƒã€Œå€‰ä½å¸‚å€¼ã€
        ratio = (ds['equity'] / (ds['equity'] + ds['cash'])).replace([np.inf, -np.inf], np.nan).clip(0,1)
        if ratio.median(skipna=True) < 0.9:
            # æŠŠ equity ç•¶æˆå€‰ä½å¸‚å€¼
            ds = ds.rename(columns={'equity':'position_value'})
            ds['portfolio_value'] = ds['position_value'] + ds['cash']
        else:
            # equity å·²æ˜¯ç¸½è³‡ç”¢ï¼Œåæ¨å€‰ä½ï¼ˆè‹¥æ²’æœ‰ position_valueï¼‰
            if 'position_value' not in ds.columns:
                ds['position_value'] = (ds['equity'] - ds['cash']).fillna(0.0)
            ds['portfolio_value'] = ds['equity']

    # ç™¾åˆ†æ¯”æ¬„ä½çµ±ä¸€
    if 'portfolio_value' in ds.columns:
        pv = ds['portfolio_value'].replace(0, np.nan)
        if 'invested_pct' not in ds.columns and 'position_value' in ds.columns:
            ds['invested_pct'] = (ds['position_value'] / pv).fillna(0.0).clip(0,1)
        if 'cash_pct' not in ds.columns and 'cash' in ds.columns:
            ds['cash_pct'] = (ds['cash'] / pv).fillna(0.0).clip(0,1)

    # ç‚ºäº†å‘ä¸‹ç›¸å®¹ï¼šä¿ç•™ equity = portfolio_valueï¼ˆä¾›èˆŠç¹ªåœ–å‡½å¼ä½¿ç”¨ï¼‰
    if 'portfolio_value' in ds.columns:
        ds['equity'] = ds['portfolio_value']

    return ds

def _initialize_app_logging():
    """åˆå§‹åŒ–æ‡‰ç”¨ç¨‹å¼æ—¥èªŒç³»çµ±"""
    # åªåœ¨å¯¦éš›éœ€è¦æ™‚æ‰åˆå§‹åŒ–æª”æ¡ˆæ—¥èªŒ
    init_logging(enable_file=True)
    logger.setLevel(logging.DEBUG)
    logger.info("=== App Dash å•Ÿå‹• - çµ±ä¸€æ—¥èªŒç³»çµ± ===")
    logger.info("å·²å•Ÿç”¨è©³ç´°èª¿è©¦æ¨¡å¼ - èª¿è©¦è³‡è¨Šå°‡å¯«å…¥æ—¥èªŒæª”æ¡ˆ")
    logger.info(f"æ—¥èªŒç›®éŒ„: {os.path.abspath('analysis/log')}")
    return logger

# ATR è¨ˆç®—å‡½æ•¸
def calculate_atr(df, window):
    """è¨ˆç®— ATR (Average True Range)"""
    try:
        # app_dash.py / 2025-08-22 14:30
        # çµ±ä¸€ OHLC æ¬„ä½å°æ‡‰ï¼šæ”¶ç›¤ã€æœ€é«˜ã€æœ€ä½åƒ¹
        high_col = None
        low_col = None
        close_col = None
        
        # å„ªå…ˆæª¢æŸ¥è‹±æ–‡æ¬„ä½åç¨±ï¼ˆæ¨™æº–æ ¼å¼ï¼‰
        if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:
            high_col = 'high'
            low_col = 'low'
            close_col = 'close'
        # æª¢æŸ¥å¤§å¯«è‹±æ–‡æ¬„ä½åç¨±
        elif 'High' in df.columns and 'Low' in df.columns and 'Close' in df.columns:
            high_col = 'High'
            low_col = 'Low'
            close_col = 'Close'
        # æª¢æŸ¥ä¸­æ–‡æ¬„ä½åç¨±
        elif 'æœ€é«˜åƒ¹' in df.columns and 'æœ€ä½åƒ¹' in df.columns and 'æ”¶ç›¤åƒ¹' in df.columns:
            high_col = 'æœ€é«˜åƒ¹'
            low_col = 'æœ€ä½åƒ¹'
            close_col = 'æ”¶ç›¤åƒ¹'
        # æª¢æŸ¥å…¶ä»–å¯èƒ½çš„æ¬„ä½åç¨±ï¼ˆé™ç´šè™•ç†ï¼‰
        elif 'open' in df.columns and 'close' in df.columns:
            # å¦‚æœæ²’æœ‰é«˜ä½åƒ¹ï¼Œç”¨é–‹ç›¤åƒ¹å’Œæ”¶ç›¤åƒ¹è¿‘ä¼¼
            high_col = 'open'
            low_col = 'close'
            close_col = 'close'
        
        if high_col and low_col and close_col:
            # æœ‰é«˜ä½åƒ¹æ™‚ï¼Œè¨ˆç®— True Range
            high = df[high_col]
            low = df[low_col]
            close = df[close_col]
            
            # ç¢ºä¿æ•¸æ“šç‚ºæ•¸å€¼å‹
            high = pd.to_numeric(high, errors='coerce')
            low = pd.to_numeric(low, errors='coerce')
            close = pd.to_numeric(close, errors='coerce')
            
            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))
            
            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr = true_range.rolling(window=window).mean()
        else:
            # åªæœ‰æ”¶ç›¤åƒ¹æ™‚ï¼Œç”¨åƒ¹æ ¼è®ŠåŒ–è¿‘ä¼¼
            if close_col:
                close = pd.to_numeric(df[close_col], errors='coerce')
            elif 'close' in df.columns:
                close = pd.to_numeric(df['close'], errors='coerce')
            elif 'Close' in df.columns:
                close = pd.to_numeric(df['Close'], errors='coerce')
            else:
                # åªè¨˜ä¸€æ¬¡è­¦å‘Šï¼Œé¿å…é‡è¤‡åˆ·å±
                if not hasattr(calculate_atr, '_warning_logged'):
                    logger.warning("æ‰¾ä¸åˆ°å¯ç”¨çš„åƒ¹æ ¼æ¬„ä½ä¾†è¨ˆç®— ATRï¼Œé™ç´šç‚º ATR-only æ¨¡å¼")
                    calculate_atr._warning_logged = True
                return pd.Series(index=df.index, dtype=float)
            
            price_change = close.diff().abs()
            atr = price_change.rolling(window=window).mean()
        
        # æª¢æŸ¥è¨ˆç®—çµæœ
        if atr is None or atr.empty or atr.isna().all():
            if not hasattr(calculate_atr, '_warning_logged'):
                logger.warning(f"ATR è¨ˆç®—çµæœç„¡æ•ˆï¼Œwindow={window}ï¼Œé™ç´šç‚º ATR-only æ¨¡å¼")
                calculate_atr._warning_logged = True
            return pd.Series(index=df.index, dtype=float)
        
        return atr
    except Exception as e:
        if not hasattr(calculate_atr, '_warning_logged'):
            logger.warning(f"ATR è¨ˆç®—å¤±æ•—: {e}ï¼Œé™ç´šç‚º ATR-only æ¨¡å¼")
            calculate_atr._warning_logged = True
        return pd.Series(index=df.index, dtype=float)


def _build_benchmark_df(df_raw):
    """å»ºç«‹åŸºæº–è³‡æ–™ DataFrameï¼Œçµ±ä¸€è™•ç†æ¬„ä½åç¨±å’Œæ•¸æ“šè½‰æ›"""
    # app_dash.py / 2025-08-22 14:30
    # çµ±ä¸€ OHLC æ¬„ä½å°æ‡‰ï¼šæ”¶ç›¤ã€æœ€é«˜ã€æœ€ä½åƒ¹
    bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
    
    # æ”¶ç›¤åƒ¹æ¬„ä½ - å„ªå…ˆä½¿ç”¨è‹±æ–‡æ¬„ä½ï¼Œå›é€€åˆ°ä¸­æ–‡æ¬„ä½
    if 'close' in df_raw.columns:
        bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
    elif 'Close' in df_raw.columns:
        bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["Close"], errors="coerce")
    elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
        bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
    
    # æœ€é«˜åƒ¹å’Œæœ€ä½åƒ¹æ¬„ä½ - å„ªå…ˆä½¿ç”¨è‹±æ–‡æ¬„ä½ï¼Œå›é€€åˆ°ä¸­æ–‡æ¬„ä½
    if 'high' in df_raw.columns and 'low' in df_raw.columns:
        bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
        bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
    elif 'High' in df_raw.columns and 'Low' in df_raw.columns:
        bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["High"], errors="coerce")
        bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["Low"], errors="coerce")
    elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
        bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
        bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")
    
    return bench

def calculate_equity_curve(open_px, w, cap, atr_ratio):
    """è¨ˆç®—æ¬Šç›Šæ›²ç·š"""
    try:
        # ç°¡åŒ–çš„æ¬Šç›Šæ›²ç·šè¨ˆç®—
        # é€™è£¡ä½¿ç”¨é–‹ç›¤åƒ¹å’Œæ¬Šé‡çš„ä¹˜ç©ä¾†æ¨¡æ“¬æ¬Šç›Šè®ŠåŒ–
        equity = (open_px * w * cap).cumsum()
        return equity
    except Exception as e:
        logger.warning(f"æ¬Šç›Šæ›²ç·šè¨ˆç®—å¤±æ•—: {e}")
        return None

def calculate_trades_from_equity(equity_curve, open_px, w, cap, atr_ratio):
    """å¾æ¬Šç›Šæ›²ç·šè¨ˆç®—äº¤æ˜“è¨˜éŒ„"""
    try:
        if equity_curve is None or equity_curve.empty:
            return None
        
        # ç°¡åŒ–çš„äº¤æ˜“è¨˜éŒ„ç”Ÿæˆ
        # é€™è£¡æ ¹æ“šæ¬Šé‡è®ŠåŒ–ä¾†è­˜åˆ¥äº¤æ˜“
        weight_changes = w.diff().abs()
        trade_dates = weight_changes[weight_changes > 0.01].index
        
        trades = []
        for date in trade_dates:
            trades.append({
                'trade_date': date,
                'return': 0.0  # ç°¡åŒ–ï¼Œå¯¦éš›æ‡‰è©²è¨ˆç®—å ±é…¬ç‡
            })
        
        if trades:
            return pd.DataFrame(trades)
        else:
            return pd.DataFrame(columns=['trade_date', 'return'])
            
    except Exception as e:
        logger.warning(f"äº¤æ˜“è¨˜éŒ„è¨ˆç®—å¤±æ•—: {e}")
        return None

# è§£åŒ…å™¨å‡½æ•¸ï¼šæ”¯æ´ pack_df/pack_series å’Œå‚³çµ± JSON å­—ä¸²å…©ç¨®æ ¼å¼
def df_from_pack(data):
    """å¾ pack_df çµæœæˆ– JSON å­—ä¸²è§£åŒ… DataFrame"""
    import io, json
    import pandas as pd
    
    # å¦‚æœå·²ç¶“æ˜¯ DataFrameï¼Œç›´æ¥è¿”å›
    if isinstance(data, pd.DataFrame):
        return data
    
    # æª¢æŸ¥æ˜¯å¦ç‚º None æˆ–ç©ºå­—ä¸²
    if data is None:
        return pd.DataFrame()
    
    # å¦‚æœæ˜¯å­—ä¸²ï¼Œé€²è¡Œé¡å¤–æª¢æŸ¥
    if isinstance(data, str):
        if data == "" or data == "[]":
            return pd.DataFrame()
        # å…ˆå˜—è©¦ split â†’ å†é€€å›é è¨­
        for orient in ("split", None):
            try:
                kw = {"orient": orient} if orient else {}
                return pd.read_json(io.StringIO(data), **kw)
            except Exception:
                pass
        return pd.DataFrame()
    
    if isinstance(data, (list, dict)):
        try:
            return pd.DataFrame(data)
        except Exception:
            return pd.DataFrame()
    
    return pd.DataFrame()

def series_from_pack(data):
    """å¾ pack_series çµæœæˆ– JSON å­—ä¸²è§£åŒ… Series"""
    import io
    import pandas as pd
    
    # å¦‚æœå·²ç¶“æ˜¯ Seriesï¼Œç›´æ¥è¿”å›
    if isinstance(data, pd.Series):
        return data
    
    # æª¢æŸ¥æ˜¯å¦ç‚º None æˆ–ç©ºå­—ä¸²
    if data is None:
        return pd.Series(dtype=float)
    
    # å¦‚æœæ˜¯å­—ä¸²ï¼Œé€²è¡Œé¡å¤–æª¢æŸ¥
    if isinstance(data, str):
        if data == "" or data == "[]":
            return pd.Series(dtype=float)
        # Series ä¹Ÿå…ˆè©¦ split
        for orient in ("split", None):
            try:
                kw = {"orient": orient} if orient else {}
                return pd.read_json(io.StringIO(data), typ="series", **kw)
            except Exception:
                pass
        return pd.Series(dtype=float)
    
    if isinstance(data, (list, dict)):
        try:
            return pd.Series(data)
        except Exception:
            return pd.Series(dtype=float)
    
    return pd.Series(dtype=float)

from SSSv096 import (
    param_presets, load_data, compute_single, compute_dual, compute_RMA,
    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_periods
)

# å½ˆæ€§åŒ¯å…¥ pack_df/pack_series å‡½æ•¸
try:
    from sss_core.schemas import pack_df, pack_series
except Exception:
    from schemas import pack_df, pack_series

# åŒ¯å…¥æ¬Šé‡æ¬„ä½ç¢ºä¿å‡½å¼
try:
    from sss_core.normalize import _ensure_weight_columns
except Exception:
    # å¦‚æœç„¡æ³•åŒ¯å…¥ï¼Œå®šç¾©ä¸€å€‹ç©ºçš„å‡½å¼ä½œç‚º fallback
    def _ensure_weight_columns(df):
        return df

# å‡è¨­ä½ æœ‰ get_version_history_html
try:
    from version_history import get_version_history_html
except ImportError:
    def get_version_history_html() -> str:
        return "<b>ç„¡æ³•è¼‰å…¥ç‰ˆæœ¬æ­·å²è¨˜éŒ„</b>"

# --- ä¿è­‰æ”¾é€² Store çš„éƒ½æ˜¯ JSON-safe ---
def _pack_any(x):
    import pandas as pd
    if isinstance(x, pd.DataFrame):
        return pack_df(x)          # orient="split" + date_format="iso"
    if isinstance(x, pd.Series):
        return pack_series(x)      # orient="split" + date_format="iso"
    return x

def _pack_result_for_store(result: dict) -> dict:
    # çµ±ä¸€æŠŠæ‰€æœ‰ pandas ç‰©ä»¶è½‰æˆå­—ä¸²ï¼ˆJSONï¼‰
    keys = [
        'trade_df', 'trades_df', 'signals_df',
        'equity_curve', 'cash_curve', 'price_series',
        'daily_state', 'trade_ledger',
        'daily_state_std', 'trade_ledger_std',
        'weight_curve',
        # âŠ æ–°å¢ï¼šä¿å­˜æœªå¥—é–¥é–€ baseline
        'daily_state_base', 'trade_ledger_base', 'weight_curve_base',
        # â‹ æ–°å¢ï¼šä¿å­˜ valve ç‰ˆæœ¬
        'daily_state_valve', 'trade_ledger_valve', 'weight_curve_valve', 'equity_curve_valve'
    ]
    out = dict(result)
    for k in keys:
        if k in out:
            out[k] = _pack_any(out[k])
    # å¦å¤–æŠŠ datetime tuple çš„ trades è½‰å¯åºåˆ—åŒ–ï¼ˆä½ åŸæœ¬ä¹Ÿæœ‰åšï¼‰
    if 'trades' in out and isinstance(out['trades'], list):
        out['trades'] = [
            (str(t[0]), t[1], str(t[2])) if isinstance(t, tuple) and len(t) == 3 else t
            for t in out['trades']
        ]
    return out

default_tickers = ["00631L.TW", "2330.TW", "AAPL", "VOO"]
strategy_names = list(param_presets.keys())

theme_list = ['theme-dark', 'theme-light', 'theme-blue']

def get_theme_label(theme):
    if theme == 'theme-dark':
        return 'ğŸŒ‘ æ·±è‰²ä¸»é¡Œ'
    elif theme == 'theme-light':
        return 'ğŸŒ• æ·ºè‰²ä¸»é¡Œ'
    else:
        return 'ğŸ’™ è—é»ƒä¸»é¡Œ'

def get_column_display_name(column_name):
    """å°‡è‹±æ–‡æ¬„ä½åè½‰æ›ç‚ºä¸­æ–‡é¡¯ç¤ºåç¨±"""
    column_mapping = {
        'trade_date': 'äº¤æ˜“æ—¥æœŸ',
        'signal_date': 'ä¿¡è™Ÿæ—¥æœŸ',
        'type': 'äº¤æ˜“é¡å‹',
        'price': 'åƒ¹æ ¼',
        'weight_change': 'æ¬Šé‡è®ŠåŒ–',
        'w_before': 'äº¤æ˜“å‰æ¬Šé‡',
        'w_after': 'äº¤æ˜“å¾Œæ¬Šé‡',
        'delta_units': 'è‚¡æ•¸è®ŠåŒ–',
        'exec_notional': 'åŸ·è¡Œé‡‘é¡',
        'equity_after': 'äº¤æ˜“å¾Œæ¬Šç›Š',
        'cash_after': 'äº¤æ˜“å¾Œç¾é‡‘',
        'equity_pct': 'æ¬Šç›Š%',
        'cash_pct': 'ç¾é‡‘%',
        'invested_pct': 'æŠ•è³‡æ¯”ä¾‹',
        'position_value': 'éƒ¨ä½åƒ¹å€¼',
        'return': 'å ±é…¬ç‡',
        'comment': 'å‚™è¨»'
    }
    return column_mapping.get(column_name, column_name)

# é¡¯ç¤ºå±¤æ¬„åå°æ‡‰ï¼ˆâ†’ ä¸­æ–‡ï¼‰
DISPLAY_NAME = {
    'trade_date': 'äº¤æ˜“æ—¥æœŸ',
    'signal_date': 'è¨Šè™Ÿæ—¥æœŸ',
    'type': 'äº¤æ˜“é¡å‹',
    'price': 'åƒ¹æ ¼',
    'weight_change': 'æ¬Šé‡è®ŠåŒ–',
    'w_before': 'äº¤æ˜“å‰æ¬Šé‡',
    'w_after': 'äº¤æ˜“å¾Œæ¬Šé‡',
    'delta_units': 'è‚¡æ•¸è®ŠåŒ–',
    'exec_notional': 'åŸ·è¡Œé‡‘é¡',
    'equity_after': 'äº¤æ˜“å¾Œæ¬Šç›Š',
    'cash_after': 'äº¤æ˜“å¾Œç¾é‡‘',
    'equity_pct': 'æ¬Šç›Š%',
    'cash_pct': 'ç¾é‡‘%',
    'invested_pct': 'æŠ•è³‡æ¯”ä¾‹',
    'position_value': 'éƒ¨ä½å¸‚å€¼',
    'return': 'å ±é…¬',
    'comment': 'å‚™è¨»',
}

# é¡¯ç¤ºå±¤ã€Œéš±è—ã€æ¬„ä½ï¼ˆè¨ˆç®—ä¿ç•™ã€UI ä¸é¡¯ç¤ºï¼‰
HIDE_COLS = {
    'shares_before', 'shares_after', 'fee_buy', 'fee_sell', 'sell_tax', 'tax',
    'date', 'open', 'equity_open_after_trade'  # â† ä½ æåˆ°çš„é›œæ¬„ï¼Œçµ±ä¸€éš±è—
}

# é¡¯ç¤ºå±¤æ¬„ä½é †åºï¼ˆå­˜åœ¨æ‰æ’ï¼Œä¸å­˜åœ¨å°±è·³éï¼‰
PREFER_ORDER = [
    'trade_date','signal_date','type','price',
    'weight_change','w_before','w_after',
    'delta_units','exec_notional',
    'equity_after','cash_after','equity_pct','cash_pct',
    'invested_pct','position_value','return','comment'
]

def format_trade_like_df_for_display(df):
    """é¡¯ç¤ºå±¤ï¼šéš±è—é›œæ¬„ â†’ è£œç™¾åˆ†æ¯” â†’ æ ¼å¼åŒ– â†’ ä¸­æ–‡æ¬„å â†’ å®‰å…¨æ’åº"""
    import pandas as pd
    if df is None or len(df)==0:
        return df

    d = df.copy()

    # 1) éš±è—é›œæ¬„
    hide = [c for c in HIDE_COLS if c in d.columns]
    if hide:
        d = d.drop(columns=hide, errors='ignore')

    # 2) å¿…è¦æ¬„ä½è£œé½Šç™¾åˆ†æ¯”ï¼ˆè‹¥å·²å­˜åœ¨å°±ç•¥éï¼‰
    if {'equity_after','cash_after'}.issubset(d.columns):
        tot = d['equity_after'] + d['cash_after']
        if 'equity_pct' not in d.columns:
            d['equity_pct'] = d.apply(
                lambda r: "" if pd.isna(r['equity_after']) or pd.isna(tot.loc[r.name]) or tot.loc[r.name] <= 0
                          else f"{(r['equity_after']/tot.loc[r.name]):.2%}", axis=1)
        if 'cash_pct' not in d.columns:
            d['cash_pct'] = d.apply(
                lambda r: "" if pd.isna(r['cash_after']) or pd.isna(tot.loc[r.name]) or tot.loc[r.name] <= 0
                          else f"{(r['cash_after']/tot.loc[r.name]):.2%}", axis=1)

    # 3) æ ¼å¼åŒ–
    def _fmt_date_col(s):
        dt = pd.to_datetime(s, errors='coerce')
        return dt.dt.strftime('%Y-%m-%d').fillna("")

    for col in ['trade_date','signal_date']:
        if col in d.columns:
            d[col] = _fmt_date_col(d[col])

    if 'price' in d.columns:
        d['price'] = d['price'].apply(lambda x: f"{x:,.2f}" if pd.notnull(x) else "")
    if 'weight_change' in d.columns:
        d['weight_change'] = d['weight_change'].apply(lambda x: f"{x:.2%}" if pd.notnull(x) else "")
    for col in ['w_before','w_after']:
        if col in d.columns:
            d[col] = d[col].apply(lambda x: f"{x:,.4f}" if pd.notnull(x) else "")
    for col in ['exec_notional','equity_after','cash_after','position_value']:
        if col in d.columns:
            d[col] = d[col].apply(lambda x: f"{int(round(x)):,}" if pd.notnull(x) else "")
    if 'delta_units' in d.columns:
        d['delta_units'] = d['delta_units'].apply(lambda x: f"{int(round(x)):,}" if pd.notnull(x) else "")
    if 'return' in d.columns:
        d['return'] = d['return'].apply(lambda x: "-" if pd.isna(x) else f"{x:.2%}")

    # 4) å®‰å…¨æ’åºï¼ˆåªæ’å­˜åœ¨çš„æ¬„ä½ï¼‰
    exist = [c for c in PREFER_ORDER if c in d.columns]
    others = [c for c in d.columns if c not in exist]
    d = d[exist + others]

    # 5) ä¸­æ–‡æ¬„å
    d = d.rename(columns={k: DISPLAY_NAME.get(k, k) for k in d.columns})
    return d

def is_price_data_up_to_date(csv_path):
    if not os.path.exists(csv_path):
        return False
    try:
        df = pd.read_csv(csv_path)
        if 'date' in df.columns:
            last_date = pd.to_datetime(df['date'].iloc[-1])
        else:
            last_date = pd.to_datetime(df.iloc[-1, 0])
        
        # ç²å–å°åŒ—æ™‚é–“çš„ä»Šå¤©
        today = pd.Timestamp.now(tz='Asia/Taipei').normalize()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå·¥ä½œæ—¥ï¼ˆé€±ä¸€åˆ°é€±äº”ï¼‰
        if today.weekday() >= 5:  # é€±å…­(5)æˆ–é€±æ—¥(6)
            # å¦‚æœæ˜¯é€±æœ«ï¼Œæª¢æŸ¥æœ€å¾Œæ•¸æ“šæ˜¯å¦ç‚ºä¸Šå€‹å·¥ä½œæ—¥
            last_weekday = today - pd.Timedelta(days=today.weekday() - 4)  # ä¸Šå€‹é€±äº”
            return last_date >= last_weekday
        else:
            # å¦‚æœæ˜¯å·¥ä½œæ—¥ï¼Œæª¢æŸ¥æ˜¯å¦ç‚ºä»Šå¤©æˆ–æ˜¨å¤©ï¼ˆè€ƒæ…®æ•¸æ“šå»¶é²ï¼‰
            yesterday = today - pd.Timedelta(days=1)
            return last_date >= yesterday
    except Exception:
        return False

def fetch_yf_data(ticker: str, filename: Path, start_date: str = "2000-01-01", end_date: str | None = None):
    now_taipei = pd.Timestamp.now(tz='Asia/Taipei')
    try:
        end_date_str = end_date if end_date is not None else now_taipei.strftime('%Y-%m-%d')
        df = yf.download(ticker, start=start_date, end=end_date_str, auto_adjust=True)
        if df is None or df.empty:
            raise ValueError("ä¸‹è¼‰çš„æ•¸æ“šç‚ºç©º")
        df.to_csv(filename)
        print(f"æˆåŠŸä¸‹è¼‰ '{ticker}' æ•¸æ“šåˆ° '{filename}'.")
    except Exception as e:
        print(f"è­¦å‘Š: '{ticker}' ä¸‹è¼‰å¤±æ•—: {e}")

def ensure_all_price_data_up_to_date(ticker_list, data_dir):
    """æ™ºèƒ½æª¢æŸ¥ä¸¦æ›´æ–°è‚¡åƒ¹æ•¸æ“šï¼Œåªåœ¨å¿…è¦æ™‚ä¸‹è¼‰"""
    for ticker in ticker_list:
        filename = Path(data_dir) / f"{ticker.replace(':','_')}_data_raw.csv"
        if not is_price_data_up_to_date(filename):
            print(f"{ticker} è‚¡åƒ¹è³‡æ–™éœ€è¦æ›´æ–°ï¼Œé–‹å§‹ä¸‹è¼‰...")
            fetch_yf_data(ticker, filename)
        else:
            print(f"{ticker} è‚¡åƒ¹è³‡æ–™å·²æ˜¯æœ€æ–°ï¼Œè·³éä¸‹è¼‰ã€‚")

# ç°¡åŒ–çš„è‚¡åƒ¹æ•¸æ“šä¸‹è¼‰å‰è»Šæ©Ÿåˆ¶
def should_download_price_data():
    """æª¢æŸ¥æ˜¯å¦éœ€è¦ä¸‹è¼‰è‚¡åƒ¹æ•¸æ“šçš„å‰è»Šæ©Ÿåˆ¶"""
    try:
        # æª¢æŸ¥æ˜¯å¦ç‚ºäº¤æ˜“æ™‚é–“ï¼ˆé¿å…åœ¨äº¤æ˜“æ™‚é–“é »ç¹ä¸‹è¼‰ï¼‰
        now = pd.Timestamp.now(tz='Asia/Taipei')
        if now.weekday() < 5:  # å·¥ä½œæ—¥
            hour = now.hour
            if 9 <= hour <= 13:  # äº¤æ˜“æ™‚é–“
                print("ç•¶å‰ç‚ºäº¤æ˜“æ™‚é–“ï¼Œè·³éè‚¡åƒ¹æ•¸æ“šä¸‹è¼‰ä»¥é¿å…å¹¹æ“¾")
                return False
        
        # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”è¼ƒæ–°ï¼ˆé¿å…é‡è¤‡ä¸‹è¼‰ï¼‰
        data_files_exist = all(
            os.path.exists(Path(DATA_DIR) / f"{ticker.replace(':','_')}_data_raw.csv")
            for ticker in TICKER_LIST
        )
        
        if data_files_exist:
            print("è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³éåˆå§‹ä¸‹è¼‰")
            return False
        
        return True
    except Exception as e:
        print(f"å‰è»Šæ©Ÿåˆ¶æª¢æŸ¥å¤±æ•—: {e}ï¼Œå…è¨±ä¸‹è¼‰")
        return True

# åœ¨ app å•Ÿå‹•æ™‚å‘¼å«ï¼ˆæ·»åŠ å‰è»Šæ©Ÿåˆ¶ï¼‰
TICKER_LIST = ['2330.TW', '2412.TW', '2414.TW', '^TWII']  # ä¾å¯¦éš›éœ€æ±‚èª¿æ•´
DATA_DIR = 'data'  # ä¾å¯¦éš›è·¯å¾‘èª¿æ•´

# å®‰å…¨çš„å•Ÿå‹•æ©Ÿåˆ¶
def safe_startup():
    """å®‰å…¨çš„å•Ÿå‹•å‡½æ•¸ï¼Œé¿å…ç·šç¨‹è¡çª"""
    try:
        # åªæœ‰åœ¨å‰è»Šæ©Ÿåˆ¶å…è¨±æ™‚æ‰ä¸‹è¼‰
        if should_download_price_data():
            ensure_all_price_data_up_to_date(TICKER_LIST, DATA_DIR)
        else:
            print("è‚¡åƒ¹æ•¸æ“šä¸‹è¼‰å·²ç”±å‰è»Šæ©Ÿåˆ¶é˜»æ­¢")
    except Exception as e:
        print(f"å•Ÿå‹•æ™‚æ•¸æ“šä¸‹è¼‰å¤±æ•—: {e}ï¼Œç¹¼çºŒå•Ÿå‹•æ‡‰ç”¨")

