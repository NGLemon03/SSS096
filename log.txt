1.

Configuring language runtimes...
# Python: 3.12
# Node.js: v20 (default: v22)
default -> 20 (-> v20.19.4)
Now using node v20.19.4 (npm v10.9.3)
# Ruby: 3.4.4 (default: 3.2.3)
mise ~/.config/mise/config.toml tools: ruby@3.4.4
# Rust: 1.88.0 (default: 1.89.0)
info: using existing install for '1.88.0-x86_64-unknown-linux-gnu'
info: default toolchain set to '1.88.0-x86_64-unknown-linux-gnu'

  1.88.0-x86_64-unknown-linux-gnu unchanged - rustc 1.88.0 (6b00bc388 2025-06-23)

# Go: go1.24.3 (default: go1.24.3)
# Swift: 6.1 (default: 6.1)
# PHP: 8.4 (default: 8.4)
Running auto setup...
Scanning repository (up to depth 3) for dependency configurations...
No known dependency configuration files found up to depth 3.
å¾Œé¢ç‰ˆæœ¬ä¸æ˜¯æ–°å¢äº†é¢¨éšªé–¥é–€å¥—ç”¨åœ¨å„ç­–ç•¥å—
ä½†ä¸çŸ¥é“ç‚ºä»€éº¼å¥—ç”¨å¾Œ å„æŒ‡æ¨™æœƒåªå‰©ä¸‹å¹³å‡æŒå€‰å¤©æ•¸,å…¶ä»–æŒ‡æ¨™éƒ½æ¶ˆå¤±äº† è«‹ä¿®å¾©


^Cbash: mise: No such file or directory
root@478772c9b537:/workspace# cd SSS096
bash: mise: No such file or directory
bash: mise: No such file or directory
root@478772c9b537:/workspace/SSS096# ls -a
bash: ls: No such file or directory
bash: mise: No such file or directory
The "bash: ls: No such file or directory" error suggests a missing or misconfigured PATH.

root@478772c9b537:/workspace/SSS096# command -v ls
bash: mise: No such file or directory
Hmm, the "mise" seems injected by the environment, maybe causing failures. Let's try /bin/ls.

root@478772c9b537:/workspace/SSS096# /bin/ls -a
.                                   assets                        run_enhanced_ensemble.py
..                                  config.yaml                   run_workflow.py
.git                                convert_results_to_trades.py  run_workflow_example.py
.vscode                             data                          runners
AGENTS.md                           debug_enhanced_data.py        setup.sh
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md  ensemble_wrapper.py           sss_backtest_outputs
ENSEMBLE_WORKFLOW.md                extract_params.py             sss_core
LOGGING_SYSTEM_UPDATE_SUMMARY.md    leverage.py                   test
PORTFOLIO_LEDGER_README.md          list.txt                      tools
SSS_EnsembleTab.py                  list_folder_structure.py      utils_payload.py
SSSv096.py                          presets                       version_history.py
analysis                            ruff.toml                     workflow_summary_20250812_023903.txt
app_dash.py                         run_enhanced_debug.py         workflow_summary_20250818_040656.txt
bash: mise: No such file or directory
I'll avoid aliases by directly using /usr/bin/git to check status.

root@478772c9b537:/workspace/SSS096# /usr/bin/git status --short
bash: mise: No such file or directory
I'm wondering if "mise" is blocking git status. Iâ€™ll check with which git.

root@478772c9b537:/workspace/SSS096# /usr/bin/which git
bash: mise: No such file or directory
Looks like the "mise" error could be from a pre-command function or alias. Let's explore AGENTS.md.

root@478772c9b537:/workspace/SSS096# /bin/cat AGENTS.md
# SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

SSS096 æ˜¯ä¸€å€‹è‚¡ç¥¨ç­–ç•¥å›æ¸¬èˆ‡åˆ†æç³»çµ±ï¼Œä¸»è¦åŒ…å«ï¼š
- ç­–ç•¥å›æ¸¬å¼•æ“ï¼ˆSSSv096.pyï¼‰
- Web UI ç•Œé¢ï¼ˆapp_dash.pyï¼‰
- å¢å¼·åˆ†ææ¨¡çµ„ï¼ˆanalysis/ï¼‰
- Ensemble ç­–ç•¥åŸ·è¡Œï¼ˆrunners/ï¼‰
- æ•¸æ“šè™•ç†èˆ‡è½‰æ›å·¥å…·

## ğŸ¯ å·¥ä½œé‡é»å€åŸŸ

### æ ¸å¿ƒæª”æ¡ˆ
- `SSSv096.py` - ä¸»è¦ç­–ç•¥å›æ¸¬å¼•æ“
- `app_dash.py` - Web UI ä¸»æ‡‰ç”¨
- `ensemble_wrapper.py` - Ensemble ç­–ç•¥åŒ…è£å™¨
- `analysis/` - åˆ†ææ¨¡çµ„ç›®éŒ„

### é¿å…ä¿®æ”¹çš„æª”æ¡ˆ
- `tools/quick_check.ps1` - è‡ªå‹•åŒ–æª¢æŸ¥è…³æœ¬ï¼ˆé™¤éå¿…è¦ï¼‰
- å·²æ¨™è¨˜ç‚º "past/" çš„èˆŠç‰ˆæœ¬æª”æ¡ˆ
- ç·¨è­¯å¾Œçš„ `.pyc` æª”æ¡ˆ

## ğŸ”§ é–‹ç™¼ç’°å¢ƒè¨­å®š

### Codex ç’°å¢ƒè¨­ç½®ï¼ˆæ¨è–¦ï¼‰
```bash
# åŸ·è¡Œè‡ªå‹•è¨­ç½®è…³æœ¬
chmod +x setup.sh
./setup.sh

# æ¸¬è©¦è¨­ç½®æ˜¯å¦æˆåŠŸ
python test_setup.py
```

### Python ç’°å¢ƒï¼ˆæ‰‹å‹•è¨­ç½®ï¼‰
```bash
# å®‰è£ä¾è³´å¥—ä»¶
pip install pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components

# æª¢æŸ¥ Python ç‰ˆæœ¬ï¼ˆå»ºè­° 3.8+ï¼‰
python --version
```

### ä»£ç†è¨­ç½®ï¼ˆå¦‚æœé‡åˆ° 403 éŒ¯èª¤ï¼‰
```bash
# æª¢æŸ¥ä»£ç†è­‰æ›¸
echo $CODEX_PROXY_CERT

# é…ç½® pip ä½¿ç”¨ä»£ç†
pip config set global.cert "$CODEX_PROXY_CERT"
pip config set global.trusted-host "proxy:8080"
```

### å°ˆæ¡ˆçµæ§‹å°èˆª
```bash
# å¿«é€ŸæŸ¥çœ‹ç›®éŒ„çµæ§‹
python list_folder_structure.py

# æŸ¥çœ‹ç‰¹å®šç›®éŒ„å…§å®¹
ls analysis/
ls runners/
```

## ğŸ§ª æ¸¬è©¦èˆ‡é©—è­‰

### å¿«é€Ÿæª¢æŸ¥
```bash
# åŸ·è¡Œè‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆé‡è¦ï¼ï¼‰
powershell -ExecutionPolicy Bypass -File tools\quick_check.ps1
```

### å›æ¸¬æ¸¬è©¦
```bash
# åŸ·è¡Œå–®ä¸€ç­–ç•¥å›æ¸¬
python SSSv096.py --strategy RMA_Factor --param_preset op.json

# åŸ·è¡Œ Ensemble ç­–ç•¥
python run_enhanced_ensemble.py --method majority --top_k 5
```

### UI æ¸¬è©¦
```bash
# å•Ÿå‹• Web UI
python app_dash.py
```

## ğŸ“ ç¨‹å¼ç¢¼è¦ç¯„

### è¨»è§£èˆ‡è¼¸å‡º
- **ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**é€²è¡Œè¨»è§£å’Œè¼¸å‡º
- ä¿®æ”¹ç´€éŒ„éœ€åŠ å…¥æ—¥æœŸæ™‚é–“æˆ³è¨˜
- è·¯å¾‘èªªæ˜æ ¼å¼ï¼š`#å­è³‡æ–™å¤¾/æª”æ¡ˆå`

### æ—¥èªŒè¨˜éŒ„
- ä½¿ç”¨ `analysis/logging_config.py` ä¸­çš„æ—¥èªŒå™¨
- é‡è¦æ“ä½œéœ€è¨˜éŒ„åˆ°æ—¥èªŒæª”æ¡ˆ
- éŒ¯èª¤è™•ç†éœ€åŒ…å«è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯

### è³‡æ–™æ ¼å¼
- æ—¥æœŸæ¬„ä½çµ±ä¸€ä½¿ç”¨ ISO æ ¼å¼ï¼š`YYYY-MM-DD`
- æ•¸å€¼æ¬„ä½ä½¿ç”¨ float é¡å‹
- é¿å…ä½¿ç”¨ä¸­æ–‡æ¬„ä½åç¨±ï¼ˆé™¤éå¿…è¦ï¼‰

## ğŸ” é™¤éŒ¯æŒ‡å—

### å¸¸è¦‹å•é¡Œ
1. **æ¨¡çµ„å°å…¥å¤±æ•—**ï¼šæª¢æŸ¥ `sys.path` å’Œç›¸å°å°å…¥
2. **æ•¸æ“šæ ¼å¼éŒ¯èª¤**ï¼šé©—è­‰ CSV æª”æ¡ˆçµæ§‹å’Œæ¬„ä½åç¨±
3. **è¨˜æ†¶é«”ä¸è¶³**ï¼šæª¢æŸ¥å¤§æ•¸æ“šé›†çš„è™•ç†æ–¹å¼

### é™¤éŒ¯å·¥å…·
```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(level=logging.DEBUG)

# æª¢æŸ¥æ•¸æ“šçµæ§‹
print(df.info())
print(df.head())
```

## ğŸ“Š æ•¸æ“šè™•ç†è¦ç¯„

### è¼¸å…¥æ•¸æ“š
- æ”¯æ´ CSVã€Excelã€JSON æ ¼å¼
- å¿…è¦æ¬„ä½ï¼šäº¤æ˜“æ—¥æœŸã€æ¬Šé‡è®ŠåŒ–ã€ç›ˆè™§%
- å¯é¸æ¬„ä½ï¼šäº¤æ˜“é¡å‹ã€åƒ¹æ ¼ã€æˆäº¤é‡

### è¼¸å‡ºæ•¸æ“š
- çµ±ä¸€ Schemaï¼šequityã€tradesã€daily_stateã€trade_ledger
- é¿å… KeyError å’Œæ¬„ä½ç¼ºå¤±
- æ”¯æ´å¤šç¨®è¼¸å‡ºæ ¼å¼

## ğŸš€ éƒ¨ç½²èˆ‡ç¶­è­·

### æª”æ¡ˆç®¡ç†
- å®šæœŸæ¸…ç†èˆŠçš„æ—¥èªŒå’Œå¿«å–æª”æ¡ˆ
- å‚™ä»½é‡è¦çš„é…ç½®å’Œçµæœæª”æ¡ˆ
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶è¿½è¹¤è®Šæ›´

### æ€§èƒ½å„ªåŒ–
- å¤§æ•¸æ“šé›†ä½¿ç”¨å¿«å–æ©Ÿåˆ¶
- é¿å…é‡è¤‡è¨ˆç®—
- ä½¿ç”¨é©ç•¶çš„æ•¸æ“šçµæ§‹

## âš ï¸ æ³¨æ„äº‹é …

### å®‰å…¨è€ƒé‡
- ä¸è¦ç¡¬ç·¨ç¢¼ API é‡‘é‘°
- é©—è­‰æ‰€æœ‰ç”¨æˆ¶è¼¸å…¥
- ä¿è­·æ•æ„Ÿæ•¸æ“š

### ç›¸å®¹æ€§
- ç¶­æŒèˆ‡ç¾æœ‰å·¥ä½œæµç¨‹çš„ç›¸å®¹æ€§
- æ¸¬è©¦æ‰€æœ‰æ•´åˆé»
- é¿å…ç ´å£ç¾æœ‰åŠŸèƒ½

## ğŸš€ Codex ç’°å¢ƒè¨­ç½®

### è‡ªå‹•è¨­ç½®
```bash
# åŸ·è¡Œè¨­ç½®è…³æœ¬
./setup.sh

# è…³æœ¬æœƒè‡ªå‹•ï¼š
# 1. é…ç½®ä»£ç†è¨­ç½®
# 2. å®‰è£æ‰€æœ‰å¿…è¦ä¾è³´
# 3. å‰µå»ºå›é€€æ—¥èªŒç³»çµ±
# 4. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```

### æ‰‹å‹•è¨­ç½®ï¼ˆå¦‚æœè‡ªå‹•è¨­ç½®å¤±æ•—ï¼‰
```bash
# 1. é…ç½®ä»£ç†
export PIP_CERT="$CODEX_PROXY_CERT"
export NODE_EXTRA_CA_CERTS="$CODEX_PROXY_CERT"

# 2. å®‰è£æ ¸å¿ƒä¾è³´
pip install --no-cache-dir pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components yfinance pyyaml joblib

# 3. å®‰è£åˆ†æå¥—ä»¶
pip install --no-cache-dir scikit-learn scipy statsmodels plotly kaleido

# 4. å‰µå»ºå¿…è¦ç›®éŒ„
mkdir -p analysis/log analysis/cache cache log results sss_backtest_outputs
```

### å¸¸è¦‹å•é¡Œè§£æ±º
- **pip install 403 éŒ¯èª¤**ï¼šæª¢æŸ¥ `$CODEX_PROXY_CERT` ç’°å¢ƒè®Šæ•¸
- **joblib å°å…¥å¤±æ•—**ï¼šä½¿ç”¨ `logging_config_fallback.py` å›é€€ç‰ˆæœ¬
- **æ¨¡çµ„è·¯å¾‘å•é¡Œ**ï¼šè¨­ç½® `PYTHONPATH` ç’°å¢ƒè®Šæ•¸

## ğŸ“ æŠ€è¡“æ”¯æ´

### å•é¡Œå›å ±
- æä¾›å®Œæ•´çš„éŒ¯èª¤è¨Šæ¯å’Œå †ç–Šè¿½è¹¤
- åŒ…å«é‡ç¾æ­¥é©Ÿå’Œç’°å¢ƒä¿¡æ¯
- æª¢æŸ¥ç›¸é—œçš„æ—¥èªŒæª”æ¡ˆ

### æ–‡æª”æ›´æ–°
- ä¿®æ”¹åŠŸèƒ½æ™‚åŒæ­¥æ›´æ–°ç›¸é—œæ–‡æª”
- ä½¿ç”¨æ¸…æ™°çš„ç¯„ä¾‹å’Œèªªæ˜
- ä¿æŒæ–‡æª”çš„æ™‚æ•ˆæ€§

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€å¾Œæ›´æ–°ï¼š** 2025-08-18
**é©ç”¨æ–¼ï¼š** SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—
bash: mise: No such file or directory
Let's search for AGENTS.md in the subdirectories to guide necessary modifications.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "é¢¨éšªé–¥" -n
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md
36:- åœ¨ç¹ªè£½é¢¨éšªé–¥é–€æ™‚åºåœ–å‰æª¢æŸ¥å¿…è¦æ¬„ä½

SSS_EnsembleTab.py
1287:# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
1292:    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š

analysis/ENHANCED_ANALYSIS_README.md
11:1. **é¢¨éšªé–¥é–€å›æ¸¬** - åœ¨æ¥µç«¯è¡Œæƒ…ä¸‹æš«åœåŠ ç¢¼æ©Ÿåˆ¶
17:### 1. é¢¨éšªé–¥é–€å›æ¸¬ (`risk_valve_backtest`)
141:- é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åºåœ–
184:### 1. é¢¨éšªé–¥é–€ç­–ç•¥

analysis/UI_INTEGRATION_GUIDE.md
11:1. é¢¨éšªé–¥é–€å›æ¸¬
260:- **è‡ªå®šç¾©åŸºæº–æ•¸æ“š**ï¼šä¸Šå‚³åŸºæº–æ•¸æ“šé€²è¡Œé¢¨éšªé–¥é–€åˆ†æ

analysis/enhanced_trade_analysis.py
4:æ•´åˆé¢¨éšªé–¥é–€å›æ¸¬ã€äº¤æ˜“è²¢ç»æ‹†è§£ã€åŠ ç¢¼æ¢¯åº¦å„ªåŒ–
97:        é¢¨éšªé–¥é–€å›æ¸¬
109:        print("=== é¢¨éšªé–¥é–€å›æ¸¬åˆ†æ ===")
112:        # æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼
115:        # è¨ˆç®—é¢¨éšªé–¥é–€æ•ˆæœ
121:        """æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼"""
123:            print("è­¦å‘Šï¼šç¼ºå°‘åŸºæº–æ•¸æ“šï¼Œç„¡æ³•é€²è¡Œé¢¨éšªé–¥é–€å›æ¸¬")
179:        # é¢¨éšªé–¥é–€è§¸ç™¼æ¢ä»¶
184:        # ç¶œåˆé¢¨éšªé–¥é–€
194:        """è¨ˆç®—é¢¨éšªé–¥é–€å°ç¸¾æ•ˆçš„å½±éŸ¿"""
200:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'æ—¥æœŸ' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
204:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
207:        # æ‰¾å‡ºé¢¨éšªé–¥é–€è§¸ç™¼çš„æœŸé–“
243:        print(f"é¢¨éšªé–¥é–€è§¸ç™¼æœŸé–“æ•¸ï¼š{len(risk_periods)}")
555:        # 1. é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº
559:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
561:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
575:                               color='red', s=50, alpha=0.8, label='é¢¨éšªé–¥é–€è§¸ç™¼')
578:                ax1.set_title('é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº')
637:    print("2. åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬ï¼šanalyzer.risk_valve_backtest()")

app_dash.py
445:                        html.Label("é¢¨éšªé–¥é–€ CAP", style={"fontSize":"12px","color":"#888"}),
455:                        dbc.Checkbox(id='force-valve-trigger', value=False, label="å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼ˆæ¸¬è©¦ç”¨ï¼‰", style={"font
Size":"11px","color":"#dc3545"}),
456:                        html.Small("ğŸ’¡ å‹¾é¸å¾Œå°‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼Œç”¨æ–¼æ¸¬è©¦åŠŸèƒ½", style={"color":"#dc3545","fontSize":"10px"})
460:                    # === é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤ºå€åŸŸ ===
464:                        html.Small("ğŸ”’ é¢¨éšªé–¥é–€èªªæ˜:", style={"color":"#28a745","fontWeight":"bold","fontSize":"11px"}),
573:# --------- é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ---------
587:    """å‹•æ…‹æ›´æ–°é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤º"""
588:    logger.info(f"=== é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ===")
598:        logger.info("é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
600:            html.Small("ğŸ”´ é¢¨éšªé–¥é–€æœªå•Ÿç”¨", style={"color":"#dc3545","fontWeight":"bold"}),
655:                            # åˆ¤æ–·æ˜¯å¦éœ€è¦è§¸ç™¼é¢¨éšªé–¥é–€
658:                            # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
661:                                logger.info(f"å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
663:                            # è¨˜éŒ„é¢¨éšªé–¥é–€ç‹€æ…‹åˆ°æ—¥èªŒ
665:                            logger.info(f"é¢¨éšªé–¥é–€é–€æª»: {atr_ratio}, ç•¶å‰æ¯”å€¼: {atr_ratio_current:.4f}")
666:                            logger.info(f"é¢¨éšªé–¥é–€è§¸ç™¼: {'æ˜¯' if valve_triggered else 'å¦'}")
667:                            logger.info(f"é¢¨éšªé–¥é–€ç‹€æ…‹: {'ğŸ”´ è§¸ç™¼' if valve_triggered else 'ğŸŸ¢ æ­£å¸¸'}")
680:                                    html.Small(f"{status_icon} é¢¨éšªé–¥é–€ç‹€æ…‹: {status_text}",
742:        logger.error(f"é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–°å¤±æ•—: {e}")
797:    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
810:        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
811:        logger.info(f"[{strat}] é¢¨éšªé–¥é–€é–‹é—œç‹€æ…‹: global_apply={global_apply}, é¡å‹={type(global_apply)}")
813:            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")
836:                        logger.warning(f"[{strat}] ATR(60) å€¼ç•°å¸¸: {atr_60_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
840:                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
851:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
856:            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")
864:            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
870:            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
941:                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
943:                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
945:                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
981:                # è¨»è§£æ‰åŸæœ¬çš„ç„¡æ¢ä»¶é¢¨éšªé–¥é–€èª¿æ•´ï¼ˆæœƒé€ æˆ floor æ–¹å‘éŒ¯èª¤ï¼‰
983:                #     logger.info(f"[{strat}] Ensemble ç­–ç•¥æ‡‰ç”¨é¢¨éšªé–¥é–€: åŸå§‹ delta_cap={ensemble_params.delta_cap}, floor={
ensemble_params.floor}")
1020:                # --- æ–°å¢ï¼šåªåœ¨ ATR è§¸ç™¼æ™‚å•Ÿç”¨é¢¨éšªé–¥é–€ ---
1047:                                # å¢åŠ é¢¨éšªé–¥é–€è§¸ç™¼çš„è©³ç´°è³‡è¨Š
1049:                                    logger.info(f"[{strat}] ğŸ”´ é¢¨éšªé–¥é–€è§¸ç™¼ï¼ATRæ¯”å€¼({ratio:.4f}) > é–€æª»({atr_ratio})")
1051:                                    logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼ŒATRæ¯”å€¼({ratio:.4f}) <= é–€æª»({atr_ratio})")
1063:                # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
1066:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
1078:                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
1093:                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
1095:                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1108:                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
1116:                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
1175:                    "applied": False,  # å…¶ä»–ç­–ç•¥é¡å‹æš«æ™‚ä¸æ”¯æ´é¢¨éšªé–¥é–€
1205:        # === å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨ï¼ˆèˆ‡å¢å¼·åˆ†æä¸€è‡´ï¼‰ ===
1211:                logger.warning(f"[{strat}] daily_state ä¸å« 'w'ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1217:                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— ATR20/60ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1316:                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
1318:                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")
1701:            # === è¨ˆç®—é¢¨éšªé–¥é–€å¾½ç« å…§å®¹ ===
1713:                "ğŸ›¡ï¸ é¢¨éšªé–¥é–€ï¼š" + valve_badge_text + ((" | " + " | ".join(valve_badge_extra)) if valve_badge_extra else ""),
1968:                html.Summary("é¢¨éšªé–¥é–€å›æ¸¬"),
1981:                    html.Button("åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬", id="run-rv", n_clicks=0, style={"marginTop":"8px"})
2369:# --------- å¢å¼·åˆ†æ Callbackï¼šé¢¨éšªé–¥é–€å›æ¸¬ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---------
2434:        return f"é¢¨éšªé–¥é–€å›æ¸¬åŸ·è¡Œå¤±æ•—: {e}", no_update, no_update
3488:        return go.Figure(), "âŒ ç„¡æ³•å¾é¢¨éšªé–¥é–€å›æ¸¬çš„åƒæ•¸çµ„åˆä¸­å–å¾—è³‡æ–™"
bash: mise: No such file or directory
I'm searching for "avg_holding" or "å¹³å‡æŒå€‰" to understand why metrics vanish after applying the risk valve.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "å¹³å‡æŒå€‰" -n
SSSv096.py
927:    è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸.
933:        float: å¹³å‡æŒå€‰å¤©æ•¸.
1828:            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1864:    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
1867:    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
2293:            # é¡¯ç¤ºç­–ç•¥æ‘˜è¦,å‚³å…¥ trade_df ä»¥è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸

analysis/optuna_16.py
10:v10-12  æ”¯æ´å–®ä¸€/éš¨æ©Ÿ/ä¾åºæ•¸æ“šæºæ¨¡å¼,æ–°å¢å¹³å‡æŒå€‰å¤©æ•¸,ç›¸é—œä¿‚æ•¸åˆ†æ,ä¸¦è‡ªå‹•è¼¸å‡ºç†±åœ–,è©¦é©—çµæœèˆ‡æœ€ä½³åƒæ•¸è‡ªå‹•è¼¸å‡º CSV/JSON

analysis/OSv3.py
82:        'avg_hold_days': 'å¹³å‡æŒå€‰å¤©æ•¸ï¼šå¹³å‡æ¯æ¬¡äº¤æ˜“çš„æŒå€‰æ™‚é–“',

app_dash.py
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
bash: mise: No such file or directory
I'm checking for a bug where the metrics dictionary gets overwritten after applying the risk valve.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "avg_holding_period" -n
SSSv096.py
849:        'avg_holding_period': np.nan,# æ–°å¢
1808:        elif k in ["max_drawdown_duration", "avg_holding_period"]:
1828:            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1865:    avg_holding_period = calculate_holding_periods(trade_df)
1868:    metrics['avg_holding_period'] = avg_holding_period

app_dash.py
1545:            metrics['avg_holding_period'] = avg_holding
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1573:                elif k in ["max_drawdown_duration", "avg_holding_period"]:
bash: mise: No such file or directory
I suspect the bug could be in how "rv" results are handled in SSS_EnsembleTab.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "avg_holding_period" SSS_EnsembleTab.py
bash: mise: No such file or directory
Looking into the "mise" error and how risk valve metrics might be affected by code reset.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "metrics =" -n SSS_EnsembleTab.py
bash: mise: No such file or directory
I'll open the SSS_EnsembleTab around line 1075 to check the metrics merging and risk valve effect.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1060,1105p' SSS_EnsembleTab.py
                delta_cap=delta_cap,
                majority_k=6,  # é€™å€‹å€¼æœƒè¢« majority_k_pct è¦†è“‹
                min_cooldown_days=min_cooldown_days,
                min_trade_dw=min_trade_dw,
            )

            cost = CostParams(
                buy_fee_bp=buy_fee_bp,
                sell_fee_bp=sell_fee_bp,
                sell_tax_bp=sell_tax_bp,
            )

            cfg = RunConfig(
                ticker=ticker,
                method=method,
                params=params,
                cost=cost,
                majority_k_pct=majority_k_pct
            )

            # é‹è¡Œ ensemble
            with st.spinner("åŸ·è¡Œä¸­..."):
                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

            # é¡¯ç¤ºçµæœ
            st.success(f"åŸ·è¡Œå®Œæˆ: {method_name}")

            # ç¸¾æ•ˆæŒ‡æ¨™
            col6, col7, col8 = st.columns(3)
            with col6:
                st.metric("ç¸½å ±é…¬ç‡", f"{stats.get('total_return', 0):.2%}")
                st.metric("å¹´åŒ–å ±é…¬ç‡", f"{stats.get('annual_return', 0):.2%}")
            with col7:
                st.metric("æœ€å¤§å›æ’¤", f"{stats.get('max_drawdown', 0):.2%}")
                st.metric("å¤æ™®æ¯”ç‡", f"{stats.get('sharpe_ratio', 0):.2f}")
            with col8:
                st.metric("å¡ç‘ªæ¯”ç‡", f"{stats.get('calmar_ratio', 0):.2f}")
                st.metric("äº¤æ˜“æ¬¡æ•¸", stats.get('num_trades', 0))

            # === æ–°å¢ï¼šäº¤æ˜“æ˜ç´°é¡¯ç¤º ===
            st.subheader("äº¤æ˜“æ˜ç´° (trade_ledger)")
            if trade_ledger is not None and not trade_ledger.empty:
                # ä½¿ç”¨ normalize_trades_for_ui æ¨™æº–åŒ–äº¤æ˜“æ˜ç´°
                trades_ui = normalize_trades_for_ui(trade_ledger)

                # === æ–°å¢ï¼šåˆªé™¤è²»ç”¨æ¬„ä½ï¼ˆåƒ…UIå±¤ç§»é™¤é¡¯ç¤ºï¼‰ ===
bash: mise: No such file or directory
I'm thinking the issue might be in the risk valve apply function, like "apply_risk_valve".

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "apply_risk_valve" -n
bash: mise: No such file or directory
Iâ€™ll check for â€œrisk_valveâ€ in SSSv096 to explore its integration with daily weights.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "risk_valve" -n SSSv096.py
bash: mise: No such file or directory
I think the risk valve could be mishandling dictionary merging, so I'll search for related lines.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "rv\['metrics'\]" -n SSS_EnsembleTab.py
bash: mise: No such file or directory
I'll search around the risk valve code in SSS_EnsembleTab, especially near lines 1700-1200.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "metrics" SSS_EnsembleTab.py | head -n 50
bash: head: No such file or directory
bash: mise: No such file or directory
Looks like the head command isn't in the path. I'll use /bin/head instead.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "metrics" SSS_EnsembleTab.py | /bin/head -n 50
        "metrics": {
bash: mise: No such file or directory
I'll switch to using rg -n to get line numbers instead of just the first chunk.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics" SSS_EnsembleTab.py | /bin/head -n 50
1405:        "metrics": {
bash: mise: No such file or directory
I'll check line 1405 and the surrounding area to see what's going on with "metrics".

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1390,1420p' SSS_EnsembleTab.py
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)

    return {
        "signals": sig,
        "weights_orig": w,
        "weights_valve": w2,
        "daily_state_orig": daily_state1,
        "daily_state_valve": daily_state2,
        "trade_ledger_orig": ledger1,
        "trade_ledger_valve": ledger2,
        "metrics": {
            "pf_orig": float(pf1), "pf_valve": float(pf2),
            "mdd_orig": float(mdd1), "mdd_valve": float(mdd2),
            "right_tail_sum_orig": float(rt1), "right_tail_sum_valve": float(rt2),
            "right_tail_reduction": float(right_tail_reduction)
        }
    }


# === PATCH: äº¤æ˜“è²¢ç»æ‹†è§£ï¼ˆæŒ‰åŠ ç¢¼/æ¸›ç¢¼éšæ®µï¼‰===
def trade_contribution_by_phase(trade_ledger: pd.DataFrame,
                                daily_equity: pd.Series,
                                min_spacing_days: int = 0,
                                cooldown_days: int = 0) -> pd.DataFrame:
    """å°‡äº¤æ˜“ä¾ã€æ¬Šé‡è®ŠåŒ–æ­£/è² ã€åˆ†æ®µï¼Œçµ±è¨ˆå„éšæ®µçš„è³£å‡ºå¯¦ç¾å ±é…¬èˆ‡è©²æ®µå…§çš„ MDDã€‚"""
    tl = trade_ledger.copy()
bash: mise: No such file or directory
Seems like the metrics dictionary is getting replaced after the risk valve. Let's search for merging.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "rv\['metrics'\]" -g "SSS_EnsembleTab.py"
bash: mise: No such file or directory
Iâ€™m trying variations of search terms to home in on the right metrics or functions.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1280,1320p' SSS_EnsembleTab.py
    # é¢¨éšªè§¸ç™¼ï¼šå…©æ–œç‡çš† < é–€æª»ï¼Œä¸” ATR æŠ¬é ­è¶…éå€ç‡
    b["risk_trigger"] = (b["slope_20d"] < slope20_thresh) & \
                        (b["slope_60d"] < slope60_thresh) & \
                        (b["atr_ratio"] > atr_ratio_mult)
    return b[["slope_20d", "slope_60d", "atr", "atr_ratio", "risk_trigger"]]


# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
def apply_valve_to_weights(w: pd.Series,
                           risk_trigger: pd.Series,
                           mode: str = "cap",   # "cap" æˆ– "ban_add"
                           cap_level: float = 0.5) -> pd.Series:
    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š
       - cap: é¢¨éšªæ—¥å°‡ w é™åœ¨ cap_level ä»¥ä¸‹
       - ban_add: é¢¨éšªæ—¥ç¦æ­¢ã€Œæé«˜ã€å€‰ä½ï¼ˆå…è¨±æ¸›å€‰ï¼‰ï¼Œç”¨è¿­ä»£æ³•ç¢ºä¿å–®èª¿"""
    w = w.copy().reindex(risk_trigger.index).ffill().fillna(0.0)
    out = w.copy()

    if mode == "cap":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        out[mask] = np.minimum(out[mask], cap_level)

    elif mode == "ban_add":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        for i in range(1, len(out)):
            if mask.iloc[i] and (out.iloc[i] > out.iloc[i-1]):
                out.iloc[i] = out.iloc[i-1]  # ç¦æ­¢å¢åŠ 
    else:
        raise ValueError("mode éœ€ç‚º 'cap' æˆ– 'ban_add'")
    return out.clip(0.0, 1.0)


# === PATCH: ç”± trade_ledger å»ºç«‹ã€Œæ¯ç­†ç›ˆè™§%ã€èˆ‡å¿«ç…§ï¼ˆå¼•ç”¨ re.txt åšæ³•ï¼‰===
def build_trade_snapshots_from_ledger(trade_ledger: pd.DataFrame) -> pd.DataFrame:
    """ä»¥äº¤æ˜“å¾Œæ¬Šç›Š/ç¾é‡‘ç‚ºè³‡ç”¢å¿«ç…§ï¼Œè¨ˆç®—æ¯ç­†ç›ˆè™§%ï¼ˆåªåœ¨æœ‰è®Šå‹•çš„äº¤æ˜“æ—¥è¨˜ä¸€ç­†ï¼‰"""
    df = trade_ledger.copy()
    # æ¬„åå°é½Š
    if "date" in df.columns: df.rename(columns={"date": "äº¤æ˜“æ—¥æœŸ"}, inplace=True)
    if "type" in df.columns: df.rename(columns={"type": "äº¤æ˜“é¡å‹"}, inplace=True)
    if "equity_after" in df.columns: df.rename(columns={"equity_after": "äº¤æ˜“å¾Œæ¬Šç›Š"}, inplace=True)
    if "cash_after" in df.columns: df.rename(columns={"cash_after": "äº¤æ˜“å¾Œç¾é‡‘"}, inplace=True)
bash: mise: No such file or directory
I suspect the risk valve is replacing metrics, not merging them as expected. Let's search.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics\s*=\s*{" -l
rg: regex parse error:
    (?:metrics\s*=\s*{)
                      ^
error: repetition quantifier expects a valid decimal
bash: mise: No such file or directory
I'll escape the curly braces (\{) to fix the parse error.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics\s*=\s*\{" -l
SSSv096.py
analysis/optuna_16.py
analysis/OSv3.py
analysis/walk_forward_v14.py
analysis/stress_testv1.py
analysis/past/risk_enhanced_clustering_analysis_v2.py
analysis/OSv3_6270833.py
bash: mise: No such file or directory
I'll start by checking around where SSSv096 defines the metrics dictionary.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1800,1880p' SSSv096.py
    # å…ˆæŠŠ metrics çš„ key/value è½‰æˆã€Œé¡¯ç¤ºç”¨çš„ labelã€å’Œã€Œæ ¼å¼åŒ–å¾Œçš„ value å­—ä¸²ã€
    items = []
    for k, v in metrics.items():
        # ä¾æ“š k æ±ºå®šè¦ä¸è¦ä»¥ç™¾åˆ†æ¯”ã€æˆ–å°æ•¸ã€æˆ–ç´”æ–‡å­—ä¾†æ ¼å¼åŒ–
        if k in ["total_return", "annual_return", "win_rate", "max_drawdown", "annualized_volatility", "avg_win", "avg_loss"]:
            txt = f"{v:.2%}" if pd.notna(v) else ""
        elif k in ["calmar_ratio", "sharpe_ratio", "sortino_ratio", "payoff_ratio", "profit_factor"]:
            txt = f"{v:.2f}" if pd.notna(v) else ""
        elif k in ["max_drawdown_duration", "avg_holding_period"]:
            txt = f"{v:.1f} å¤©" if pd.notna(v) else ""
        elif k in ["num_trades", "max_consecutive_wins", "max_consecutive_losses"]:
            txt = str(int(v)) if pd.notna(v) else ""
        else:
            # å…¶ä»–å°±å…ˆç›¡é‡ç•¶ç´”æ–‡å­—é¡¯ç¤º
            txt = f"{v}"
        # æŠŠå­—å…¸ key â†’ ä¸­æ–‡é¡¯ç¤º label
        label_map = {
            "total_return": "ç¸½å›å ±ç‡",
            "annual_return": "å¹´åŒ–å›å ±ç‡",
            "win_rate": "å‹ç‡",
            "max_drawdown": "æœ€å¤§å›æ’¤",
            "max_drawdown_duration": "å›æ’¤æŒçºŒ",
            "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
            "sharpe_ratio": "å¤æ™®æ¯”ç‡",
            "sortino_ratio": "ç´¢æè«¾æ¯”ç‡",
            "payoff_ratio": "ç›ˆè™§æ¯”",
            "profit_factor": "ç›ˆè™§å› å­",
            "num_trades": "äº¤æ˜“æ¬¡æ•¸",
            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
            "annualized_volatility": "å¹´åŒ–æ³¢å‹•ç‡",
            "max_consecutive_wins": "æœ€å¤§é€£çºŒç›ˆåˆ©",
            "max_consecutive_losses": "æœ€å¤§é€£çºŒè™§æ",
            "avg_win": "å¹³å‡ç›ˆåˆ©",
            "avg_loss": "å¹³å‡è™§æ",
        }
        label = label_map.get(k, k)
        items.append((label, txt))

    # é–‹å§‹ç”¢ç”Ÿ HTML:å¤–å±¤ä¸€å€‹ flex container,å…§å±¤æ¯çµ„(æŒ‡æ¨™+æ•¸å€¼) éƒ½æ˜¯ flex item
    html = """
<div style="display:flex;flex-wrap:wrap;gap:12px;align-items:flex-start;">
"""
    for label, val in items:
        html += f"""
  <div style="flex:0 1 150px;border:1px solid #444;border-radius:4px;padding:8px 12px;background:#1a1a1a;">
    <div style="font-size:14px;color:#aaa;">{label}</div>
    <div style="font-size:20px;font-weight:bold;color:#fff;margin-top:4px;">{val}</div>
  </div>
"""
    html += "</div>"

    # å»æ‰å¤šé¤˜ç¸®æ’,é¿å…é–‹é ­ç©ºç™½è¢«è§£è®€æˆ code block
    html = textwrap.dedent(html)

    st.markdown(html, unsafe_allow_html=True)

def display_strategy_summary(strategy: str, params: Dict, metrics: Dict, smaa_source: str, trade_df: pd.DataFrame):
    """
    é¡¯ç¤ºç­–ç•¥åƒæ•¸èˆ‡å›æ¸¬ç¸¾æ•ˆæ‘˜è¦,ä½¿ç”¨ HTML Flexbox å¡ç‰‡å±•ç¤º.
    """
    # åƒæ•¸å±•ç¤º
    param_display = {k: v for k, v in params.items() if k != "strategy_type"}
    st.write("**åƒæ•¸è¨­å®š**: " + ", ".join(f"{k}: {v}" for k, v in param_display.items()))

    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
    avg_holding_period = calculate_holding_periods(trade_df)

    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
    metrics['avg_holding_period'] = avg_holding_period

    # ç¸¾æ•ˆæŒ‡æ¨™å±•ç¤º
    if metrics:
        display_metrics_flex(metrics)
    else:
        st.warning("å°šæœªåŸ·è¡Œå›æ¸¬,ç„¡æ³•é¡¯ç¤ºç¸¾æ•ˆæŒ‡æ¨™.")
# --- ä¸»æ‡‰ç”¨ç¨‹å¼ ---
def run_app():
    st.set_page_config(layout="wide")
    st.sidebar.title("00631Lç­–ç•¥ç³»çµ±")
    page = st.sidebar.selectbox("ğŸ“‘ é é¢å°èˆª", ["ç­–ç•¥å›æ¸¬", "æŠ•è³‡çµ„åˆæ¬Šé‡åˆ†æ", "å„ç‰ˆæœ¬æ²¿é©ç´€éŒ„", "å¿«å–ç®¡ç†"])

bash: mise: No such file or directory
It seems the risk valve might be resetting the metrics to only include avg_holding_period.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "risk_valve" -g "SSS_EnsembleTab.py"
SSS_EnsembleTab.py
1241:def compute_risk_valve_signals(benchmark_df: pd.DataFrame,
1359:def risk_valve_backtest(open_px: pd.Series,
1371:    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
bash: mise: No such file or directory
The bug arises when applying risk valve to strategies; metrics only show avg holding days.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "apply_valve" -g "*.py"
SSS_EnsembleTab.py
1288:def apply_valve_to_weights(w: pd.Series,
1373:    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)
bash: mise: No such file or directory
The bug might be in the metrics display from the risk valve code. I'm checking for possible overwriting of the metrics variable.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics = {}" -g "SSSv096.py"
rg: regex parse error:
    (?:metrics = {})
                  ^
error: repetition quantifier expects a valid decimal
bash: mise: No such file or directory
I realized I missed something because of the regex. I need to escape the braces.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics = \{\}" SSSv096.py
2480:            performance_metrics = {}
bash: mise: No such file or directory
I see line 2480 sets performance_metrics to an empty dictionaryâ€”let's check the surrounding code.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '2440,2490p' SSSv096.py
    # åˆå§‹æ¬Šé‡
    n_strategies = len(equity_curves)
    current_weights = {strategy: 1.0 / n_strategies for strategy in equity_curves.keys()}

    # é‡æ–°å¹³è¡¡é »ç‡è½‰æ›
    if rebalance_freq == "å­£åº¦":
        rebalance_months = 3
    elif rebalance_freq == "å¹´åº¦":
        rebalance_months = 12
    else:
        rebalance_months = None

    # è¨ˆç®—é‡æ–°å¹³è¡¡æ—¥æœŸ
    rebalance_dates = []
    if rebalance_months:
        current_date = pd.to_datetime(common_dates[0])
        end_date = pd.to_datetime(common_dates[-1])

        while current_date <= end_date:
            rebalance_dates.append(current_date)
            # è¨ˆç®—ä¸‹ä¸€å€‹é‡æ–°å¹³è¡¡æ—¥æœŸ
            if current_date.month + rebalance_months > 12:
                year = current_date.year + 1
                month = (current_date.month + rebalance_months) % 12
                if month == 0:
                    month = 12
            else:
                year = current_date.year
                month = current_date.month + rebalance_months

            current_date = current_date.replace(year=year, month=month)

    # åŸ·è¡ŒæŠ•è³‡çµ„åˆè¨ˆç®—
    for i, date in enumerate(common_dates):
        if i == 0:
            continue

        # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¹³è¡¡
        if rebalance_months and pd.to_datetime(date) in rebalance_dates:
            # è¨ˆç®—æ–°çš„æ¬Šé‡
            performance_metrics = {}
            for strategy_name, equity_curve in equity_curves.items():
                if date in equity_curve.index:
                    # è¨ˆç®—éå»æœŸé–“çš„è¡¨ç¾
                    lookback_start = max(0, i - lookback_period)
                    if lookback_start < len(equity_curve):
                        past_equity = equity_curve.iloc[lookback_start:i+1]
                        if len(past_equity) > 1:
                            if weight_method == "ç­‰æ¬Šé‡":
                                performance_metrics[strategy_name] = 1.0
                            elif weight_method == "å¤æ™®æ¯”ç‡æ¬Šé‡":
bash: mise: No such file or directory
Looks like the bug is due to risk valve replacing strategy metrics, leaving only avg_holding.

The issue likely occurs when metrics is reset to {} in display_strategy_summary after risk valve.

Inside display_strategy_summary, metrics defaults to {}, adding avg_holding_period. Likely, caller omits original metrics.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "display_strategy_summary" -g "SSSv096.py"
SSSv096.py
1856:def display_strategy_summary(strategy: str, params: Dict, metrics: Dict, smaa_source: str, trade_df: pd.DataFrame):
2294:            display_strategy_summary(strategy, params, result['metrics'], smaa_source, result['trade_df'])
bash: mise: No such file or directory
It looks like display_strategy_summary is triggered near line 2294 in run_app/CLI.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '2240,2310p' SSSv096.py
                ticker,
                start_date=start_date_input,
                end_date=end_date_input if end_date_input else None,
                smaa_source=smaa_source,
                force_update=force_update
            )

            # è¨ˆç®—å›æ¸¬çµæœ
            if strategy_type == 'ssma_turn':
                calc_keys = ['linlen', 'factor', 'smaalen', 'prom_factor', 'min_dist',
                             'buy_shift', 'exit_shift', 'vol_window', 'signal_cooldown_days',
                             'quantile_win']
                ssma_params = {k: v for k, v in params.items() if k in calc_keys}
                # 2) ç¢ºä¿ stop_loss å‚³éçµ¦ backtest_unified
                backtest_params = ssma_params.copy()
                backtest_params['stop_loss'] = params.get('stop_loss', 0.0)
                df_ind, buy_dates, sell_dates = compute_ssma_turn_combined(
                    df_raw, df_factor, **ssma_params, smaa_source=smaa_source
                )
                if df_ind.empty:
                    st.warning(f"{strategy} ç­–ç•¥è¨ˆç®—å¤±æ•—,å¯èƒ½æ˜¯æ•¸æ“šä¸è¶³.")
                    continue
                result = backtest_unified(
                    df_ind, strategy_type, backtest_params, buy_dates, sell_dates,
                    discount=discount, trade_cooldown_bars=trade_cooldown_bars, bad_holding=bad_holding
                )
            else:
                if strategy_type == 'single':
                    df_ind = compute_single(
                        df_raw, df_factor, params["linlen"], params["factor"], params["smaalen"],
                        params["devwin"], smaa_source=smaa_source
                    )
                elif strategy_type == 'dual':
                    df_ind = compute_dual(
                        df_raw, df_factor, params["linlen"], params["factor"], params["smaalen"],
                        params["short_win"], params["long_win"], smaa_source=smaa_source
                    )
                elif strategy_type == 'RMA':
                    df_ind = compute_RMA(
                        df_raw, df_factor, params["linlen"], params["factor"], params["smaalen"],
                        params["rma_len"], params["dev_len"], smaa_source=smaa_source
                    )
                if df_ind.empty:
                    st.warning(f"{strategy} ç­–ç•¥è¨ˆç®—å¤±æ•—,å¯èƒ½æ˜¯æ•¸æ“šä¸è¶³.")
                    continue
                result = backtest_unified(
                    df_ind, strategy_type, params, discount=discount,
                    trade_cooldown_bars=trade_cooldown_bars, bad_holding=bad_holding
                )

            results[strategy] = (df_ind, result['trades'], result['trade_df'],
                                 result['signals_df'], result['metrics'])

            # é¡¯ç¤ºç­–ç•¥æ‘˜è¦,å‚³å…¥ trade_df ä»¥è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
            display_strategy_summary(strategy, params, result['metrics'], smaa_source, result['trade_df'])

            # é¡¯ç¤ºåœ–è¡¨èˆ‡äº¤æ˜“æ˜ç´°
            has_trades = ('trade_df' in result) and (result['trade_df'] is not None) and (not result['trade_df'].empty)
            if has_trades:
                st.plotly_chart(plot_stock_price(df_raw, result['trade_df'], ticker),
                                use_container_width=True, key=f"stock_price_{strategy}")

                # âœ… å„ªå…ˆä½¿ç”¨ daily_stateï¼ˆEnsemble æ­£ç¢ºè·¯å¾‘ï¼‰ï¼Œæ²’æœ‰å†é€€å› trade_df
                ds_or_trades = result.get('daily_state', result.get('trade_df'))
                # æ¨™æº–åŒ– daily_state å†ç¹ªåœ–ï¼Œé¿å…æ¬„ä½åèª¤ç”¨
                if ds_or_trades is not None and not ds_or_trades.empty:
                    ds_or_trades = normalize_daily_state(ds_or_trades)

                # ğŸ” åµéŒ¯è¼¸å‡ºï¼šæŠŠçœŸæ­£è¦ç•«çš„è³‡æ–™ç›´æ¥åå‡º csv
                try:
                    print(f"ğŸ” é–‹å§‹åµéŒ¯è¼¸å‡ºï¼š{strategy}")
bash: mise: No such file or directory
I need to locate the backtest_unified function to see how metrics are computed.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "backtest_unified" -g "*.py"
SSSv096.py
39:    "compute_ssma_turn_combined", "backtest_unified",
953:def backtest_unified(
1360:            result = backtest_unified(df_ind, strategy_type, params, buy_dates, sell_dates,
1380:            result = backtest_unified(df_ind, strategy_type, params, discount=discount, trade_cooldown_bars=trade_cooldown_
bars, bad_holding=bad_holding)
2057:                            result = backtest_unified(
2082:                            result = backtest_unified(
2169:                    result = backtest_unified(
2192:                    result = backtest_unified(
2253:                # 2) ç¢ºä¿ stop_loss å‚³éçµ¦ backtest_unified
2262:                result = backtest_unified(
2285:                result = backtest_unified(

version_history.py
121:      <li>ç‰ˆæœ¬è™Ÿæå‡ç‚º 092ï¼Œæ–°å¢ <code>backtest_unified</code> å‡½å¼ä»¥é›†ä¸­ç®¡ç†å›æ¸¬æµç¨‹ã€‚</li>
Error: Output for session 'shell' contained a line exceeding the max of 4096 bytes (observed at least 4429 bytes).

The byte sequence which exceeded the limit started with: b'\x1b[0m\x1b[32m127\x1b[0m:  <'

The exec session has been deleted. Please start a new session.

Tip - rerun the command and extract only what you need, e.g.:
  * grep -nE 'PATTERN' FILE | cut -c1-200
  * grep -o 'PATTERN' FILE
  * jq -r '.json.path' FILE
  * grep -a PATTERN FILE
I'm going to change my directory back to the repo.



root@478772c9b537:/workspace# cd /workspace/SSS096
root@478772c9b537:/workspace/SSS096# ^C
I'll limit the backtest to a smaller portion to avoid overwhelming output.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "def backtest_unified" SSSv096.py
953:def backtest_unified(
I'll take a closer look at the function and dive into the details.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '953,1040p' SSSv096.py
def backtest_unified(
    df_ind: pd.DataFrame,
    strategy_type: str,
    params: Dict,
    buy_dates: Optional[List[pd.Timestamp]] = None,
    sell_dates: Optional[List[pd.Timestamp]] = None,
    discount: float = 0.30,
    trade_cooldown_bars: int = 3,
    bad_holding: bool = False,
    use_leverage: bool = False,
    lev_params: Optional[Dict] = None
) -> Dict:
    if not isinstance(df_ind, pd.DataFrame):
        logger.error(f"df_ind å¿…é ˆæ˜¯ä¸€å€‹ pandas.DataFrameï¼Œå»å‚³å…¥ {type(df_ind)}")
        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metrics':
[], 'equity_curve': pd.Series()}

    # è™•ç† ensemble ç­–ç•¥é¡å‹
    elif strategy_type == "ensemble":
        import traceback
        from contextlib import nullcontext

        status_ctx = st.status("åŸ·è¡Œ Ensemble å›æ¸¬ä¸­â€¦", state="running") if hasattr(st, "status") else nullcontext()
        try:
            with status_ctx:
                # å¾ Streamlit session_state å–å¾— ensemble åƒæ•¸
                method = st.session_state.get('ensemble_method', 'majority')
                floor  = st.session_state.get('ensemble_floor', 0.2)
                ema    = st.session_state.get('ensemble_ema', 3)
                delta  = st.session_state.get('ensemble_delta', 0.3)

                # ---- CostParamsï¼šUI ä»¥ bp è¨­å®šï¼Œå…§éƒ¨è½‰æˆ rate ----
                buy_fee_bp  = float(st.session_state.get('buy_fee_bp',  4.27))
                sell_fee_bp = float(st.session_state.get('sell_fee_bp', 4.27))
                sell_tax_bp = float(st.session_state.get('sell_tax_bp', 30.0))
                slip_bp     = float(st.session_state.get('slippage_bp', 0.0))

                # åœ¨ ensemble åˆ†æ”¯è£¡é¢å…ˆæ±ºå®š ticker_name
                ticker_name = (
                    params.get('ticker')
                    or (getattr(df_ind, 'name', None) if hasattr(df_ind, 'name') else None)
                    or 'UNKNOWN'
                )

                cost = CostParams(
                    buy_fee_bp   = buy_fee_bp,
                    sell_fee_bp  = sell_fee_bp,
                    sell_tax_bp  = sell_tax_bp,
                )
                params = EnsembleParams(
                    floor=floor,
                    ema_span=ema,
                    delta_cap=delta,
                    majority_k=6,
                    min_cooldown_days=1,
                    min_trade_dw=0.01
                )
                cfg = RunConfig(
                    ticker=ticker_name,
                    method=method,          # method æ”¾åœ¨ RunConfig
                    params=params,
                    cost=cost,
                    majority_k_pct=0.55    # é è¨­æ¯”ä¾‹é–€æª»
                )

                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

                trade_df_ui = normalize_trades_for_ui(trades)
                ledger_ui   = normalize_trades_for_ui(trade_ledger)

                result = {
                    'trades': trades.to_dict('records') if hasattr(trades,'to_dict') else [],
                    'trade_df': trade_df_ui,
                    'trade_ledger': ledger_ui,
                    'signals_df': pd.DataFrame(),
                    'metrics': stats if isinstance(stats, dict) else {},
                    'equity_curve': equity,
                    'daily_state': daily_state,  # åŠ å…¥ daily_state ä¾›åµéŒ¯ä½¿ç”¨
                    'weight_curve': w,  # åŠ å…¥ weight_curve ä¾›åµéŒ¯ä½¿ç”¨
                }

                # åŸ·è¡Œæ‘˜è¦è¨Šæ¯
                st.info(f"[Ensemble] {method} @ {ticker_name} | äº¤æ˜“ç­†æ•¸={len(trades)} | å›æ¸¬åˆ—æ•¸={len(equity)}")

                # åŸºæœ¬è¨Šæ¯
                st.success(f"Ensemble å®Œæˆï¼š{method_name}")
                st.caption(f"åƒæ•¸ï¼šmethod={method}, floor={floor}, ema={ema}, delta={delta}, "
                           f"è²·/è³£æ‰‹çºŒè²»bp={buy_fee_bp}/{sell_fee_bp}ï¼Œè­‰äº¤ç¨…bp={sell_tax_bp}ï¼Œæ»‘åƒ¹bp={slip_bp}")
This function is large, but the metrics issue might be elsewhere. I'll search for "apply_risk_valve".

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "é¢¨éšªé–¥é–€" SSSv096.py
I'll search the repo for "result['metrics'] = {}" to pin down the bug.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "result\['metrics'\]" -g "*.py"
SSSv096.py
2198:                                     result['signals_df'], result['metrics'])
2291:                                 result['signals_df'], result['metrics'])
2294:            display_strategy_summary(strategy, params, result['metrics'], smaa_source, result['trade_df'])

app_dash.py
1878:            metrics = result['metrics']

analysis/optuna_16.py
266:    metrics = result['metrics']

analysis/OSv3.py
1668:                result['metrics'] = {
1681:                if result['metrics']['max_drawdown'] != 0:
1682:                    result['metrics']['calmar_ratio'] = result['metrics']['annual_return'] / abs(result['metrics']['max_dra
wdown'])
1684:                    result['metrics']['calmar_ratio'] = 0
1981:                    if 'metrics' in result and result['metrics']:
1982:                        metrics = result['metrics']
2360:                if 'metrics' in result and result['metrics']:
2361:                    metrics = result['metrics']

analysis/exit_shift_test_v3.py
88:        **result['metrics']

analysis/OSv3_6270833.py
918:                result['metrics'] = {
940:                    result['metrics']['win_rate'] = win_rate
952:                    result['metrics']['win_rate'] = 0
956:                if result['metrics']['max_drawdown'] != 0:
957:                    result['metrics']['calmar_ratio'] = result['metrics']['annual_return'] / abs(result['metrics']['max_draw
down'])
959:                    result['metrics']['calmar_ratio'] = 0
1421:                if 'metrics' in result and result['metrics']:
1422:                    metrics = result['metrics']
Iâ€™ll check app_dash.py around lines 797-870 where the global risk valve might be applied.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '780,880p' app_dash.py

    # è¼‰å…¥æ•¸æ“š
    df_raw, df_factor = load_data(ticker, start_date, end_date, "Self")
    if df_raw.empty:
        logger.warning(f"ç„¡æ³•è¼‰å…¥ {ticker} çš„æ•¸æ“š")
        return {"error": f"ç„¡æ³•è¼‰å…¥ {ticker} çš„æ•¸æ“š"}

    ctx_trigger = ctx.triggered_id

    # åªåœ¨ auto-run ç‚º True æˆ–æŒ‰éˆ•è¢«é»æ“Šæ™‚é‹ç®—
    if not auto_run and ctx_trigger != 'run-btn':
        logger.debug(f"è·³éå›æ¸¬ï¼šauto_run={auto_run}, ctx_trigger={ctx_trigger}")
        return stored_data

    logger.info(f"é–‹å§‹åŸ·è¡Œå›æ¸¬ - ticker: {ticker}, ç­–ç•¥æ•¸: {len(strategy_names)}")
    results = {}

    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
    valve_triggered = False
    atr_ratio_current = None

    for strat in strategy_names:
        # åªä½¿ç”¨ param_presets ä¸­çš„åƒæ•¸
        strat_params = param_presets[strat].copy()
        strat_type = strat_params["strategy_type"]
        smaa_src = strat_params.get("smaa_source", "Self")

        # ç‚ºæ¯å€‹ç­–ç•¥è¼‰å…¥å°æ‡‰çš„æ•¸æ“š
        df_raw, df_factor = load_data(ticker, start_date, end_date if end_date else None, smaa_source=smaa_src)

        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        logger.info(f"[{strat}] é¢¨éšªé–¥é–€é–‹é—œç‹€æ…‹: global_apply={global_apply}, é¡å‹={type(global_apply)}")
        if global_apply:
            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")

            # è¨ˆç®— ATR æ¯”å€¼ï¼ˆä½¿ç”¨æœ€æ–°æ•¸æ“šï¼Œåƒ…ç”¨æ–¼æ—¥èªŒé¡¯ç¤ºï¼‰
            try:
                atr_20 = calculate_atr(df_raw, 20)
                atr_60 = calculate_atr(df_raw, 60)

                # ç¢ºä¿ ATR æ•¸æ“šæœ‰æ•ˆ
                if not atr_20.empty and not atr_60.empty:
                    atr_20_valid = atr_20.dropna()
                    atr_60_valid = atr_60.dropna()

                    # æª¢æŸ¥æ¨£æœ¬æ•¸é‡æ˜¯å¦è¶³å¤ 
                    min_samples_20, min_samples_60 = 30, 60  # è‡³å°‘éœ€è¦ 30 å’Œ 60 å€‹æ¨£æœ¬
                    if len(atr_20_valid) < min_samples_20 or len(atr_60_valid) < min_samples_60:
                        logger.warning(f"[{strat}] ATR æ¨£æœ¬ä¸è¶³ï¼Œ20æœŸ:{len(atr_20_valid)}/{min_samples_20}, 60æœŸ:{len(atr_60_val
id)}/{min_samples_60}")
                        continue

                    atr_20_latest = atr_20_valid.iloc[-1]
                    atr_60_latest = atr_60_valid.iloc[-1]

                    # æª¢æŸ¥ ATR å€¼æ˜¯å¦åˆç†
                    if atr_60_latest <= 0 or not np.isfinite(atr_60_latest):
                        logger.warning(f"[{strat}] ATR(60) å€¼ç•°å¸¸: {atr_60_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
                        continue

                    if atr_20_latest <= 0 or not np.isfinite(atr_20_latest):
                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
                        continue

                    atr_ratio_current = atr_20_latest / atr_60_latest
                    logger.info(f"[{strat}] æœ€æ–°ATRæ¯”å€¼: {atr_ratio_current:.4f} (20æœŸ:{atr_20_latest:.4f}, 60æœŸ:{atr_60_latest:
.4f})")
                else:
                    logger.warning(f"[{strat}] ATR è¨ˆç®—çµæœç‚ºç©º")

                # å¼·åˆ¶è§¸ç™¼æ™‚è¨­ç½®æ¨™è¨˜
                if force_trigger:
                    valve_triggered = True
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")

            except Exception as e:
                logger.warning(f"[{strat}] ATR è¨ˆç®—å¤±æ•—: {e}")
        else:
            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")

        if strat_type == 'ssma_turn':
            calc_keys = ['linlen', 'factor', 'smaalen', 'prom_factor', 'min_dist', 'buy_shift', 'exit_shift', 'vol_window', 'sig
nal_cooldown_days', 'quantile_win']
            ssma_params = {k: v for k, v in strat_params.items() if k in calc_keys}
            backtest_params = ssma_params.copy()
            backtest_params['stop_loss'] = strat_params.get('stop_loss', 0.0)

            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
            df_ind, buy_dates, sell_dates = compute_ssma_turn_combined(df_raw, df_factor, **ssma_params, smaa_source=smaa_src)
            if df_ind.empty:
                continue
            result = backtest_unified(df_ind, strat_type, backtest_params, buy_dates, sell_dates, discount=discount, trade_coold
own_bars=cooldown, bad_holding=bad_holding)

            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
            if global_apply:
                # åˆ¤æ–·æ˜¯å¦è¦è§¸ç™¼ï¼ˆèˆ‡ä½ çš„ ATR æª¢æŸ¥æˆ–å¼·åˆ¶è§¸ç™¼ä¸€è‡´ï¼‰
                valve_triggered_local = False
                ratio_local = None
                try:
                    atr_20 = calculate_atr(df_raw, 20)
                    atr_60 = calculate_atr(df_raw, 60)
                    if not atr_20.empty and not atr_60.empty:
                        a20 = atr_20.dropna().iloc[-1]
                        a60 = atr_60.dropna().iloc[-1]
root@478772c9b537:/workspace/SSS096# /bin/sed -n '880,980p' app_dash.py
                        a60 = atr_60.dropna().iloc[-1]
                        if a60 > 0:
                            ratio_local = float(a20 / a60)
                            valve_triggered_local = (ratio_local >= atr_ratio)  # å»ºè­°ä½¿ç”¨ >=
                except Exception:
                    pass

                if force_trigger:
                    valve_triggered_local = True
                    if ratio_local is None:
                        ratio_local = 1.5

                if valve_triggered_local:
                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
                    # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
                    open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
                    # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
                    w_series = None
                    try:
                        ds_std = df_from_pack(result.get('daily_state_std'))
                        if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
                            w_series = ds_std['w']
                    except Exception:
                        pass
                    if w_series is None:
                        ds = df_from_pack(result.get('daily_state'))
                        if ds is not None and not ds.empty and 'w' in ds.columns:
                            w_series = ds['w']

                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
                        )

                        rv = risk_valve_backtest(
                            open_px=open_px,
                            w=w_series,
                            cost=cost_params,
                            benchmark_df=df_raw,
                            mode="cap",
                            cap_level=float(risk_cap),
                        )

                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
                        result['daily_state']      = pack_df(rv["daily_state_valve"])
                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
                        result['weight_curve']     = pack_series(rv["weights_valve"])
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
                        result['valve'] = {
                            "applied": True,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio_local
                        }

                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
                    else:
                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
                else:
                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                    result['valve'] = {
                        "applied": False,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio_local if ratio_local is not None else "N/A"
                    }
        elif strat_type == 'ensemble':
            # ä½¿ç”¨æ–°çš„ ensemble_runner é¿å…å¾ªç’°ä¾è³´
            try:
                from runners.ensemble_runner import run_ensemble_backtest
                from SSS_EnsembleTab import EnsembleParams, CostParams, RunConfig

                # æŠŠ SSSv096 çš„å·¢ç‹€åƒæ•¸æ”¤å¹³
                flat_params = {}
                flat_params.update(strat_params.get('params', {}))
                flat_params.update(strat_params.get('trade_cost', {}))
                flat_params['method'] = strat_params.get('method', 'majority')
                flat_params['ticker'] = ticker

                # ä½¿ç”¨æ¯”ä¾‹é–€æª»é¿å… N è®Šå‹•æ™‚å¤±çœŸ
                if 'majority_k' in flat_params and flat_params.get('method') == 'majority':
                    flat_params['majority_k_pct'] = 0.55
                    flat_params.pop('majority_k', None)
                    logger.info(f"[Ensemble] ä½¿ç”¨æ¯”ä¾‹é–€æª» majority_k_pct={flat_params['majority_k_pct']}")

                # å‰µå»ºé…ç½®
                ensemble_params = EnsembleParams(
                    floor=flat_params.get("floor", 0.2),
                    ema_span=flat_params.get("ema_span", 3),
                    delta_cap=flat_params.get("delta_cap", 0.3),
                    majority_k=flat_params.get("majority_k", 6),
                    min_cooldown_days=flat_params.get("min_cooldown_days", 1),
                    min_trade_dw=flat_params.get("min_trade_dw", 0.01)
                )
I'm checking the end of backtest_unified for its return in non-ensemble cases.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1040,1160p' SSSv096.py

                # æ²’äº¤æ˜“æ™‚çš„è¨ºæ–·
                if (trades is None) or (len(trades) == 0):
                    st.warning(
                        f"Ensemble æ²’æœ‰ç”¢ç”Ÿäº¤æ˜“ï¼Œå¯èƒ½åŸå› ï¼š\n"
                        f"â€¢ å­ç­–ç•¥æŒå€‰å…¨ 0 æˆ–è¢«å¥åº·æª¢æŸ¥éæ¿¾\n"
                        f"â€¢ é–€æª»éé«˜ï¼ˆmajority_k_pct={cfg.majority_k_pct}ï¼‰æˆ– delta_cap / min_trade_dw éåš´\n"
                        f"â€¢ æˆæœ¬è¨­å®šä½¿å¾—å¾®å°è®Šå‹•è¢«å¿½ç•¥\n"
                        f"è«‹é™ä½é–€æª»ã€æ”¾å¯¬ delta_cap æˆ–æª¢æŸ¥å­ç­–ç•¥ trades æª”ã€‚"
                    )
                    # é¡å¤–æç¤ºï¼šé¡¯ç¤ºéƒ¨åˆ† daily_state / w ä¾›ä½¿ç”¨è€…è‡ªæŸ¥
                    if daily_state is not None and not daily_state.empty:
                        st.info("ä»¥ä¸‹ç‚ºæœ€è¿‘ 10 ç­†æ¬Šé‡è®ŠåŒ–ä¾›æª¢è¦–ï¼š")
                        st.dataframe(daily_state[['w']].tail(10) if 'w' in daily_state.columns else daily_state.tail(10))

                logger.info(f"[Ensemble] æˆåŠŸåŸ·è¡Œ {method_name} ç­–ç•¥ï¼Œç”¢ç”Ÿ {len(trades)} ç­†äº¤æ˜“")
                return result

        except Exception as e:
            logger.exception("[Ensemble] åŸ·è¡Œå¤±æ•—: %s", e)
            st.error(f"[Ensemble] åŸ·è¡Œå¤±æ•—ï¼š{e}")
            st.code(traceback.format_exc(), language="text")
            st.stop()
        finally:
            if hasattr(st, "status"):
                status_ctx.update(label="å®Œæˆ" if 'trades' in locals() else "çµæŸï¼ˆå¯èƒ½å¤±æ•—ï¼‰",
                                  state=("complete" if 'trades' in locals() else "error"))

    BUY_FEE_RATE = BASE_FEE_RATE * discount
    SELL_FEE_RATE = BASE_FEE_RATE * discount + TAX_RATE
    ROUND_TRIP_FEE = BUY_FEE_RATE + SELL_FEE_RATE

    if use_leverage:
        from leverage import LeverageEngine
        lev = LeverageEngine(**(lev_params or {}))
    else:
        lev = None

    required_cols = ['open', 'close'] if strategy_type == 'ssma_turn' else ['open', 'close', 'smaa', 'base', 'sd']
    if df_ind.empty or not all(col in df_ind.columns for col in required_cols):
        logger.warning(f"æŒ‡æ¨™æ•¸æ“šä¸å®Œæ•´ï¼Œç„¡æ³•åŸ·è¡Œå›æ¸¬(ç¼ºå°‘æ¬„ä½: {set(required_cols) - set(df_ind.columns)})")
        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metrics':
{'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_ratio': 0.0, 'num_trades': 0}, 'eq
uity_curve': pd.Series()}

    try:
        trade_cooldown_bars = int(trade_cooldown_bars)
        if trade_cooldown_bars < 0:
            raise ValueError("trade_cooldown_bars å¿…é ˆç‚ºéè² æ•´æ•¸")
        params['stop_loss'] = float(params.get('stop_loss', 0.0))
        if bad_holding and params['stop_loss'] <= 0:
            raise ValueError("ç•¶å•Ÿç”¨ bad_holding æ™‚ï¼Œstop_loss å¿…é ˆç‚ºæ­£æ•¸")
        if strategy_type == 'ssma_turn':
            params['exit_shift'] = int(params.get('exit_shift', 0))
            if params['exit_shift'] < 0:
                raise ValueError("exit_shift å¿…é ˆç‚ºéè² æ•´æ•¸")
        else:
            params['buy_mult'] = float(params.get('buy_mult', 0.5))
            params['sell_mult'] = float(params.get('sell_mult', 0.5))
            params['prom_factor'] = float(params.get('prom_factor', 0.5))
            params['min_dist'] = int(params.get('min_dist', 5))
            if params['buy_mult'] < 0 or params['sell_mult'] < 0:
                raise ValueError("buy_mult å’Œ sell_mult å¿…é ˆç‚ºéè² æ•¸")
            if params['min_dist'] < 1:
                raise ValueError("min_dist å¿…é ˆç‚ºæ­£æ•´æ•¸")
    except (ValueError, TypeError) as e:
        logger.error(f"åƒæ•¸é©—è­‰å¤±æ•—: {e}")
        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metrics':
[], 'equity_curve': pd.Series()}

    initial_cash = 100000
    cash = initial_cash
    total_shares = 0
    trades = []
    trade_records = []
    signals = []
    in_pos = False
    entry_price = 0.0
    entry_date = None
    accum_interest = 0.0
    last_trade_idx = -trade_cooldown_bars - 1
    buy_idx = 0
    sell_idx = 0

    # åˆå§‹åŒ– Equity Curve
    equity_curve = pd.Series(initial_cash, index=df_ind.index, dtype=float)
    cash_series = pd.Series(initial_cash, index=df_ind.index, dtype=float)
    shares_series = pd.Series(0, index=df_ind.index, dtype=float)

    signals_list = []
    if strategy_type == 'ssma_turn':
        buy_dates = sorted(buy_dates or [])
        sell_dates = sorted(sell_dates or [])
        for dt in buy_dates:
            signals_list.append(TradeSignal(ts=dt, side="BUY", reason="ssma_turn_valley"))
        for dt in sell_dates:
            signals_list.append(TradeSignal(ts=dt, side="SELL", reason="ssma_turn_peak"))
    else:
        for i in range(len(df_ind)):
            date = df_ind.index[i]
            if df_ind['smaa'].iloc[i] < df_ind['base'].iloc[i] + df_ind['sd'].iloc[i] * params['buy_mult']:
                signals_list.append(TradeSignal(ts=date, side="BUY", reason=f"{strategy_type}_buy"))
            elif df_ind['smaa'].iloc[i] > df_ind['base'].iloc[i] + df_ind['sd'].iloc[i] * params['sell_mult']:
                signals_list.append(TradeSignal(ts=date, side="SELL", reason=f"{strategy_type}_sell"))

    signals_list.sort(key=lambda x: x.ts)

    n = len(df_ind)
    scheduled_buy = np.zeros(n, dtype=bool)
    scheduled_sell = np.zeros(n, dtype=bool)
    scheduled_forced = np.zeros(n, dtype=bool)
    idx_by_date = {date: i for i, date in enumerate(df_ind.index)}

    for sig in signals_list:
        ts = pd.Timestamp(sig.ts).tz_localize(None) if sig.ts.tzinfo else sig.ts
        if ts in idx_by_date:
            i = idx_by_date[ts]
            if i + 1 < n:
                if sig.side == "BUY":
                    scheduled_buy[i + 1] = True
                elif sig.side in ["SELL", "STOP_LOSS", "FORCE_SELL"]:
                    scheduled_sell[i + 1] = True if sig.side == "SELL" else False
                    scheduled_forced[i + 1] = True if sig.side in ["STOP_LOSS", "FORCE_SELL"] else scheduled_forced[i + 1]
Iâ€™ll narrow down the backtest_unified function to focus on building or returning metrics.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics = {" SSSv096.py | /bin/head -n 20
rg: regex parse error:
    (?:metrics = {)
                  ^
error: repetition quantifier expects a valid decimal
root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics = \{" SSSv096.py | /bin/head -n 20
834:    metrics = {
2480:            performance_metrics = {}
2703:        detailed_metrics = {
I'll browse the code around line 834 to check the metrics dictionary definition.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '834,910p' SSSv096.py
    metrics = {
        'total_return': 0.0,
        'annual_return': 0.0,
        'max_drawdown': 0.0,  # é€™è£¡ä¹‹å¾Œæœƒç›´æ¥ç”¨ max_mmd è¦†è“‹
        'max_drawdown_duration': 0,
        'calmar_ratio': np.nan,
        'num_trades': 0,
        'win_rate': 0.0,
        'avg_win': np.nan,
        'avg_loss': np.nan,
        'payoff_ratio': np.nan,
        'sharpe_ratio': np.nan,
        'sortino_ratio': np.nan,
        'max_consecutive_wins': 0,# æ–°å¢
        'max_consecutive_losses': 0,# æ–°å¢
        'avg_holding_period': np.nan,# æ–°å¢
        'annualized_volatility': np.nan,# æ–°å¢
        'profit_factor': np.nan,# æ–°å¢
        # 'avg_mmd': np.nan, # ä¸å†éœ€è¦
        # 'max_mmd': np.nan, # ä¸å†éœ€è¦å–®ç¨æ¬„ä½
    }
    if not trades:
        return metrics

    trade_metrics = pd.DataFrame(trades, columns=['entry_date', 'return', 'exit_date']).set_index('exit_date')
    trade_metrics['equity'] = (1 + trade_metrics['return']).cumprod()
    roll_max = trade_metrics['equity'].cummax()
    daily_drawdown = trade_metrics['equity'] / roll_max - 1

    # åŸºæœ¬æŒ‡æ¨™
    metrics['total_return'] = trade_metrics['equity'].iloc[-1] - 1
    years = max((trade_metrics.index[-1] - trade_metrics.index[0]).days / 365.25, 1)
    metrics['annual_return'] = (1 + metrics['total_return']) ** (1 / years) - 1

    # æ—¢æœ‰æ—¥å…§å›æ’¤ï¼ˆnumpy é™£åˆ—ï¼‰
    dd_np = (daily_drawdown < 0).astype(bool)

    # è½‰æˆ pandas Seriesï¼ˆç´¢å¼•ç”¨å› trade_metrics.indexï¼‰ï¼Œæ‰èƒ½å®‰å…¨ groupby
    dd_series = pd.Series(dd_np, index=trade_metrics.index)

    # é€£çºŒå›æ’¤å€æ®µçš„æœ€é•·å¤©æ•¸ï¼ˆTrue é€£çºŒæ®µçš„é•·åº¦æœ€å¤§å€¼ï¼‰
    blocks = (~dd_series).cumsum()
    dd_dur = int((dd_series.groupby(blocks).cumcount() + 1).where(dd_series).max() or 0)
    metrics['max_drawdown_duration'] = dd_dur

    # å›å¡« max_drawdownï¼ˆå®‰å…¨åœ°ç”¨ float è½‰æ›ï¼‰
    metrics['max_drawdown'] = float(daily_drawdown.min())
    # metrics['calmar_ratio'] = metrics['annual_return'] / abs(metrics['max_drawdown']) if metrics['max_drawdown'] < 0 else np.n
an
    metrics['num_trades'] = len(trade_metrics)
    metrics['win_rate'] = (trade_metrics['return'] > 0).sum() / metrics['num_trades'] if metrics['num_trades'] > 0 else 0
    metrics['avg_win'] = trade_metrics[trade_metrics['return'] > 0]['return'].mean() if metrics['win_rate'] > 0 else np.nan
    metrics['avg_loss'] = trade_metrics[trade_metrics['return'] < 0]['return'].mean() if metrics['win_rate'] < 1 else np.nan
    metrics['payoff_ratio'] = abs(metrics['avg_win'] / metrics['avg_loss']) if metrics['avg_loss'] != 0 and not np.isnan(metrics
['avg_win']) else np.nan

    # æ–°å¢:è¨ˆç®—æ¯æ—¥å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡
    daily_dates = df_ind.index.intersection(pd.date_range(start=trade_metrics.index.min(), end=trade_metrics.index.max(), freq='
B'))
    daily_equity = pd.Series(index=daily_dates, dtype=float)
    for date, row in trade_metrics.iterrows():
        daily_equity.loc[date] = row['equity']
    daily_equity = daily_equity.ffill()
    daily_returns = daily_equity.pct_change().dropna()

    metrics['sharpe_ratio'] = (daily_returns.mean() * np.sqrt(252)) / daily_returns.std() if daily_returns.std() != 0 else np.na
n
    downside_returns = daily_returns[daily_returns < 0]
    downside_std = downside_returns.std() if len(downside_returns) > 0 else np.nan
    metrics['sortino_ratio'] = (daily_returns.mean() * np.sqrt(252)) / downside_std if downside_std != 0 else np.nan

    # æ–°å¢:è¨ˆç®—æœ€å¤§é€£çºŒç›ˆåˆ©å’Œæœ€å¤§é€£çºŒè™§æ
    trade_metrics['win_flag'] = trade_metrics['return'] > 0
    trade_metrics['grp'] = (trade_metrics['win_flag'] != trade_metrics['win_flag'].shift(1)).cumsum()
    consec = trade_metrics.groupby(['grp', 'win_flag']).size()
    metrics['max_consecutive_wins'] = consec[consec.index.get_level_values('win_flag') == True].max() if True in consec.index.ge
t_level_values('win_flag') else 0
    metrics['max_consecutive_losses'] = consec[consec.index.get_level_values('win_flag') == False].max() if False in consec.inde
x.get_level_values('win_flag') else 0

    # æ–°å¢:è¨ˆç®—å¹´åŒ–æ³¢å‹•ç‡
    metrics['annualized_volatility'] = daily_returns.std() * np.sqrt(252) if len(daily_returns) > 0 else np.nan
I'm looking for where backtest_unified returns the result, searching near the bottom.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "return {'trades'" SSSv096.py | /bin/head -n 20
rg: regex parse error:
    (?:return {'trades')
               ^
error: repetition quantifier expects a valid decimal
root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "return \{'trades'" SSSv096.py
967:        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metric
s': [], 'equity_curve': pd.Series()}
1081:        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metri
cs': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_ratio': 0.0, 'num_trades': 0}
, 'equity_curve': pd.Series()}
1105:        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metri
cs': [], 'equity_curve': pd.Series()}
The metrics aren't updating in the result after applying the risk valve. I'll check if they're returned correctly from backtest_unified.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "'metrics': metrics" SSSv096.py
1313:        'metrics': metrics,
I'll check out lines 1300-1320 to look at the final result dictionary with metrics.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1300,1325p' SSSv096.py
        'cash': cash_series,
        'shares': shares_series
    })
    # è¨ˆç®—æ¬Šé‡ï¼ˆæŠ•è³‡æ¯”ä¾‹ï¼‰
    daily_state['w'] = (daily_state['equity'] - daily_state['cash']) / daily_state['equity']
    daily_state['w'] = daily_state['w'].fillna(0).clip(0, 1)

    logger.info(f"{strategy_type} å›æ¸¬çµæœ: ç¸½å ±é…¬ç‡ = {metrics.get('total_return', 0):.2%}, äº¤æ˜“æ¬¡æ•¸={metrics.get('num_trades',
 0)}")
    return {
        'trades': trades,
        'trade_df': trade_df,
        'trades_df': trades_df,
        'signals_df': signals_df,
        'metrics': metrics,
        'equity_curve': equity_curve,
        'daily_state': daily_state,  # åŠ å…¥ daily_state
        'weight_curve': daily_state['w']  # åŠ å…¥ weight_curve
    }
def compute_backtest_for_periods(ticker: str,periods: List[Tuple[str, str]],strategy_type: str,params: Dict,
    smaa_source: str = "Self",trade_cooldown_bars: int = 3,discount: float = 0.30,
    bad_holding: bool = False,df_price: Optional[pd.DataFrame] = None,df_factor: Optional[pd.DataFrame] = None
    ) -> List[Dict]:
    """
    å°å¤šå€‹æ™‚æ®µé€²è¡Œå›æ¸¬ï¼Œç¢ºä¿è¿”å›æ¨™æº–åŒ–çµæœã€‚
    """
    results = []
Iâ€™ll search for where result['metrics'] = is set near the risk valve.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "result\['metrics'\]" app_dash.py
1878:            metrics = result['metrics']
I'll open lines 1870-1885 to check around line 1878.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1860,1885p' app_dash.py
                    trade_df = trade_df.copy()
                    trade_df.columns = [str(c).lower() for c in trade_df.columns]
                    if "trade_date" not in trade_df.columns and "date" in trade_df.columns:
                        trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
                    if "type" not in trade_df.columns and "action" in trade_df.columns:
                        trade_df["type"] = trade_df["action"].astype(str).str.lower()
                    if "price" not in trade_df.columns:
                        for c in ["open", "price_open", "exec_price", "px", "close"]:
                            if c in trade_df.columns:
                                trade_df["price"] = trade_df[c]
                                break

            if 'trade_date' in trade_df.columns:
                trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])

            # è¨ˆç®—è©³ç´°çµ±è¨ˆä¿¡æ¯
            detailed_stats = calculate_holding_periods(trade_df)

            metrics = result['metrics']
            comparison_data.append({
                'ç­–ç•¥': strategy,
                'ç¸½å›å ±ç‡': f"{metrics.get('total_return', 0):.2%}",
                'å¹´åŒ–å›å ±ç‡': f"{metrics.get('annual_return', 0):.2%}",
                'æœ€å¤§å›æ’¤': f"{metrics.get('max_drawdown', 0):.2%}",
                'å¡ç‘ªæ¯”ç‡': f"{metrics.get('calmar_ratio', 0):.2f}",
                'äº¤æ˜“æ¬¡æ•¸': metrics.get('num_trades', 0),
I think the bug may stem from how metrics isn't being set properly or preserved.

Looks like the issue could lie in how the risk valve is applied in run_workflow.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg "é¢¨éšªé–¥é–€" -n run_enhanced_ensemble.py
I need to verify if weâ€™re correctly merging, not replacing, metrics from the risk valve.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "metrics" app_dash.py | /bin/head -n 100
1096:                    if "metrics" in rv:
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1130:                    'metrics': backtest_result.stats,
1151:                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calma
r_ratio': 0.0, 'num_trades': 0},
1302:                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
1540:            metrics = result.get('metrics', {})
1545:            metrics['avg_holding_period'] = avg_holding
1568:            for k, v in metrics.items():
1729:                dbc.Row(metric_cards, style={"flex-wrap": "wrap"}, className='metrics-cards-row'),
1731:                dcc.Graph(figure=fig1, config={'displayModeBar': True}, className='main-metrics-graph'),
1878:            metrics = result['metrics']
1881:                'ç¸½å›å ±ç‡': f"{metrics.get('total_return', 0):.2%}",
1882:                'å¹´åŒ–å›å ±ç‡': f"{metrics.get('annual_return', 0):.2%}",
1883:                'æœ€å¤§å›æ’¤': f"{metrics.get('max_drawdown', 0):.2%}",
1884:                'å¡ç‘ªæ¯”ç‡': f"{metrics.get('calmar_ratio', 0):.2f}",
1885:                'äº¤æ˜“æ¬¡æ•¸': metrics.get('num_trades', 0),
1886:                'å‹ç‡': f"{metrics.get('win_rate', 0):.2%}",
1887:                'ç›ˆè™§æ¯”': f"{metrics.get('payoff_ratio', 0):.2f}",
2436:    m = out["metrics"]
2628:    def _group_metrics(mask):
2655:    acc_metrics = dis_metrics = None
2659:        acc_metrics = _group_metrics(mask_acc)
2660:        dis_metrics = _group_metrics(mask_dis)
2664:        group_row("åŠ ç¢¼æ®µ", acc_metrics),
2665:        group_row("æ¸›ç¢¼æ®µ", dis_metrics),
3447:                if not isinstance(out, dict) or "metrics" not in out:
3451:                m = out["metrics"]
3547:def calculate_pareto_metrics(equity_curve, trade_df):
3860:                    if out and "metrics" in out:
3861:                        m = out["metrics"]
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)
3868:                        if metrics:
3873:                                'max_drawdown': metrics['max_drawdown'],
3874:                                'pf': metrics['pf'],
3875:                                'right_tail_loss': metrics['right_tail_loss'],
3876:                                'risk_trigger_days': metrics['risk_trigger_days'],
I see metrics might be cleared around line 1302 during cleanup. I'll check further.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1290,1310p' app_dash.py
                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
The issue: after popping 'metrics', no recomputation happens, leaving it empty later. I'll fix by recalculating performance metrics after applying the risk valve.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "calculate_performance_metrics" -g "*.py"
Iâ€™ll search for calculate_metricsâ€”maybe that's the function I need. Letâ€™s see.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "calculate_.*metrics" -g "*.py"
SSSv096.py
40:    "compute_backtest_for_periods", "calculate_metrics",
824:def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series
= None) -> Dict:
1295:    metrics = calculate_metrics(trades, df_ind, equity_curve)

app_dash.py
3547:def calculate_pareto_metrics(equity_curve, trade_df):
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)

analysis/optuna_16.py
312:def calculate_adjusted_score(total_ret, n_trades, sharpe_ratio, max_drawdown, profit_factor, equity_curve, cpcv_oos_mean, cp
cv_oos_min, sharpe_var, pbo_score, overfitting_metrics=None):
375:def calculate_enhanced_overfitting_metrics(equity_curve, params, strategy_name):
780:    overfitting_metrics = calculate_enhanced_overfitting_metrics(equity_curve, params, strat)
809:        score = calculate_adjusted_score(total_return, n_trades, sharpe_ratio, max_drawdown, profit_factor, equity_curve, cp
cv_oos_mean, cpcv_oos_min, sharpe_var, pbo_score, overfitting_metrics)
876:            overfitting_metrics = calculate_enhanced_overfitting_metrics(equity_curve, params, args.strategy)
878:            score = calculate_adjusted_score(total_return, n_trades, sharpe_ratio, max_drawdown, profit_factor, equity_curve
, cpcv_oos_mean, cpcv_oos_min, sharpe_var, pbo_score, overfitting_metrics)

analysis/OSv3.py
491:def calculate_overfitting_metrics(train_returns, test_returns, strategy_name):
671:def calculate_risk_adjusted_metrics(equity_curve, strategy_name):
742:        additional_metrics = calculate_risk_adjusted_metrics(equity_curve, method_name)
1623:                risk_adjusted_metrics = calculate_risk_adjusted_metrics(equity_curve, name)

analysis/enhanced_trade_analysis.py
226:        risk_metrics = self._calculate_trade_metrics(risk_trades)
227:        normal_metrics = self._calculate_trade_metrics(normal_trades)
306:            phase_metrics = self._calculate_trade_metrics(phase_trades)
455:            optimized_metrics = self._calculate_trade_metrics(optimized_trades)
464:    def _calculate_trade_metrics(self, trades_df):

analysis/ROEAv4.py
25:from SSSv095a1 import calculate_metrics, load_data_wrapper
407:    all_metric_cols = list(calculate_metrics([], pd.DataFrame()).keys())

analysis/OSv3_6270833.py
315:def calculate_overfitting_metrics(train_returns, test_returns, strategy_name):
394:def calculate_risk_adjusted_metrics(equity_curve, strategy_name):
873:                risk_adjusted_metrics = calculate_risk_adjusted_metrics(equity_curve, name)

analysis/past/risk_enhanced_clustering_analysis_v2.py
51:def calculate_risk_metrics(equity_curve, window=252):
166:                    risk_metrics = calculate_risk_metrics(equity_curve)
Iâ€™ll check out the calculate_metrics function in SSSv096 starting at line 824.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '824,875p' SSSv096.py
def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series = No
ne) -> Dict:
    """
    è¨ˆç®—å›æ¸¬ç¸¾æ•ˆæŒ‡æ¨™.
    Args:
        trades: äº¤æ˜“è¨˜éŒ„,åŒ…å«æ—¥æœŸå’Œå ±é…¬ç‡.
        df_ind: æŒ‡æ¨™æ•¸æ“š DataFrame,åŒ…å«äº¤æ˜“æ—¥ç´¢å¼•.
        equity_curve: å…¨éƒ¨å›æ¸¬æœŸé–“çš„æ¯æ—¥è³‡ç”¢æ·¨å€¼ï¼ˆå¯é¸, è‹¥è¦è¨ˆç®—æŒæœ‰æœŸé–“MMDéœ€æä¾›ï¼‰
    Returns:
        Dict: åŒ…å«ç¸½å›å ±ç‡ã€å¹´åŒ–å›å ±ç‡ã€æœ€å¤§å›æ’¤ç­‰æŒ‡æ¨™.
    """
    metrics = {
        'total_return': 0.0,
        'annual_return': 0.0,
        'max_drawdown': 0.0,  # é€™è£¡ä¹‹å¾Œæœƒç›´æ¥ç”¨ max_mmd è¦†è“‹
        'max_drawdown_duration': 0,
        'calmar_ratio': np.nan,
        'num_trades': 0,
        'win_rate': 0.0,
        'avg_win': np.nan,
        'avg_loss': np.nan,
        'payoff_ratio': np.nan,
        'sharpe_ratio': np.nan,
        'sortino_ratio': np.nan,
        'max_consecutive_wins': 0,# æ–°å¢
        'max_consecutive_losses': 0,# æ–°å¢
        'avg_holding_period': np.nan,# æ–°å¢
        'annualized_volatility': np.nan,# æ–°å¢
        'profit_factor': np.nan,# æ–°å¢
        # 'avg_mmd': np.nan, # ä¸å†éœ€è¦
        # 'max_mmd': np.nan, # ä¸å†éœ€è¦å–®ç¨æ¬„ä½
    }
    if not trades:
        return metrics

    trade_metrics = pd.DataFrame(trades, columns=['entry_date', 'return', 'exit_date']).set_index('exit_date')
    trade_metrics['equity'] = (1 + trade_metrics['return']).cumprod()
    roll_max = trade_metrics['equity'].cummax()
    daily_drawdown = trade_metrics['equity'] / roll_max - 1

    # åŸºæœ¬æŒ‡æ¨™
    metrics['total_return'] = trade_metrics['equity'].iloc[-1] - 1
    years = max((trade_metrics.index[-1] - trade_metrics.index[0]).days / 365.25, 1)
    metrics['annual_return'] = (1 + metrics['total_return']) ** (1 / years) - 1

    # æ—¢æœ‰æ—¥å…§å›æ’¤ï¼ˆnumpy é™£åˆ—ï¼‰
    dd_np = (daily_drawdown < 0).astype(bool)

    # è½‰æˆ pandas Seriesï¼ˆç´¢å¼•ç”¨å› trade_metrics.indexï¼‰ï¼Œæ‰èƒ½å®‰å…¨ groupby
    dd_series = pd.Series(dd_np, index=trade_metrics.index)

    # é€£çºŒå›æ’¤å€æ®µçš„æœ€é•·å¤©æ•¸ï¼ˆTrue é€£çºŒæ®µçš„é•·åº¦æœ€å¤§å€¼ï¼‰
    blocks = (~dd_series).cumsum()
I'm reviewing risk_valve_backtest to understand trade_ledger_valve and its columns.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1359,1395p' SSS_EnsembleTab.py
def risk_valve_backtest(open_px: pd.Series,
                        w: pd.Series,
                        cost,
                        benchmark_df: pd.DataFrame,
                        mode: str = "cap",
                        cap_level: float = 0.5,
                        slope20_thresh: float = 0.0,
                        slope60_thresh: float = 0.0,
                        atr_win: int = 20,
                        atr_ref_win: int = 60,
                        atr_ratio_mult: float = 1.3) -> dict:
    """å›å‚³ï¼šåŸå§‹èˆ‡é–¥é–€ç‰ˆæœ¬çš„ç¸¾æ•ˆã€å³å°¾å‰Šæ¸›åº¦ã€ä»¥åŠå…©ç‰ˆ equity/ledger"""
    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
                                     atr_win, atr_ref_win, atr_ratio_mult)
    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)

    # åŸå§‹ç‰ˆæœ¬
    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
    # é–¥é–€ç‰ˆæœ¬
    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)

    # äº¤æ˜“çµ±è¨ˆï¼ˆåªç”¨è³£å‡ºåˆ—ï¼›ç¬¦åˆ re.txt å°å¯¦ç¾æç›Šçš„å£å¾‘ï¼‰
    r1 = _sell_returns_pct_from_ledger(ledger1)
    r2 = _sell_returns_pct_from_ledger(ledger2)
    pf1 = (r1[r1>0].sum() / abs(r1[r1<0].sum())) if (r1[r1<0].sum()!=0) else np.inf
    pf2 = (r2[r2>0].sum() / abs(r2[r2<0].sum())) if (r2[r2<0].sum()!=0) else np.inf
    mdd1 = _mdd_from_daily_equity(daily_state1["equity"])
    mdd2 = _mdd_from_daily_equity(daily_state2["equity"])

    # å³å°¾å‰Šæ¸›ï¼šæ¯”è¼ƒå…©ç‰ˆçš„ã€Œ>P90 çš„æ­£å ±é…¬ç¸½å’Œã€
    def right_tail_sum(x: pd.Series):
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)
I need to check the build_portfolio_ledger function to confirm the ledger's structure.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "build_portfolio_ledger" -g "*.py"
SSS_EnsembleTab.py
363:def build_portfolio_ledger(open_px: pd.Series, w: pd.Series, cost: CostParams,
850:    daily_state, trade_ledger = build_portfolio_ledger(
1376:    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
1378:    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)
I'll open the build_portfolio_ledger function definition around line 363. Let's check it out!

root@478772c9b537:/workspace/SSS096# /bin/sed -n '363,470p' SSS_EnsembleTab.py
def build_portfolio_ledger(open_px: pd.Series, w: pd.Series, cost: CostParams,
                          initial_capital: float = 1_000_000.0, lot_size: int | None = None) -> Tuple[pd.DataFrame, pd.DataFrame
]:
    """
    ä¾ç…§æ¯æ—¥ open åƒ¹èˆ‡ç›®æ¨™æ¬Šé‡ w_tï¼ˆå« floorã€delta_cap ç­‰é™åˆ¶å¾Œçš„æœ€çµ‚ w_tï¼‰ï¼Œ
    ç”¢å‡ºå…©å€‹ DataFrameï¼š
      1) daily_state: æ¯æ—¥ç¾é‡‘/æŒå€‰/ç¸½è³‡ç”¢/æ¬Šé‡
      2) trade_ledger: åªæœ‰æ¬Šé‡è®Šå‹•æ—¥çš„äº¤æ˜“æ˜ç´°ï¼ˆè²·è³£é‡‘é¡ã€è²»ç”¨ã€ç¨…ã€äº¤æ˜“å¾Œè³‡ç”¢ï¼‰
    """
    if cost is None:
        cost = CostParams()

    # è¨ˆç®—æ—¥å ±é…¬ç‡
    r = open_px.shift(-1) / open_px - 1
    r = r.dropna()

    # æ¬Šç›Šæ›²ç·š
    E = pd.Series(index=r.index, dtype=float)
    E.iloc[0] = initial_capital

    # ç¾é‡‘æ›²ç·š
    cash = pd.Series(index=r.index, dtype=float)
    cash.iloc[0] = initial_capital

    # æŒå€‰åƒ¹å€¼æ›²ç·š
    position_value = pd.Series(index=r.index, dtype=float)
    position_value.iloc[0] = 0.0

    # äº¤æ˜“è¨˜éŒ„
    trades = []

    # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°è¨˜éŒ„ï¼‰
    trade_ledger = []

    for i in range(1, len(r)):
        prev_w = w.iloc[i-1] if i-1 < len(w) else 0
        curr_w = w.iloc[i] if i < len(w) else 0

        # æ¬Šé‡è®ŠåŒ–
        dw = curr_w - prev_w

        if abs(dw) > 0.001:  # æœ‰é¡¯è‘—è®ŠåŒ–
            # 1) ä»¥ã€Œç¸½æ¬Šç›Šã€è¨ˆåŸ·è¡Œåç›®é‡‘é¡
            exec_notional = abs(dw) * E.iloc[i-1]

            if dw > 0:  # è²·å…¥
                c = exec_notional * cost.buy_rate
                tax = 0.0
                total_cost = c

                # è¨ˆç®—é¡¯ç¤ºç”¨è‚¡æ•¸ï¼ˆåƒ…ä¾› UI/å ±è¡¨ï¼Œä¸å½±éŸ¿é‚è¼¯ï¼‰
                px = float(open_px.iloc[i])
                shares_before = (prev_w * E.iloc[i-1]) / px if px > 0 else 0.0
                delta_units   =  exec_notional / px if px > 0 else 0.0
                shares_after  = shares_before + delta_units

                # äº¤æ˜“æ˜ç´°ï¼ˆçµ¦åœ–è¡¨/è¡¨æ ¼ï¼‰â€”è£œä¸Šæ¬Šé‡æ¬„ä½ï¼ˆè²·å…¥ä¹Ÿæœ‰ï¼‰
                trades.append({
                    'trade_date': r.index[i],
                    'type': 'buy',
                    'price_open': float(open_px.iloc[i]),
                    'price': float(open_px.iloc[i]),
                    'weight_change': float(dw),          # signed dw
                    'w_before': float(prev_w),
                    'w_after': float(curr_w),
                    'shares_before': float(shares_before),
                    'shares_after': float(shares_after),
                    'exec_notional': float(exec_notional),
                    'equity_after': float(E.iloc[i-1]),  # äº¤æ˜“ç™¼ç”Ÿåœ¨ i é–‹ç›¤ï¼Œäº¤æ˜“å¾Œçš„ç¸½è³‡ç”¢(åç›®)â‰ˆå‰ä¸€æ—¥ E æ‰£è²»ç”¨
                    'cash_after': np.nan                 # çœŸæ­£çš„æ—¥çµ‚ç¾é‡‘åœ¨ daily_state å…§ï¼Œé€™è£¡åƒ…ä¾› UI ç™¾åˆ†æ¯”æ¨å°ï¼ˆå¯é¸ï¼‰
                })

                # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°ï¼‰
                trade_ledger.append({
                    'date': r.index[i],
                    'type': 'buy',
                    'open': open_px.iloc[i],
                    'delta_units': delta_units,
                    'exec_notional': exec_notional,
                    'fee_buy': c,
                    'fee_sell': 0.0,
                    'tax': tax,  # è³£å‡ºæ‰æœƒæœ‰è­‰äº¤ç¨…ï¼Œé€™è£¡ç‚º 0
                    'shares_before': shares_before,
                    'shares_after': shares_after,
                    'w_before': prev_w,
                    'w_after': curr_w,
                    'cash_after': cash.iloc[i-1] - exec_notional - c,
                    'equity_open_after_trade': E.iloc[i-1] - total_cost,
                    'equity_after': E.iloc[i-1] * (1 + r.iloc[i] * curr_w) - total_cost
                })
            else:  # è³£å‡º
                c = exec_notional * cost.sell_rate
                tax = exec_notional * cost.sell_tax_rate
                total_cost = c + tax

                # è¨ˆç®—é¡¯ç¤ºç”¨è‚¡æ•¸ï¼ˆåƒ…ä¾› UI/å ±è¡¨ï¼Œä¸å½±éŸ¿é‚è¼¯ï¼‰
                px = float(open_px.iloc[i])
                shares_before = (prev_w * E.iloc[i-1]) / px if px > 0 else 0.0
                delta_units   = -exec_notional / px if px > 0 else 0.0
                shares_after  = shares_before + delta_units

                # äº¤æ˜“æ˜ç´°ï¼ˆçµ¦åœ–è¡¨/è¡¨æ ¼ï¼‰â€”è£œä¸Šæ¬Šé‡æ¬„ä½
                trades.append({
                    'trade_date': r.index[i],
                    'type': 'sell',
                    'price_open': float(open_px.iloc[i]),
                    'price': float(open_px.iloc[i]),
                    'weight_change': float(dw),          # è² æ•¸
                    'w_before': float(prev_w),
Iâ€™m considering using calculate_pareto_metrics for basic metrics, should check its definition first.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '3547,3585p' app_dash.py
def calculate_pareto_metrics(equity_curve, trade_df):
    """è¨ˆç®— Pareto Map æ‰€éœ€çš„æŒ‡æ¨™"""
    try:
        # åˆå§‹åŒ–æŒ‡æ¨™
        max_drawdown = 0.0
        pf = 1.0
        right_tail_loss = 0.0  # é è¨­ï¼šç„¡èª¿æ•´
        risk_trigger_days = 50

        # è™•ç†æ¬Šç›Šæ›²ç·šæ•¸æ“š
        if equity_curve is not None and not equity_curve.empty:
            # ç¢ºä¿æ¬Šç›Šæ›²ç·šæ˜¯ Series
            if isinstance(equity_curve, pd.DataFrame):
                if len(equity_curve.columns) == 1:
                    equity_curve = equity_curve.iloc[:, 0]
                else:
                    # å¦‚æœæœ‰å¤šåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
                    equity_curve = equity_curve.iloc[:, 0]

            # è¨ˆç®—æ—¥å ±é…¬ç‡
            daily_returns = equity_curve.pct_change().dropna()

            if len(daily_returns) > 0:
                # 1. æœ€å¤§å›æ’¤ï¼ˆæ„ˆå·¦æ„ˆå¥½ï¼‰
                peak = equity_curve.expanding().max()
                drawdown = (equity_curve - peak) / peak
                max_drawdown = abs(drawdown.min()) if not drawdown.empty else 0.0

                # 2. PFï¼ˆç²åˆ©å› å­ï¼Œæ„ˆä¸Šæ„ˆå¥½ï¼‰
                if trade_df is not None and not trade_df.empty and 'return' in trade_df.columns:
                    profits = trade_df[trade_df['return'] > 0]['return'].sum()
                    losses = abs(trade_df[trade_df['return'] < 0]['return'].sum())
                    pf = profits / losses if losses > 0 else (profits if profits > 0 else 1.0)
                else:
                    # å¦‚æœæ²’æœ‰äº¤æ˜“æ•¸æ“šï¼Œç”¨å¹´åŒ–å ±é…¬ç‡ä»£æ›¿
                    total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0] - 1)
                    annual_return = total_return * (252 / len(daily_returns))
                    pf = 1 + annual_return  # è½‰æ›ç‚º PF æ ¼å¼
I'll recompute metrics by transforming the trade_ledger into a list of trades. Let's search for related functions.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "trade_ledger" SSSv096.py | /bin.head -n 40
bash: /bin.head: No such file or directory
I'll try again with /bin/head using "rg ... | /bin/head".

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "trade_ledger" SSSv096.py | /bin/head -n 50
1017:                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)
1020:                ledger_ui   = normalize_trades_for_ui(trade_ledger)
1025:                    'trade_ledger': ledger_ui,
2349:                # å„ªå…ˆä½¿ç”¨ trade_ledgerï¼ˆEnsemble çš„å®Œæ•´äº¤æ˜“è¨˜éŒ„ï¼‰ï¼Œå…¶æ¬¡ç”¨ trade_df
2351:                if strategy_type == 'ensemble' and 'trade_ledger' in result and not result['trade_ledger'].empty:
2352:                    display_df = result['trade_ledger']
Looks like build_trade_snapshots_from_ledger at line 1288 might help. Checking it again.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1240,1288p' SSS_EnsembleTab.py

def compute_risk_valve_signals(benchmark_df: pd.DataFrame,
                               slope20_thresh: float = 0.0,
                               slope60_thresh: float = 0.0,
                               atr_win: int = 20,
                               atr_ref_win: int = 60,
                               atr_ratio_mult: float = 1.3) -> pd.DataFrame:
    """è¼¸å…¥ï¼šåŸºæº–ï¼ˆæ—¥é »ï¼‰ï¼Œæ¬„è‡³å°‘æœ‰ close/æ”¶ç›¤åƒ¹ï¼›è‹¥ç„¡é«˜ä½åƒ¹è‡ªå‹•å›é€€ã€‚
       è¼¸å‡ºï¼šå« slope_20d/60dã€atrã€atr_ratioã€risk_trigger(bool) çš„ DataFrame"""
    b = benchmark_df.copy()
    # æ¬„ä½å°é½Š
    c_close = _get_col(b, "æ”¶ç›¤åƒ¹", "close")
    c_high  = _get_col(b, "æœ€é«˜åƒ¹", "high")
    c_low   = _get_col(b, "æœ€ä½åƒ¹", "low")
    if c_close is None:
        raise ValueError("benchmark_df éœ€åŒ…å« æ”¶ç›¤åƒ¹/close")

    b = b.sort_index()
    close = pd.to_numeric(b[c_close], errors="coerce")

    # æ–œç‡ï¼šç”¨ã€Œt/t-n - 1ã€åšç°¡æ½”æ–œç‡ï¼ˆç¬¦åˆä½ è¦çš„æ–¹å‘åˆ¤æ–·ï¼‰
    b["slope_20d"] = close.pct_change(20)
    b["slope_60d"] = close.pct_change(60)

    # ATRï¼šè‹¥ç„¡é«˜ä½åƒ¹ï¼Œç”¨ |Î”close| è¿‘ä¼¼ TR
    prev_close = close.shift(1)
    if c_high and c_low:
        high = pd.to_numeric(b[c_high], errors="coerce")
        low  = pd.to_numeric(b[c_low], errors="coerce")
        tr = pd.concat([
            (high - low),
            (high - prev_close).abs(),
            (low  - prev_close).abs()
        ], axis=1).max(axis=1)
    else:
        tr = close.diff().abs()

    b["atr"] = tr.rolling(atr_win, min_periods=atr_win).mean()
    b["atr_ratio"] = b["atr"] / b["atr"].rolling(atr_ref_win, min_periods=atr_ref_win).mean()

    # é¢¨éšªè§¸ç™¼ï¼šå…©æ–œç‡çš† < é–€æª»ï¼Œä¸” ATR æŠ¬é ­è¶…éå€ç‡
    b["risk_trigger"] = (b["slope_20d"] < slope20_thresh) & \
                        (b["slope_60d"] < slope60_thresh) & \
                        (b["atr_ratio"] > atr_ratio_mult)
    return b[["slope_20d", "slope_60d", "atr", "atr_ratio", "risk_trigger"]]


# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
def apply_valve_to_weights(w: pd.Series,
I can recompute metrics with build_trade_snapshots_from_ledger, convert ledger_valve to DataFrame, and derive trades. I'll also check _sell_returns_pct_from_ledger.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "_sell_returns_pct_from_ledger" -g "SSS_EnsembleTab.py"
SSS_EnsembleTab.py
1338:def _sell_returns_pct_from_ledger(trade_ledger: pd.DataFrame) -> pd.Series:
1381:    r1 = _sell_returns_pct_from_ledger(ledger1)
1382:    r2 = _sell_returns_pct_from_ledger(ledger2)
1469:    sell_ret = _sell_returns_pct_from_ledger(trade_ledger)
1476:        sells_in_phase = _sell_returns_pct_from_ledger(tl_idxed.loc[mask_t])
I'll check out the definition at line 1338.

def _sell_returns_pct_from_ledger(trade_ledger: pd.DataFrame) -> pd.Series:
    tl = trade_ledger.copy()
    if "type" in tl.columns: tl.rename(columns={"type":"äº¤æ˜“é¡å‹"}, inplace=True)
    if "equity_after" in tl.columns: tl.rename(columns={"equity_after":"äº¤æ˜“å¾Œæ¬Šç›Š"}, inplace=True)
    if "equity_open_after_trade" in tl.columns: tl.rename(columns={"equity_open_after_trade":"äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š"}, inplace=True)
    tl = tl.sort_values("date" if "date" in tl.columns else "äº¤æ˜“æ—¥æœŸ")
    # ç”¨ã€Œäº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š â†’ äº¤æ˜“æ—¥çµ‚æ¬Šç›Šã€çš„è®ŠåŒ–ç•¶è©²ç­†äº¤æ˜“çš„æ—¥æç›Šï¼ˆå«æˆæœ¬ï¼‰
    if "äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š" in tl.columns and "äº¤æ˜“å¾Œæ¬Šç›Š" in tl.columns:
        delta = (tl["äº¤æ˜“å¾Œæ¬Šç›Š"] - tl["äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š"]) / tl["äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š"]
        tl["ç›ˆè™§%"] = delta * 100.0
    elif "ç›ˆè™§%" not in tl.columns:
        tl["ç›ˆè™§%"] = 0.0
    mask_sell = tl["äº¤æ˜“é¡å‹"].astype(str).str.lower().eq("sell")
    return pd.to_numeric(tl.loc[mask_sell, "ç›ˆè™§%"], errors="coerce").dropna()

def _mdd_from_daily_equity(equity: pd.Series) -> float:
    eq = pd.to_numeric(equity, errors="coerce").dropna()
    if eq.empty: return 0.0
    dd = eq / eq.cummax() - 1.0
    return float(dd.min())
Since calculate_metrics requires entry and exit dates, I'll create a workaround by using the sell date. I'll leverage the equity series for key metrics like total return, max drawdown, etc., while avoiding holding periods. I'll calculate other metrics like win rate and average wins using equity and returns, then expand with additional metrics like calmar ratio and holding periods.

To fix metrics vanishing after applying the risk valve, Iâ€™ll avoid removing them, and recompute post-valve metrics. I'll gather trade details from the existing ledger and snapshots for accurate computation.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1320,1338p' SSS_EnsembleTab.py
    if "cash_after" in df.columns: df.rename(columns={"cash_after": "äº¤æ˜“å¾Œç¾é‡‘"}, inplace=True)
    if "open" in df.columns: df.rename(columns={"open": "é–‹ç›¤åƒ¹"}, inplace=True)
    if "w_before" in df.columns: df.rename(columns={"w_before": "äº¤æ˜“å‰æ¬Šé‡"}, inplace=True)
    if "w_after" in df.columns: df.rename(columns={"w_after": "äº¤æ˜“å¾Œæ¬Šé‡"}, inplace=True)

    df["äº¤æ˜“æ—¥æœŸ"] = pd.to_datetime(df["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
    df = df.dropna(subset=["äº¤æ˜“æ—¥æœŸ"]).sort_values("äº¤æ˜“æ—¥æœŸ").reset_index(drop=True)

    df["ç¸½è³‡ç”¢"] = pd.to_numeric(df.get("äº¤æ˜“å¾Œæ¬Šç›Š", 0), errors="coerce").fillna(0) + \
                   pd.to_numeric(df.get("äº¤æ˜“å¾Œç¾é‡‘", 0), errors="coerce").fillna(0)
    df["å‰æ¬¡ç¸½è³‡ç”¢"] = df["ç¸½è³‡ç”¢"].shift(1)
    df["æ¯ç­†ç›ˆè™§"] = df["ç¸½è³‡ç”¢"] - df["å‰æ¬¡ç¸½è³‡ç”¢"]
    df["æ¯ç­†ç›ˆè™§%"] = (df["æ¯ç­†ç›ˆè™§"] / df["å‰æ¬¡ç¸½è³‡ç”¢"]).replace([np.inf, -np.inf], np.nan) * 100
    df.loc[df.index.min(), ["æ¯ç­†ç›ˆè™§", "æ¯ç­†ç›ˆè™§%"]] = 0.0
    return df


# === PATCH: åªç”¨è³£å‡ºåˆ—è¨ˆäº¤æ˜“çµ±è¨ˆ & MDD ä»¥æ—¥è³‡ç”¢æ›²ç·šç®— ===
def _sell_returns_pct_from_ledger(trade_ledger: pd.DataFrame) -> pd.Series:
# åŸºæº–ï¼ˆæœ‰é«˜ä½åƒ¹å°±å¸¶ä¸Šï¼‰
                        bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
                        if 'close' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
                        elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
                        if 'high' in df_raw.columns and 'low' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
                        elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")

                        result_cap = risk_valve_backtest(
                            open_px=open_px,
                            w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result
I see the issue: popping trade_df causes missing values for later calculations like avg_holding.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '1530,1565p' app_dash.py
                # ç‚ºäº† 100% ä¿è­‰ weight_change å‡ºç¾ï¼Œå…ˆç¢ºä¿æ¬Šé‡æ¬„ä½
                ledger_src = _ensure_weight_columns(ledger_src)
                # ä½¿ç”¨æ–°çš„çµ±ä¸€æ ¼å¼åŒ–å‡½å¼
                ledger_display = format_trade_like_df_for_display(ledger_src)
                ledger_columns = [{"name": i, "id": i} for i in ledger_display.columns]
                ledger_data = ledger_display.to_dict('records')
            else:
                ledger_columns = []
                ledger_data = []

            metrics = result.get('metrics', {})
            tooltip = f"{strategy} ç­–ç•¥èª¬æ˜"
            param_display = {k: v for k, v in param_presets[strategy].items() if k != "strategy_type"}
            param_str = ", ".join(f"{k}: {v}" for k, v in param_display.items())
            avg_holding = calculate_holding_periods(trade_df)
            metrics['avg_holding_period'] = avg_holding

            label_map = {
                "total_return": "ç¸½å›å ±ç‡",
                "annual_return": "å¹´åŒ–å›å ±ç‡",
                "win_rate": "å‹ç‡",
                "max_drawdown": "æœ€å¤§å›æ’¤",
                "max_drawdown_duration": "å›æ’¤æŒçºŒ",
                "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
                "sharpe_ratio": "å¤æ™®æ¯”ç‡",
                "sortino_ratio": "ç´¢æè«¾æ¯”ç‡",
                "payoff_ratio": "ç›ˆè™§æ¯”",
                "profit_factor": "ç›ˆè™§å› å­",
                "num_trades": "äº¤æ˜“æ¬¡æ•¸",
                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
                "annualized_volatility": "å¹´åŒ–æ³¢å‹•ç‡",
                "max_consecutive_wins": "æœ€å¤§é€£çºŒç›ˆåˆ©",
                "max_consecutive_losses": "æœ€å¤§é€£çºŒè™§æ",
                "avg_win": "å¹³å‡ç›ˆåˆ©",
                "avg_loss": "å¹³å‡è™§æ",
            }
After popping trade_df, I need to reconstruct it from trade_ledger_valve to ensure metrics are calculated correctly. I'll compute it using build_trade_snapshots_from_ledger and store it back in result['trade_df']. Then, I can compute the remaining metrics.

I need to confirm if functions from SSS_EnsembleTab are already imported at the top of app_dash.

root@478772c9b537:/workspace/SSS096# /bin/head -n 40 app_dash.py
import dash
from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.graph_objects as go
import json
import io
from dash.dependencies import ALL
import shutil
import os
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from analysis import config as cfg
import yfinance as yf
import logging
import numpy as np
from urllib.parse import quote as urlparse

# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
from analysis.logging_config import init_logging
import os

# å¼·åˆ¶å•Ÿç”¨æª”æ¡ˆæ—¥èªŒï¼Œä¸ä¾è³´ç’°å¢ƒè®Šæ•¸
os.environ["SSS_CREATE_LOGS"] = "1"

# ç›´æ¥å‘¼å« init_loggingï¼Œå®ƒæœƒä½¿ç”¨æ­£ç¢ºçš„æ—¥èªŒç›®éŒ„
init_logging(enable_file=True)
logger = logging.getLogger("SSS.App")

# è¨­å®šç‚º DEBUG ç´šåˆ¥ï¼Œè©³ç´°èª¿è©¦è³‡è¨Šæœƒå¯«å…¥æ—¥èªŒæª”æ¡ˆ
logger.setLevel(logging.DEBUG)
logger.info("å·²å•Ÿç”¨è©³ç´°èª¿è©¦æ¨¡å¼ - èª¿è©¦è³‡è¨Šå°‡å¯«å…¥æ—¥èªŒæª”æ¡ˆ")
logger.info(f"æ—¥èªŒç›®éŒ„: {os.path.abspath('analysis/log')}")

# ATR è¨ˆç®—å‡½æ•¸
def calculate_atr(df, window):
    """è¨ˆç®— ATR (Average True Range)"""
    try:
        # æ”¯æ´å¤šç¨®æ¬„ä½åç¨±æ ¼å¼
I need to import functions like _sell_returns_pct_from_ledger and calculate_metrics where needed.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "calculate_holding_periods" app_dash.py
211:    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_
periods
1544:            avg_holding = calculate_holding_periods(trade_df)
1876:            detailed_stats = calculate_holding_periods(trade_df)
I see the calculate_holding_periods import on line 211. I'll check lines 200-220.

root@478772c9b537:/workspace/SSS096# /bin.sed -n '200,220p' app_dash.py
bash: /bin.sed: No such file or directory
root@478772c9b537:/workspace/SSS096# /bin/sed -n '200,220p' app_dash.py

    if isinstance(data, (list, dict)):
        try:
            return pd.Series(data)
        except Exception:
            return pd.Series(dtype=float)

    return pd.Series(dtype=float)

from SSSv096 import (
    param_presets, load_data, compute_single, compute_dual, compute_RMA,
    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_peri
ods
)

# å½ˆæ€§åŒ¯å…¥ pack_df/pack_series å‡½æ•¸
try:
    from sss_core.schemas import pack_df, pack_series
except Exception:
    from schemas import pack_df, pack_series

# åŒ¯å…¥æ¬Šé‡æ¬„ä½ç¢ºä¿å‡½å¼
I need to import calculate_metrics and extend the import to include additional functions.

Iâ€™ll place computation of trade_df and metrics after popping, ensuring new metrics are preserved.

Iâ€™ll add Traditional Chinese comments with timestamps and update the import for new functions.

I'll attempt running the PowerShell script if available, noting environment limitations.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '200,220p' app_dash.py

    if isinstance(data, (list, dict)):
        try:
            return pd.Series(data)
        except Exception:
            return pd.Series(dtype=float)

    return pd.Series(dtype=float)

from SSSv096 import (
    param_presets, load_data, compute_single, compute_dual, compute_RMA,
    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_peri
ods
)

# å½ˆæ€§åŒ¯å…¥ pack_df/pack_series å‡½æ•¸
try:
    from sss_core.schemas import pack_df, pack_series
except Exception:
    from schemas import pack_df, pack_series

# åŒ¯å…¥æ¬Šé‡æ¬„ä½ç¢ºä¿å‡½å¼
I'll add calculate_metrics to the import and break the long lines. Next, finding the risk_valve_backtest import.

root@478772c9b537:/workspace/SSS096# /usr/bin/rg -n "risk_valve_backtest" -n app_dash.py
893:                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
918:                        rv = risk_valve_backtest(
1075:                    from SSS_EnsembleTab import risk_valve_backtest
1080:                    rv = risk_valve_backtest(
1238:                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
1240:                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
1264:                        result_cap = risk_valve_backtest(
1273:                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
2426:        from SSS_EnsembleTab import risk_valve_backtest
2427:        out = risk_valve_backtest(
3424:    # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3426:        from SSS_EnsembleTab import risk_valve_backtest
3427:        logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3429:        logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3430:        return go.Figure(), f"âŒ ç„¡æ³•åŒ¯å…¥ risk_valve_backtest: {e}"
3440:                out = risk_valve_backtest(
3837:        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3839:            from SSS_EnsembleTab import risk_valve_backtest
3840:            logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3842:            logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3853:                    out = risk_valve_backtest(
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
Iâ€™ll update imports at lines 893 and 1240 to include new functions to cover all bases.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '884,904p' app_dash.py
                except Exception:
                    pass

                if force_trigger:
                    valve_triggered_local = True
                    if ratio_local is None:
                        ratio_local = 1.5

                if valve_triggered_local:
                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
                    # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
                    open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
                    # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
                    w_series = None
                    try:
                        ds_std = df_from_pack(result.get('daily_state_std'))
                        if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
                            w_series = ds_std['w']
                    except Exception:
                        pass
                    if w_series is None:
I'm tracing the risk valve behavior for individual strategies and checking if metrics are recomputed.

root@478772c9b537:/workspace/SSS096# /bin.sed -n '904,940p' app_dash.py
bash: /bin.sed: No such file or directory
root@478772c9b537:/workspace/SSS096# /bin/sed -n '904,940p' app_dash.py
                    if w_series is None:
                        ds = df_from_pack(result.get('daily_state'))
                        if ds is not None and not ds.empty and 'w' in ds.columns:
                            w_series = ds['w']

                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
                        )

                        rv = risk_valve_backtest(
                            open_px=open_px,
                            w=w_series,
                            cost=cost_params,
                            benchmark_df=df_raw,
                            mode="cap",
                            cap_level=float(risk_cap),
                        )

                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
                        result['daily_state']      = pack_df(rv["daily_state_valve"])
                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
                        result['weight_curve']     = pack_series(rv["weights_valve"])
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
                        result['valve'] = {
                            "applied": True,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio_local
                        }
I'll carry on from line 940 to review the following sections.

root@478772c9b537:/workspace/SSS096# /bin/sed -n '940,980p' app_dash.py

                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
                    else:
                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
                else:
                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                    result['valve'] = {
                        "applied": False,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio_local if ratio_local is not None else "N/A"
                    }
        elif strat_type == 'ensemble':
            # ä½¿ç”¨æ–°çš„ ensemble_runner é¿å…å¾ªç’°ä¾è³´
            try:
                from runners.ensemble_runner import run_ensemble_backtest
                from SSS_EnsembleTab import EnsembleParams, CostParams, RunConfig

                # æŠŠ SSSv096 çš„å·¢ç‹€åƒæ•¸æ”¤å¹³
                flat_params = {}
                flat_params.update(strat_params.get('params', {}))
                flat_params.update(strat_params.get('trade_cost', {}))
                flat_params['method'] = strat_params.get('method', 'majority')
                flat_params['ticker'] = ticker

                # ä½¿ç”¨æ¯”ä¾‹é–€æª»é¿å… N è®Šå‹•æ™‚å¤±çœŸ
                if 'majority_k' in flat_params and flat_params.get('method') == 'majority':
                    flat_params['majority_k_pct'] = 0.55
                    flat_params.pop('majority_k', None)
                    logger.info(f"[Ensemble] ä½¿ç”¨æ¯”ä¾‹é–€æª» majority_k_pct={flat_params['majority_k_pct']}")

                # å‰µå»ºé…ç½®
                ensemble_params = EnsembleParams(
                    floor=flat_params.get("floor", 0.2),
                    ema_span=flat_params.get("ema_span", 3),
                    delta_cap=flat_params.get("delta_cap", 0.3),
                    majority_k=flat_params.get("majority_k", 6),
                    min_cooldown_days=flat_params.get("min_cooldown_days", 1),
                    min_trade_dw=flat_params.get("min_trade_dw", 0.01)
                )
Iâ€™ll adjust imports and recompute metrics in the global risk valve block after popping data.

root@478772c9b537:/workspace/SSS096# sed -n '200,220p' app_dash.py

    if isinstance(data, (list, dict)):
        try:
            return pd.Series(data)
        except Exception:
            return pd.Series(dtype=float)

    return pd.Series(dtype=float)

from SSSv096 import (
    param_presets, load_data, compute_single, compute_dual, compute_RMA,
    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_peri
ods
)

# å½ˆæ€§åŒ¯å…¥ pack_df/pack_series å‡½æ•¸
try:
    from sss_core.schemas import pack_df, pack_series
except Exception:
    from schemas import pack_df, pack_series

# åŒ¯å…¥æ¬Šé‡æ¬„ä½ç¢ºä¿å‡½å¼
I'll update the import list to include calculate_metrics.

root@478772c9b537:/workspace/SSS096# apply_patch <<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -from SSSv096 import (
> -    param_presets, load_data, compute_single, compute_dual, compute_RMA,
> -    compute_ssma_turn_combined, backtest_unified, plot_stock_price, plot_equity_cash, plot_weight_series, calculate_holding_p
eriods
> -)
> +from SSSv096 import (
> +    param_presets, load_data, compute_single, compute_dual, compute_RMA,
> +    compute_ssma_turn_combined, backtest_unified, plot_stock_price,
> +    plot_equity_cash, plot_weight_series, calculate_holding_periods,
> +    calculate_metrics
> +)
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
Iâ€™ll adjust the import for risk_valve_backtest around lines 1236-1248.

root@478772c9b537:/workspace/SSS096# sed -n '1232,1252p' app_dash.py

                    # 4) å›å¯« dsï¼Œä¸¦é‡ç®—äº¤æ˜“/æ¬Šç›Š
                    result["daily_state_std"] = pack_df(ds)

                    # open åƒ¹ï¼ˆæ²’æœ‰ open å°±é€€è€Œæ±‚å…¶æ¬¡ç”¨æ”¶ç›¤åƒ¹ï¼‰
                    open_px = (df_raw["open"] if "open" in df_raw.columns else df_raw.get("æ”¶ç›¤åƒ¹")).astype(float)
                    open_px = open_px.reindex(ds.index).dropna()

                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
                    try:
                        from SSS_EnsembleTab import risk_valve_backtest, CostParams

                        # æˆæœ¬åƒæ•¸
                        trade_cost = (strat_params.get("trade_cost", {})
                                      if isinstance(strat_params, dict) else {})
                        cost = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0)),
                        )
Iâ€™ll add a timestamp comment, like "# 2025-08-19 ä¿®æ­£...", near the import or recompute section.

root@478772c9b537:/workspace/SSS096# apply_patch <<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -                    try:
> -                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
> +                    try:
> +                        from SSS_EnsembleTab import (
> +                            risk_valve_backtest,
> +                            CostParams,
> +                            build_trade_snapshots_from_ledger,
> +                            _sell_returns_pct_from_ledger,
> +                        )
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
I'll find the lines around 1290 to patch the trade_df rebuilding after pop.

root@478772c9b537:/workspace/SSS096# sed -n '1274,1318p' app_dash.py
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
I'll add a comment in Traditional Chinese with a timestamp of 2025-08-19 and apply the patch.

root@478772c9b537:/workspace/SSS096# apply_patch <<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
>                          # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
>                          if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
>                              try:
>                                  result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
>                              except Exception:
>                                  # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
>                                  result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity')
)
>
> -                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
> -                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
> -                            if k in result:
> -                                result.pop(k, None)
> +                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
> +                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
> +                            if k in result:
> +                                result.pop(k, None)
> +
> +                        # 2025-08-19 ä¿®å¾©ï¼šé¢¨éšªé–¥é–€å¥—ç”¨å¾Œé‡å»ºäº¤æ˜“è³‡æ–™èˆ‡ç¸¾æ•ˆæŒ‡æ¨™ #app_dash.py
> +                        if 'trade_ledger_valve' in result_cap and 'daily_state_valve' in result_cap:
> +                            try:
> +                                trade_df_valve = build_trade_snapshots_from_ledger(result_cap['trade_ledger_valve'])
> +                                result['trade_df'] = pack_df(trade_df_valve)
> +                                sell_ret = _sell_returns_pct_from_ledger(result_cap['trade_ledger_valve']) / 100.0
> +                                trades_list = [(d, r, d) for d, r in sell_ret.items()]
> +                                equity_valve = result_cap['daily_state_valve']['equity']
> +                                idx_df = pd.DataFrame(index=equity_valve.index)
> +                                result['metrics'] = calculate_metrics(trades_list, idx_df, equity_valve)
> +                            except Exception as e:
> +                                logger.warning(f"[{strat}] é¢¨éšªé–¥é–€ç¸¾æ•ˆé‡ç®—å¤±æ•—: {e}")
> +                                result['metrics'] = {}
> +                        else:
> +                            result['metrics'] = {}
>
>                          # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
>                          result['_risk_valve_applied'] = True
>                          result['_risk_valve_params'] = {
>                              'cap': float(risk_cap),
>                              'atr_ratio': float(atr_ratio),
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
I don't think I need to change the earlier imports, but I do need to check the pandas import.

root@478772c9b537:/workspace/SSS096# /usr/bin/python -m py_compile app_dash.py
bash: /usr/bin/python: No such file or directory
Hmm, maybe Python is installed under /usr/bin/python3. Let's try that instead.

root@478772c9b537:/workspace/SSS096# /usr/bin/python3 -m py_compile app_dash.py
root@478772c9b537:/workspace/SSS096# pwsh -ExecutionPolicy Bypass -File tools/quick_check.ps1
bash: command not found: pwsh
PowerShell isn't installed, so I'll try running the recommended test with Python as best I can.

root@478772c9b537:/workspace/SSS096# /usr/bin/python3 SSSv096.py --strategy RMA_Factor --param_preset op.json
Traceback (most recent call last):
  File "/workspace/SSS096/SSSv096.py", line 17, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
The environment lacks pandas, and tests failed. Now, I'll check modifications with git status.

root@478772c9b537:/workspace/SSS096# /usr/bin/git status --short
 M app_dash.py
?? __pycache__/
I'll clean up the pycache generated by py_compile.

root@478772c9b537:/workspace/SSS096# /bin/rm -r __pycache__
root@478772c9b537:/workspace/SSS096# /usr/bin/git status --short
 M app_dash.py
Only app_dash.py was modified. I'll add a comment in Traditional Chinese as required.

root@478772c9b537:/workspace/SSS096# /usr/bin/git commit -am "fix: restore metrics after risk valve" && /usr/bin/git status --sh
ort
[work b4888bd] fix: restore metrics after risk valve
 1 file changed, 27 insertions(+), 4 deletions(-)
root@478772c9b537:/workspace/SSS096# /usr/bin/git status --short
I'll gather the line numbers for the changes in app_dash.py.

root@478772c9b537:/workspace/SSS096# nl -ba app_dash.py | sed -n '200,240p'
   200
   201      if isinstance(data, (list, dict)):
   202          try:
   203              return pd.Series(data)
   204          except Exception:
   205              return pd.Series(dtype=float)
   206
   207      return pd.Series(dtype=float)
   208
   209  from SSSv096 import (
   210      param_presets, load_data, compute_single, compute_dual, compute_RMA,
   211      compute_ssma_turn_combined, backtest_unified, plot_stock_price,
   212      plot_equity_cash, plot_weight_series, calculate_holding_periods,
   213      calculate_metrics
   214  )
   215
   216  # å½ˆæ€§åŒ¯å…¥ pack_df/pack_series å‡½æ•¸
   217  try:
   218      from sss_core.schemas import pack_df, pack_series
   219  except Exception:
   220      from schemas import pack_df, pack_series
   221
   222  # åŒ¯å…¥æ¬Šé‡æ¬„ä½ç¢ºä¿å‡½å¼
   223  try:
   224      from sss_core.normalize import _ensure_weight_columns
   225  except Exception:
   226      # å¦‚æœç„¡æ³•åŒ¯å…¥ï¼Œå®šç¾©ä¸€å€‹ç©ºçš„å‡½å¼ä½œç‚º fallback
   227      def _ensure_weight_columns(df):
   228          return df
   229
   230  # å‡è¨­ä½ æœ‰ get_version_history_html
   231  try:
   232      from version_history import get_version_history_html
   233  except ImportError:
   234      def get_version_history_html() -> str:
   235          return "<b>ç„¡æ³•è¼‰å…¥ç‰ˆæœ¬æ­·å²è¨˜éŒ„</b>"
   236
   237  # --- ä¿è­‰æ”¾é€² Store çš„éƒ½æ˜¯ JSON-safe ---
   238  def _pack_any(x):
   239      import pandas as pd
   240      if isinstance(x, pd.DataFrame):
I'll grab the lines around 1274 to document the new code after the recomputation.

root@478772c9b537:/workspace/SSS096# nl -ba app_dash.py | sed -n '1260,1325p'
  1260                          if 'close' in df_raw.columns:
  1261                              bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
  1262                          elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
  1263                              bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
  1264                          if 'high' in df_raw.columns and 'low' in df_raw.columns:
  1265                              bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
  1266                              bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
  1267                          elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
  1268                              bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
  1269                              bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")
  1270
  1271                          result_cap = risk_valve_backtest(
  1272                              open_px=open_px,
  1273                              w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
  1274                              cost=cost,
  1275                              benchmark_df=bench,
  1276                              mode="cap",
  1277                              cap_level=1.0,
  1278                          )
  1279                      except Exception as e:
  1280                          logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
  1281                          result_cap = None
  1282
  1283                      if result_cap:
  1284                          # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
  1285                          logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataF
rame()))} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")
  1286
  1287                          # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
  1288                          if 'trade_ledger_valve' in result_cap:
  1289                              result['trades'] = pack_df(result_cap['trade_ledger_valve'])
  1290                              result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
  1291                              result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
  1292
  1293                          if 'daily_state_valve' in result_cap:
  1294                              result['daily_state'] = pack_df(result_cap['daily_state_valve'])
  1295                              result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])
  1296
  1297                          if 'weights_valve' in result_cap:
  1298                              result['weight_curve'] = pack_series(result_cap['weights_valve'])
  1299
  1300                          # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
  1301                          if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
  1302                              try:
  1303                                  result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
  1304                              except Exception:
  1305                                  # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
  1306                                  result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equ
ity'))
  1307
  1308                          # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
  1309                          for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
  1310                              if k in result:
  1311                                  result.pop(k, None)
  1312
  1313                          # 2025-08-19 ä¿®å¾©ï¼šé¢¨éšªé–¥é–€å¥—ç”¨å¾Œé‡å»ºäº¤æ˜“è³‡æ–™èˆ‡ç¸¾æ•ˆæŒ‡æ¨™ #app_dash.py
  1314                          if 'trade_ledger_valve' in result_cap and 'daily_state_valve' in result_cap:
  1315                              try:
  1316                                  trade_df_valve = build_trade_snapshots_from_ledger(result_cap['trade_ledger_valve'])
  1317                                  result['trade_df'] = pack_df(trade_df_valve)
  1318                                  sell_ret = _sell_returns_pct_from_ledger(result_cap['trade_ledger_valve']) / 100.0
  1319                                  trades_list = [(d, r, d) for d, r in sell_ret.items()]
  1320                                  equity_valve = result_cap['daily_state_valve']['equity']
  1321                                  idx_df = pd.DataFrame(index=equity_valve.index)
  1322                                  result['metrics'] = calculate_metrics(trades_list, idx_df, equity_valve)
  1323                              except Exception as e:
  1324                                  logger.warning(f"[{strat}] é¢¨éšªé–¥é–€ç¸¾æ•ˆé‡ç®—å¤±æ•—: {e}")
  1325                                  result['metrics'] = {}
I'll keep going and capture lines following line 1325.

root@478772c9b537:/workspace/SSS096# nl -ba app_dash.py | sed -n '1325,1345p'
  1325                                  result['metrics'] = {}
  1326                          else:
  1327                              result['metrics'] = {}
  1328
  1329                          # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
  1330                          result['_risk_valve_applied'] = True
  1331                          result['_risk_valve_params'] = {
  1332                              'cap': float(risk_cap),
  1333                              'atr_ratio': float(atr_ratio),
  1334                              'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
  1335                              'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
  1336                          }
  1337
  1338                          true_days = int(mask_aligned.sum())
  1339                          logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
  1340                      else:
  1341                          logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")
  1342
  1343          results[strat] = result
  1344
  1345      # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
I'll include lines 209-214 and 1308-1327, plus a note on the bug fix and failed tests.

2.
Configuring language runtimes...
# Python: 3.12
# Node.js: v20 (default: v22)
default -> 20 (-> v20.19.4)
Now using node v20.19.4 (npm v10.9.3)
# Ruby: 3.4.4 (default: 3.2.3)
mise ~/.config/mise/config.toml tools: ruby@3.4.4
# Rust: 1.88.0 (default: 1.89.0)
info: using existing install for '1.88.0-x86_64-unknown-linux-gnu'
info: default toolchain set to '1.88.0-x86_64-unknown-linux-gnu'

  1.88.0-x86_64-unknown-linux-gnu unchanged - rustc 1.88.0 (6b00bc388 2025-06-23)

# Go: go1.24.3 (default: go1.24.3)
# Swift: 6.1 (default: 6.1)
# PHP: 8.4 (default: 8.4)
Running auto setup...
Scanning repository (up to depth 3) for dependency configurations...
No known dependency configuration files found up to depth 3.
å¾Œé¢ç‰ˆæœ¬ä¸æ˜¯æ–°å¢äº†é¢¨éšªé–¥é–€å¥—ç”¨åœ¨å„ç­–ç•¥å—
ä½†ä¸çŸ¥é“ç‚ºä»€éº¼å¥—ç”¨å¾Œ å„æŒ‡æ¨™æœƒåªå‰©ä¸‹å¹³å‡æŒå€‰å¤©æ•¸,å…¶ä»–æŒ‡æ¨™éƒ½æ¶ˆå¤±äº† è«‹ä¿®å¾©
I'll use ls to view the top-level of the repo, avoiding ls -R or grep -R.

ls
Hm, I think output might be missing due to a timing issue. Let's try waiting.

root@5583bff288d6:/workspace# ls
SSS096
I need to cd into /workspace/SSS096 to check the repo details.

root@5583bff288d6:/workspace# cd SSS096
Ah, I need to run ls again after changing directories in the same container.

root@5583bff288d6:/workspace/SSS096# ls
AGENTS.md                           data                      run_workflow_example.py
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md  debug_enhanced_data.py    runners
ENSEMBLE_WORKFLOW.md                ensemble_wrapper.py       setup.sh
LOGGING_SYSTEM_UPDATE_SUMMARY.md    extract_params.py         sss_backtest_outputs
PORTFOLIO_LEDGER_README.md          leverage.py               sss_core
SSS_EnsembleTab.py                  list.txt                  test
SSSv096.py                          list_folder_structure.py  tools
analysis                            presets                   utils_payload.py
app_dash.py                         ruff.toml                 version_history.py
assets                              run_enhanced_debug.py     workflow_summary_20250812_023903.txt
config.yaml                         run_enhanced_ensemble.py  workflow_summary_20250818_040656.txt
convert_results_to_trades.py        run_workflow.py
I've found AGENTS.md at the root. Let's open it and read what's inside.

root@5583bff288d6:/workspace/SSS096# cat AGENTS.md
# SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

SSS096 æ˜¯ä¸€å€‹è‚¡ç¥¨ç­–ç•¥å›æ¸¬èˆ‡åˆ†æç³»çµ±ï¼Œä¸»è¦åŒ…å«ï¼š
- ç­–ç•¥å›æ¸¬å¼•æ“ï¼ˆSSSv096.pyï¼‰
- Web UI ç•Œé¢ï¼ˆapp_dash.pyï¼‰
- å¢å¼·åˆ†ææ¨¡çµ„ï¼ˆanalysis/ï¼‰
- Ensemble ç­–ç•¥åŸ·è¡Œï¼ˆrunners/ï¼‰
- æ•¸æ“šè™•ç†èˆ‡è½‰æ›å·¥å…·

## ğŸ¯ å·¥ä½œé‡é»å€åŸŸ

### æ ¸å¿ƒæª”æ¡ˆ
- `SSSv096.py` - ä¸»è¦ç­–ç•¥å›æ¸¬å¼•æ“
- `app_dash.py` - Web UI ä¸»æ‡‰ç”¨
- `ensemble_wrapper.py` - Ensemble ç­–ç•¥åŒ…è£å™¨
- `analysis/` - åˆ†ææ¨¡çµ„ç›®éŒ„

### é¿å…ä¿®æ”¹çš„æª”æ¡ˆ
- `tools/quick_check.ps1` - è‡ªå‹•åŒ–æª¢æŸ¥è…³æœ¬ï¼ˆé™¤éå¿…è¦ï¼‰
- å·²æ¨™è¨˜ç‚º "past/" çš„èˆŠç‰ˆæœ¬æª”æ¡ˆ
- ç·¨è­¯å¾Œçš„ `.pyc` æª”æ¡ˆ

## ğŸ”§ é–‹ç™¼ç’°å¢ƒè¨­å®š

### Codex ç’°å¢ƒè¨­ç½®ï¼ˆæ¨è–¦ï¼‰
```bash
# åŸ·è¡Œè‡ªå‹•è¨­ç½®è…³æœ¬
chmod +x setup.sh
./setup.sh

# æ¸¬è©¦è¨­ç½®æ˜¯å¦æˆåŠŸ
python test_setup.py
```

### Python ç’°å¢ƒï¼ˆæ‰‹å‹•è¨­ç½®ï¼‰
```bash
# å®‰è£ä¾è³´å¥—ä»¶
pip install pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components

# æª¢æŸ¥ Python ç‰ˆæœ¬ï¼ˆå»ºè­° 3.8+ï¼‰
python --version
```

### ä»£ç†è¨­ç½®ï¼ˆå¦‚æœé‡åˆ° 403 éŒ¯èª¤ï¼‰
```bash
# æª¢æŸ¥ä»£ç†è­‰æ›¸
echo $CODEX_PROXY_CERT

# é…ç½® pip ä½¿ç”¨ä»£ç†
pip config set global.cert "$CODEX_PROXY_CERT"
pip config set global.trusted-host "proxy:8080"
```

### å°ˆæ¡ˆçµæ§‹å°èˆª
```bash
# å¿«é€ŸæŸ¥çœ‹ç›®éŒ„çµæ§‹
python list_folder_structure.py

# æŸ¥çœ‹ç‰¹å®šç›®éŒ„å…§å®¹
ls analysis/
ls runners/
```

## ğŸ§ª æ¸¬è©¦èˆ‡é©—è­‰

### å¿«é€Ÿæª¢æŸ¥
```bash
# åŸ·è¡Œè‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆé‡è¦ï¼ï¼‰
powershell -ExecutionPolicy Bypass -File tools\quick_check.ps1
```

### å›æ¸¬æ¸¬è©¦
```bash
# åŸ·è¡Œå–®ä¸€ç­–ç•¥å›æ¸¬
python SSSv096.py --strategy RMA_Factor --param_preset op.json

# åŸ·è¡Œ Ensemble ç­–ç•¥
python run_enhanced_ensemble.py --method majority --top_k 5
```

### UI æ¸¬è©¦
```bash
# å•Ÿå‹• Web UI
python app_dash.py
```

## ğŸ“ ç¨‹å¼ç¢¼è¦ç¯„

### è¨»è§£èˆ‡è¼¸å‡º
- **ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**é€²è¡Œè¨»è§£å’Œè¼¸å‡º
- ä¿®æ”¹ç´€éŒ„éœ€åŠ å…¥æ—¥æœŸæ™‚é–“æˆ³è¨˜
- è·¯å¾‘èªªæ˜æ ¼å¼ï¼š`#å­è³‡æ–™å¤¾/æª”æ¡ˆå`

### æ—¥èªŒè¨˜éŒ„
- ä½¿ç”¨ `analysis/logging_config.py` ä¸­çš„æ—¥èªŒå™¨
- é‡è¦æ“ä½œéœ€è¨˜éŒ„åˆ°æ—¥èªŒæª”æ¡ˆ
- éŒ¯èª¤è™•ç†éœ€åŒ…å«è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯

### è³‡æ–™æ ¼å¼
- æ—¥æœŸæ¬„ä½çµ±ä¸€ä½¿ç”¨ ISO æ ¼å¼ï¼š`YYYY-MM-DD`
- æ•¸å€¼æ¬„ä½ä½¿ç”¨ float é¡å‹
- é¿å…ä½¿ç”¨ä¸­æ–‡æ¬„ä½åç¨±ï¼ˆé™¤éå¿…è¦ï¼‰

## ğŸ” é™¤éŒ¯æŒ‡å—

### å¸¸è¦‹å•é¡Œ
1. **æ¨¡çµ„å°å…¥å¤±æ•—**ï¼šæª¢æŸ¥ `sys.path` å’Œç›¸å°å°å…¥
2. **æ•¸æ“šæ ¼å¼éŒ¯èª¤**ï¼šé©—è­‰ CSV æª”æ¡ˆçµæ§‹å’Œæ¬„ä½åç¨±
3. **è¨˜æ†¶é«”ä¸è¶³**ï¼šæª¢æŸ¥å¤§æ•¸æ“šé›†çš„è™•ç†æ–¹å¼

### é™¤éŒ¯å·¥å…·
```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(level=logging.DEBUG)

# æª¢æŸ¥æ•¸æ“šçµæ§‹
print(df.info())
print(df.head())
```

## ğŸ“Š æ•¸æ“šè™•ç†è¦ç¯„

### è¼¸å…¥æ•¸æ“š
- æ”¯æ´ CSVã€Excelã€JSON æ ¼å¼
- å¿…è¦æ¬„ä½ï¼šäº¤æ˜“æ—¥æœŸã€æ¬Šé‡è®ŠåŒ–ã€ç›ˆè™§%
- å¯é¸æ¬„ä½ï¼šäº¤æ˜“é¡å‹ã€åƒ¹æ ¼ã€æˆäº¤é‡

### è¼¸å‡ºæ•¸æ“š
- çµ±ä¸€ Schemaï¼šequityã€tradesã€daily_stateã€trade_ledger
- é¿å… KeyError å’Œæ¬„ä½ç¼ºå¤±
- æ”¯æ´å¤šç¨®è¼¸å‡ºæ ¼å¼

## ğŸš€ éƒ¨ç½²èˆ‡ç¶­è­·

### æª”æ¡ˆç®¡ç†
- å®šæœŸæ¸…ç†èˆŠçš„æ—¥èªŒå’Œå¿«å–æª”æ¡ˆ
- å‚™ä»½é‡è¦çš„é…ç½®å’Œçµæœæª”æ¡ˆ
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶è¿½è¹¤è®Šæ›´

### æ€§èƒ½å„ªåŒ–
- å¤§æ•¸æ“šé›†ä½¿ç”¨å¿«å–æ©Ÿåˆ¶
- é¿å…é‡è¤‡è¨ˆç®—
- ä½¿ç”¨é©ç•¶çš„æ•¸æ“šçµæ§‹

## âš ï¸ æ³¨æ„äº‹é …

### å®‰å…¨è€ƒé‡
- ä¸è¦ç¡¬ç·¨ç¢¼ API é‡‘é‘°
- é©—è­‰æ‰€æœ‰ç”¨æˆ¶è¼¸å…¥
- ä¿è­·æ•æ„Ÿæ•¸æ“š

### ç›¸å®¹æ€§
- ç¶­æŒèˆ‡ç¾æœ‰å·¥ä½œæµç¨‹çš„ç›¸å®¹æ€§
- æ¸¬è©¦æ‰€æœ‰æ•´åˆé»
- é¿å…ç ´å£ç¾æœ‰åŠŸèƒ½

## ğŸš€ Codex ç’°å¢ƒè¨­ç½®

### è‡ªå‹•è¨­ç½®
```bash
# åŸ·è¡Œè¨­ç½®è…³æœ¬
./setup.sh

# è…³æœ¬æœƒè‡ªå‹•ï¼š
# 1. é…ç½®ä»£ç†è¨­ç½®
# 2. å®‰è£æ‰€æœ‰å¿…è¦ä¾è³´
# 3. å‰µå»ºå›é€€æ—¥èªŒç³»çµ±
# 4. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```

### æ‰‹å‹•è¨­ç½®ï¼ˆå¦‚æœè‡ªå‹•è¨­ç½®å¤±æ•—ï¼‰
```bash
# 1. é…ç½®ä»£ç†
export PIP_CERT="$CODEX_PROXY_CERT"
export NODE_EXTRA_CA_CERTS="$CODEX_PROXY_CERT"

# 2. å®‰è£æ ¸å¿ƒä¾è³´
pip install --no-cache-dir pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components yfinance pyyaml joblib

# 3. å®‰è£åˆ†æå¥—ä»¶
pip install --no-cache-dir scikit-learn scipy statsmodels plotly kaleido

# 4. å‰µå»ºå¿…è¦ç›®éŒ„
mkdir -p analysis/log analysis/cache cache log results sss_backtest_outputs
```

### å¸¸è¦‹å•é¡Œè§£æ±º
- **pip install 403 éŒ¯èª¤**ï¼šæª¢æŸ¥ `$CODEX_PROXY_CERT` ç’°å¢ƒè®Šæ•¸
- **joblib å°å…¥å¤±æ•—**ï¼šä½¿ç”¨ `logging_config_fallback.py` å›é€€ç‰ˆæœ¬
- **æ¨¡çµ„è·¯å¾‘å•é¡Œ**ï¼šè¨­ç½® `PYTHONPATH` ç’°å¢ƒè®Šæ•¸

## ğŸ“ æŠ€è¡“æ”¯æ´

### å•é¡Œå›å ±
- æä¾›å®Œæ•´çš„éŒ¯èª¤è¨Šæ¯å’Œå †ç–Šè¿½è¹¤
- åŒ…å«é‡ç¾æ­¥é©Ÿå’Œç’°å¢ƒä¿¡æ¯
- æª¢æŸ¥ç›¸é—œçš„æ—¥èªŒæª”æ¡ˆ

### æ–‡æª”æ›´æ–°
- ä¿®æ”¹åŠŸèƒ½æ™‚åŒæ­¥æ›´æ–°ç›¸é—œæ–‡æª”
- ä½¿ç”¨æ¸…æ™°çš„ç¯„ä¾‹å’Œèªªæ˜
- ä¿æŒæ–‡æª”çš„æ™‚æ•ˆæ€§

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€å¾Œæ›´æ–°ï¼š** 2025-08-18
**é©ç”¨æ–¼ï¼š** SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—
I need to find files related to "risk valve" or "risk" using rg search.

root@5583bff288d6:/workspace/SSS096# rg "risk" -n
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md
25:**1.1 _simulate_risk_valves å‡½å¼**
30:**1.2 _calculate_risk_valve_impact å‡½å¼**
32:- æª¢æŸ¥ `benchmark_enhanced` æ˜¯å¦æœ‰ `risk_valve_triggered` æ¬„ä½

app_dash.py
446:                        dcc.Input(id='risk-cap-input', type='number', min=0.1, max=1.0, step=0.1, value=0.3,
461:                    html.Div(id='risk-valve-status', style={"marginTop":"8px","padding":"8px","backgroundColor":"#f8f9fa","b
orderRadius":"4px","border":"1px solid #dee2e6"}),
575:    Output('risk-valve-status', 'children'),
578:        Input('risk-cap-input', 'value'),
586:def update_risk_valve_status(global_apply, risk_cap, atr_ratio, force_trigger, ticker, start_date, end_date):
590:    logger.info(f"risk_cap: {risk_cap}")
688:                                    html.Small(f"é¢¨éšªCAP: {risk_cap*100:.0f}%", style={"color":"#666","fontSize":"11px"}),
690:                                    html.Small(f"ç¾é‡‘ä¿ç•™ä¸‹é™: {(1-risk_cap)*100:.0f}%", style={"color":"#666","fontSize":"1
1px"}),
762:        Input('risk-cap-input', 'value'),
771:def run_backtest(n_clicks, auto_run, ticker, start_date, end_date, discount, cooldown, bad_holding, global_apply, risk_cap,
atr_ratio, force_trigger, strategy, param_values, param_ids, stored_data):
813:            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")
893:                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
918:                        rv = risk_valve_backtest(
924:                            cap_level=float(risk_cap),
937:                            "cap": float(risk_cap),
941:                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
949:                        "cap": float(risk_cap),
986:                #     if ensemble_params.delta_cap > risk_cap:
987:                #         ensemble_params.delta_cap = risk_cap
991:                #     min_floor = 1 - risk_cap
1075:                    from SSS_EnsembleTab import risk_valve_backtest
1078:                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
1080:                    rv = risk_valve_backtest(
1086:                        cap_level=float(risk_cap),
1093:                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
1103:                        "cap": float(risk_cap),
1112:                            "cap": float(risk_cap),
1176:                    "cap": float(risk_cap),
1228:                    w_new[mask_aligned] = np.minimum(w_new[mask_aligned], float(risk_cap))
1238:                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
1240:                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
1264:                        result_cap = risk_valve_backtest(
1273:                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
1307:                        result['_risk_valve_applied'] = True
1308:                        result['_risk_valve_params'] = {
1309:                            'cap': float(risk_cap),
1316:                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
2426:        from SSS_EnsembleTab import risk_valve_backtest
2427:        out = risk_valve_backtest(
2439:    sig = out["signals"]["risk_trigger"]
2553:    risk_eff = np.nan
2555:        risk_eff = avg_net / abs(avg_mdd)
2624:        kpi("é¢¨éšªæ•ˆç‡", f"{risk_eff:.3f}" if pd.notna(risk_eff) else "â€”"),
2932:                            risk_eff = avg_net / abs(avg_mdd) if avg_mdd != 0 else 0
2940:                                "é¢¨éšªæ•ˆç‡": round(risk_eff, 3),
3424:    # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3426:        from SSS_EnsembleTab import risk_valve_backtest
3427:        logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3429:        logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3430:        return go.Figure(), f"âŒ ç„¡æ³•åŒ¯å…¥ risk_valve_backtest: {e}"
3440:                out = risk_valve_backtest(
3452:                sig = out["signals"]["risk_trigger"]
3471:                    "risk_trigger_days": trigger_days,
3507:            size=np.clip(dfp['risk_trigger_days'] / 5.0, 6, 30),
3554:        risk_trigger_days = 50
3619:                    risk_trigger_days = len(trade_df['trade_date'].dt.date.unique())
3623:                    risk_trigger_days = min(100, int(volatility * 1000))  # è½‰æ›ç‚ºåˆç†ç¯„åœ
3636:                    risk_trigger_days = len(trade_df['trade_date'].dt.date.unique())
3638:                    risk_trigger_days = len(trade_df)
3648:            'risk_trigger_days': risk_trigger_days
3668:            size=df['risk_trigger_days'] / 10,  # é»å¤§å°ï¼šé¢¨éšªè§¸ç™¼å¤©æ•¸
3837:        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3839:            from SSS_EnsembleTab import risk_valve_backtest
3840:            logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3842:            logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3853:                    out = risk_valve_backtest(
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
3876:                                'risk_trigger_days': metrics['risk_trigger_days'],

list.txt
1365:      ğŸ“„ risk_enhanced_clustering_analysis_v2.py (20.9 KB, 2025-07-04 13:02:52)

analysis/OSv3_6270833.py
394:def calculate_risk_adjusted_metrics(equity_curve, strategy_name):
873:                risk_adjusted_metrics = calculate_risk_adjusted_metrics(equity_curve, name)
927:                    **risk_adjusted_metrics  # æ·»åŠ é¢¨éšªèª¿æ•´æŒ‡æ¨™

SSS_EnsembleTab.py
1241:def compute_risk_valve_signals(benchmark_df: pd.DataFrame,
1248:       è¼¸å‡ºï¼šå« slope_20d/60dã€atrã€atr_ratioã€risk_trigger(bool) çš„ DataFrame"""
1281:    b["risk_trigger"] = (b["slope_20d"] < slope20_thresh) & \
1284:    return b[["slope_20d", "slope_60d", "atr", "atr_ratio", "risk_trigger"]]
1289:                           risk_trigger: pd.Series,
1295:    w = w.copy().reindex(risk_trigger.index).ffill().fillna(0.0)
1299:        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
1303:        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
1359:def risk_valve_backtest(open_px: pd.Series,
1371:    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
1373:    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)

analysis/OSv3.py
63:        'overfitting_risk': 'éæ“¬åˆé¢¨éšªï¼šç¶œåˆéæ“¬åˆé¢¨éšªè©•åˆ†ï¼Œ0-100',
89:        'low_risk': 'ä½é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšªâ‰¤30çš„ç­–ç•¥æ¯”ä¾‹',
90:        'medium_risk': 'ä¸­é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª31-60çš„ç­–ç•¥æ¯”ä¾‹',
91:        'high_risk': 'é«˜é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª>60çš„ç­–ç•¥æ¯”ä¾‹',
92:        'risk_return_correlation': 'é¢¨éšªå ±é…¬ç›¸é—œæ€§ï¼šéæ“¬åˆé¢¨éšªèˆ‡ç¸½å ±é…¬ç‡çš„ç›¸é—œä¿‚æ•¸'
543:    new_metrics = ['parameter_sensitivity', 'consistency_score', 'stability_score', 'overfitting_risk']
565:            if metric == 'overfitting_risk':
566:                low_risk = np.sum(valid_values <= 30) / len(valid_values)
567:                medium_risk = np.sum((valid_values > 30) & (valid_values <= 60)) / len(valid_values)
568:                high_risk = np.sum(valid_values > 60) / len(valid_values)
569:                analysis_results[f'{metric}_risk_distribution'] = {
570:                    'low_risk': low_risk,
571:                    'medium_risk': medium_risk,
572:                    'high_risk': high_risk
576:    if 'overfitting_risk' in available_metrics and 'total_return' in strategy_trials.columns:
577:        risk_values = pd.to_numeric(strategy_trials['overfitting_risk'], errors='coerce')
580:        valid_mask = risk_values.notna() & return_values.notna()
582:            correlation = risk_values[valid_mask].corr(return_values[valid_mask])
583:            analysis_results['risk_return_correlation'] = correlation
671:def calculate_risk_adjusted_metrics(equity_curve, strategy_name):
742:        additional_metrics = calculate_risk_adjusted_metrics(equity_curve, method_name)
1623:                risk_adjusted_metrics = calculate_risk_adjusted_metrics(equity_curve, name)
1677:                    **risk_adjusted_metrics  # æ·»åŠ é¢¨éšªèª¿æ•´æŒ‡æ¨™
...
566:                risk_dates = self.benchmark_enhanced[
567:                    self.benchmark_enhanced['risk_valve_triggered']
570:                if len(risk_dates) > 0:
571:                    ax1.scatter(risk_dates,
573:                                   self.benchmark_enhanced['risk_valve_triggered'], 'æ”¶ç›¤åƒ¹'
637:    print("2. åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬ï¼šanalyzer.risk_valve_backtest()")

analysis/past/adaptive_clustering_analysis_v7.py
26:INPUT_DIR = 'results/risk_enhanced_clustering'
30:def load_risk_clustered_data():
32:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
73:    risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
74:    for col in risk_cols:
313:    param_risk_cols = [col for col in df.columns if col.startswith('param_') or
314:                      col in ['annual_volatility', 'downside_risk', 'var_95_annual']]
315:    available_cols = [col for col in param_risk_cols if col in df.columns]
416:    data_dict = load_risk_clustered_data()
428:        risk_cols = ['annual_volatility', 'downside_risk', 'sortino_ratio', 'var_95_annual',
434:        enhanced_risk_cols = [col for col in enhanced_df.columns if any(risk in col for risk in risk_cols)]
436:        all_feature_cols = enhanced_param_cols + enhanced_risk_cols

analysis/past/comprehensive_distance_test.py
164:            overfitting_risk = result.get('overfitting_risk', {})
172:                'overfitting_risk_score': overfitting_risk.get('risk_score', 0),
173:                'overfitting_risk_level': overfitting_risk.get('overfitting_risk', 'unknown'),
225:        self.comparison_df['overfitting_risk_score'].plot(kind='bar', ax=ax3, color='salmon')
297:                    'overfitting_risk': result.get('overfitting_risk', {}),

analysis/past/simple_fine_clustering_v17.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
64:        if any(keyword in col.lower() for keyword in ['return', 'score', 'sharpe', 'calmar', 'sortino', 'volatility', 'risk',
 'var', 'cvar', 'downside', 'drawdown']):
368:    data_dict = load_risk_clustered_data()

analysis/past/adaptive_fine_clustering_v16.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
64:        if any(keyword in col.lower() for keyword in ['return', 'score', 'sharpe', 'calmar', 'sortino', 'volatility', 'risk',
 'var', 'cvar', 'downside', 'drawdown']):
507:    data_dict = load_risk_clustered_data()

analysis/past/fast_adaptive_clustering_v8.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
228:                col in ['annual_volatility', 'downside_risk']]
296:    data_dict = load_risk_clustered_data()
308:        risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
311:        all_feature_cols = param_cols + risk_cols

analysis/past/supervised_clustering_analysis_v3.py
27:INPUT_DIR = 'results/risk_enhanced_clustering'
31:def load_risk_clustered_data():
33:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
66:              f"risk_cluster={row['risk_cluster']}")
94:    risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
96:    for risk in risk_cols:
97:        if risk in case_df.columns:
98:            values = case_df[risk].values
103:                print(f"{risk}: å‡å€¼={mean_val:.3f}, æ¨™æº–å·®={std_val:.3f}, è®Šç•°ä¿‚æ•¸={cv:.3f}")
126:    risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual']
127:    feature_cols.extend([col for col in risk_cols if col in df.columns])
188:    risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
189:    risk_cols = [col for col in risk_cols if col in df.columns]
190:    feature_cols.extend(risk_cols)
241:        'original_risk': df['risk_cluster'].values if 'risk_cluster' in df.columns else None
399:    data_dict = load_risk_clustered_data()

analysis/past/quick_distance_test.py
111:                overfitting_risk = analyzer.detect_overfitting_risk(df_processed)
121:                    'overfitting_risk_score': overfitting_risk.get('risk_score', 0),
122:                    'overfitting_risk_level': overfitting_risk.get('overfitting_risk', 'unknown'),
136:                logger.info(f"  éæ“¬åˆé¢¨éšªåˆ†æ•¸: {results[config['name']]['overfitting_risk_score']:.3f}")
152:        key_metrics = ['similar_pairs_count', 'avg_distance', 'overfitting_risk_score', 'parameter_correlation_avg']
175:        risk_scores = comparison_df['overfitting_risk_score']
176:        best_methods['overfitting_risk'] = risk_scores.idxmin()
200:        euclidean_risk = results.get('euclidean', {}).get('overfitting_risk_score', 1.0)
201:        other_methods_risk = [v.get('overfitting_risk_score', 1.0) for k, v in results.items() if k != 'euclidean']
203:        if other_methods_risk and min(other_methods_risk) < euclidean_risk:
204:            improvement = (euclidean_risk - min(other_methods_risk)) / euclidean_risk * 100

analysis/past/fine_grained_clustering_v13.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
355:                col in ['annual_volatility', 'downside_risk']]
641:    data_dict = load_risk_clustered_data()
650:        risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
652:        all_feature_cols = param_cols + risk_cols

analysis/past/balanced_clustering_analysis_v10.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
267:                col in ['annual_volatility', 'downside_risk']]
476:    data_dict = load_risk_clustered_data()
485:        risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
487:        all_feature_cols = param_cols + risk_cols

analysis/past/clustering_comparison_analysis_v9.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
449:    data_dict = load_risk_clustered_data()
456:        risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
458:        all_feature_cols = param_cols + risk_cols

analysis/past/small_cluster_analysis_v11.py
25:INPUT_DIR = 'results/risk_enhanced_clustering'
29:def load_risk_clustered_data():
31:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
306:                col in ['annual_volatility', 'downside_risk']]
510:    data_dict = load_risk_clustered_data()
519:        risk_cols = ['annual_volatility', 'downside_risk', 'var_95_annual', 'cvar_95_annual']
521:        all_feature_cols = param_cols + risk_cols

analysis/past/improved_supervised_clustering_v5.py
27:INPUT_DIR = 'results/risk_enhanced_clustering'
31:def load_risk_clustered_data():
33:    all_files = glob.glob(f'{INPUT_DIR}/*_risk_clustered.csv')
406:    data_dict = load_risk_clustered_data()
Iâ€™ll search for â€œavg holding daysâ€ or related terms in the code around risk valve.

root@5583bff288d6:/workspace/SSS096# rg "å¹³å‡æŒå€‰å¤©æ•¸" -n
app_dash.py
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",

analysis/OSv3.py
82:        'avg_hold_days': 'å¹³å‡æŒå€‰å¤©æ•¸ï¼šå¹³å‡æ¯æ¬¡äº¤æ˜“çš„æŒå€‰æ™‚é–“',

analysis/optuna_16.py
10:v10-12  æ”¯æ´å–®ä¸€/éš¨æ©Ÿ/ä¾åºæ•¸æ“šæºæ¨¡å¼,æ–°å¢å¹³å‡æŒå€‰å¤©æ•¸,ç›¸é—œä¿‚æ•¸åˆ†æ,ä¸¦è‡ªå‹•è¼¸å‡ºç†±åœ–,è©¦é©—çµæœèˆ‡æœ€ä½³åƒæ•¸è‡ªå‹•è¼¸å‡º CSV/JSON

SSSv096.py
927:    è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸.
933:        float: å¹³å‡æŒå€‰å¤©æ•¸.
1828:            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1864:    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
1867:    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
2293:            # é¡¯ç¤ºç­–ç•¥æ‘˜è¦,å‚³å…¥ trade_df ä»¥è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
I'm checking if the risk valve application is overriding metrics, leaving only avg_holding_period.

root@5583bff288d6:/workspace/SSS096# sed -n '1810,1875p' SSSv096.py
        elif k in ["num_trades", "max_consecutive_wins", "max_consecutive_losses"]:
            txt = str(int(v)) if pd.notna(v) else ""
        else:
            # å…¶ä»–å°±å…ˆç›¡é‡ç•¶ç´”æ–‡å­—é¡¯ç¤º
            txt = f"{v}"
        # æŠŠå­—å…¸ key â†’ ä¸­æ–‡é¡¯ç¤º label
        label_map = {
            "total_return": "ç¸½å›å ±ç‡",
            "annual_return": "å¹´åŒ–å›å ±ç‡",
            "win_rate": "å‹ç‡",
            "max_drawdown": "æœ€å¤§å›æ’¤",
            "max_drawdown_duration": "å›æ’¤æŒçºŒ",
            "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
            "sharpe_ratio": "å¤æ™®æ¯”ç‡",
            "sortino_ratio": "ç´¢æè«¾æ¯”ç‡",
            "payoff_ratio": "ç›ˆè™§æ¯”",
            "profit_factor": "ç›ˆè™§å› å­",
            "num_trades": "äº¤æ˜“æ¬¡æ•¸",
            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
            "annualized_volatility": "å¹´åŒ–æ³¢å‹•ç‡",
            "max_consecutive_wins": "æœ€å¤§é€£çºŒç›ˆåˆ©",
            "max_consecutive_losses": "æœ€å¤§é€£çºŒè™§æ",
            "avg_win": "å¹³å‡ç›ˆåˆ©",
            "avg_loss": "å¹³å‡è™§æ",
        }
        label = label_map.get(k, k)
        items.append((label, txt))

    # é–‹å§‹ç”¢ç”Ÿ HTML:å¤–å±¤ä¸€å€‹ flex container,å…§å±¤æ¯çµ„(æŒ‡æ¨™+æ•¸å€¼) éƒ½æ˜¯ flex item
    html = """
<div style="display:flex;flex-wrap:wrap;gap:12px;align-items:flex-start;">
"""
    for label, val in items:
        html += f"""
  <div style="flex:0 1 150px;border:1px solid #444;border-radius:4px;padding:8px 12px;background:#1a1a1a;">
    <div style="font-size:14px;color:#aaa;">{label}</div>
    <div style="font-size:20px;font-weight:bold;color:#fff;margin-top:4px;">{val}</div>
  </div>
"""
    html += "</div>"

    # å»æ‰å¤šé¤˜ç¸®æ’,é¿å…é–‹é ­ç©ºç™½è¢«è§£è®€æˆ code block
    html = textwrap.dedent(html)

    st.markdown(html, unsafe_allow_html=True)

def display_strategy_summary(strategy: str, params: Dict, metrics: Dict, smaa_source: str, trade_df: pd.DataFrame):
    """
    é¡¯ç¤ºç­–ç•¥åƒæ•¸èˆ‡å›æ¸¬ç¸¾æ•ˆæ‘˜è¦,ä½¿ç”¨ HTML Flexbox å¡ç‰‡å±•ç¤º.
    """
    # åƒæ•¸å±•ç¤º
    param_display = {k: v for k, v in params.items() if k != "strategy_type"}
    st.write("**åƒæ•¸è¨­å®š**: " + ", ".join(f"{k}: {v}" for k, v in param_display.items()))

    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
    avg_holding_period = calculate_holding_periods(trade_df)

    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
    metrics['avg_holding_period'] = avg_holding_period

    # ç¸¾æ•ˆæŒ‡æ¨™å±•ç¤º
    if metrics:
        display_metrics_flex(metrics)
    else:
        st.warning("å°šæœªåŸ·è¡Œå›æ¸¬,ç„¡æ³•é¡¯ç¤ºç¸¾æ•ˆæŒ‡æ¨™.")
# --- ä¸»æ‡‰ç”¨ç¨‹å¼ ---
I'll check where risk valve is applied to see if it's replacing the full metrics.

root@5583bff288d6:/workspace/SSS096# rg "metrics" -n SSS_EnsembleTab.py
1405:        "metrics": {
It looks like the metrics dictionary might be around line 1405, near the backtest.

root@5583bff288d6:/workspace/SSS096# sed -n '1390,1415p' SSS_EnsembleTab.py
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)

    return {
        "signals": sig,
        "weights_orig": w,
        "weights_valve": w2,
        "daily_state_orig": daily_state1,
        "daily_state_valve": daily_state2,
        "trade_ledger_orig": ledger1,
        "trade_ledger_valve": ledger2,
        "metrics": {
            "pf_orig": float(pf1), "pf_valve": float(pf2),
            "mdd_orig": float(mdd1), "mdd_valve": float(mdd2),
            "right_tail_sum_orig": float(rt1), "right_tail_sum_valve": float(rt2),
            "right_tail_reduction": float(right_tail_reduction)
        }
    }


# === PATCH: äº¤æ˜“è²¢ç»æ‹†è§£ï¼ˆæŒ‰åŠ ç¢¼/æ¸›ç¢¼éšæ®µï¼‰===
def trade_contribution_by_phase(trade_ledger: pd.DataFrame,
I'll check around lines 1060-1110 in app_dash to understand how risk valve is applied.

root@5583bff288d6:/workspace/SSS096# sed -n '1050,1115p' app_dash.py
                                else:
                                    logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼ŒATRæ¯”å€¼({ratio:.4f}) <= é–€æª»({atr_ratio})")
                            else:
                                logger.warning(f"[{strat}] Ensemble ATR(60) å€¼ç‚º 0ï¼Œç„¡æ³•è¨ˆç®—æ¯”å€¼")
                        else:
                            logger.warning(f"[{strat}] Ensemble ATR æ•¸æ“šä¸è¶³")
                    else:
                        logger.warning(f"[{strat}] Ensemble ATR è¨ˆç®—çµæœç‚ºç©º")

                except Exception as e:
                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— Ensemble ATR æ¯”å€¼: {e}")
                    logger.warning(f"[{strat}] éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")

                # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
                if force_trigger:
                    valve_triggered = True
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
                    if ratio is None:
                        ratio = 1.5  # è¨­å®šä¸€å€‹é è¨­å€¼ç”¨æ–¼é¡¯ç¤º

                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
                backtest_result = run_ensemble_backtest(cfg)

                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
                if global_apply and valve_triggered:
                    from SSS_EnsembleTab import risk_valve_backtest
                    bench = df_raw  # å·²å« open/high/low/close/volume

                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")

                    rv = risk_valve_backtest(
                        open_px=backtest_result.price_series,
                        w=backtest_result.weight_curve,
                        cost=cost_params,
                        benchmark_df=bench,
                        mode="cap",
                        cap_level=float(risk_cap),
                    )
                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
                    backtest_result.daily_state = rv["daily_state_valve"]
                    backtest_result.ledger = rv["trade_ledger_valve"]
                    backtest_result.weight_curve = rv["weights_valve"]
                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")

                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
                    if "metrics" in rv:
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['metr
ics'].get('pf_valve', 'N/A'):.2f}")
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={rv['
metrics'].get('mdd_valve', 'N/A'):.2f}%")

                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
                    result['valve'] = {
                        "applied": True,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio
                    }
                else:
                    if global_apply:
                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio if ratio is not None else "N/A"
                        }
                    else:
Iâ€™m checking the result variable in run_backtest to see if metrics are lost.

root@5583bff288d6:/workspace/SSS096# rg "result =" -n app_dash.py
868:            result = backtest_unified(df_ind, strat_type, backtest_params, buy_dates, sell_dates, discount=discount, trade_c
ooldown_bars=cooldown, bad_holding=bad_holding)
1071:                backtest_result = run_ensemble_backtest(cfg)
1125:                result = {
1146:                result = {
1170:            result = backtest_unified(df_ind, strat_type, strat_params, discount=discount, trade_cooldown_bars=cooldown, ba
d_holding=bad_holding)
1203:        result = _pack_result_for_store(result)
1397:            result = results.get(strategy)
1795:            result = results.get(strategy)
1846:            result = results.get(strategy)
2108:    result = results.get(strategy)
2161:    result = results.get(strategy)
3095:    result = results[selected_strategy]
3227:    best_result = results[best_strategy]
3363:        result = results[selected_strategy]
3790:            result = results[selected_strategy]
I'm checking how _pack_result_for_store stores metrics to understand if they're replaced by risk valve.

root@5583bff288d6:/workspace/SSS096# rg "_pack_result_for_store" -n app_dash.py
244:def _pack_result_for_store(result: dict) -> dict:
1203:        result = _pack_result_for_store(result)
root@5583bff288d6:/workspace/SSS096# sed -n '244,262p' app_dash.py
def _pack_result_for_store(result: dict) -> dict:
    # çµ±ä¸€æŠŠæ‰€æœ‰ pandas ç‰©ä»¶è½‰æˆå­—ä¸²ï¼ˆJSONï¼‰
    keys = [
        'trade_df', 'trades_df', 'signals_df',
        'equity_curve', 'cash_curve', 'price_series',
        'daily_state', 'trade_ledger',
        'daily_state_std', 'trade_ledger_std',
        'weight_curve'
    ]
    out = dict(result)
    for k in keys:
        if k in out:
            out[k] = _pack_any(out[k])
    # å¦å¤–æŠŠ datetime tuple çš„ trades è½‰å¯åºåˆ—åŒ–ï¼ˆä½ åŸæœ¬ä¹Ÿæœ‰åšï¼‰
    if 'trades' in out and isinstance(out['trades'], list):
        out['trades'] = [
            (str(t[0]), t[1], str(t[2])) if isinstance(t, tuple) and len(t) == 3 else t
            for t in out['trades']
        ]
Iâ€™ll look for the point where 'result' is built around line 868. Let's find more!

root@5583bff288d6:/workspace/SSS096# sed -n '840,909p' app_dash.py
                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
                        continue

                    atr_ratio_current = atr_20_latest / atr_60_latest
                    logger.info(f"[{strat}] æœ€æ–°ATRæ¯”å€¼: {atr_ratio_current:.4f} (20æœŸ:{atr_20_latest:.4f}, 60æœŸ:{atr_60_latest:
.4f})")
                else:
                    logger.warning(f"[{strat}] ATR è¨ˆç®—çµæœç‚ºç©º")

                # å¼·åˆ¶è§¸ç™¼æ™‚è¨­ç½®æ¨™è¨˜
                if force_trigger:
                    valve_triggered = True
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")

            except Exception as e:
                logger.warning(f"[{strat}] ATR è¨ˆç®—å¤±æ•—: {e}")
        else:
            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")

        if strat_type == 'ssma_turn':
            calc_keys = ['linlen', 'factor', 'smaalen', 'prom_factor', 'min_dist', 'buy_shift', 'exit_shift', 'vol_window', 'sig
nal_cooldown_days', 'quantile_win']
            ssma_params = {k: v for k, v in strat_params.items() if k in calc_keys}
            backtest_params = ssma_params.copy()
            backtest_params['stop_loss'] = strat_params.get('stop_loss', 0.0)

            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
            df_ind, buy_dates, sell_dates = compute_ssma_turn_combined(df_raw, df_factor, **ssma_params, smaa_source=smaa_src)
            if df_ind.empty:
                continue
            result = backtest_unified(df_ind, strat_type, backtest_params, buy_dates, sell_dates, discount=discount, trade_coold
own_bars=cooldown, bad_holding=bad_holding)

            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
            if global_apply:
                # åˆ¤æ–·æ˜¯å¦è¦è§¸ç™¼ï¼ˆèˆ‡ä½ çš„ ATR æª¢æŸ¥æˆ–å¼·åˆ¶è§¸ç™¼ä¸€è‡´ï¼‰
                valve_triggered_local = False
                ratio_local = None
                try:
                    atr_20 = calculate_atr(df_raw, 20)
                    atr_60 = calculate_atr(df_raw, 60)
                    if not atr_20.empty and not atr_60.empty:
                        a20 = atr_20.dropna().iloc[-1]
                        a60 = atr_60.dropna().iloc[-1]
                        if a60 > 0:
                            ratio_local = float(a20 / a60)
                            valve_triggered_local = (ratio_local >= atr_ratio)  # å»ºè­°ä½¿ç”¨ >=
                except Exception:
                    pass

                if force_trigger:
                    valve_triggered_local = True
                    if ratio_local is None:
                        ratio_local = 1.5

                if valve_triggered_local:
                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
                    # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
                    open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
                    # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
                    w_series = None
                    try:
                        ds_std = df_from_pack(result.get('daily_state_std'))
                        if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
                            w_series = ds_std['w']
                    except Exception:
                        pass
                    if w_series is None:
                        ds = df_from_pack(result.get('daily_state'))
                        if ds is not None and not ds.empty and 'w' in ds.columns:
                            w_series = ds['w']

                    if w_series is not None:
root@5583bff288d6:/workspace/SSS096# sed -n '909,977p' app_dash.py
                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
                        )

                        rv = risk_valve_backtest(
                            open_px=open_px,
                            w=w_series,
                            cost=cost_params,
                            benchmark_df=df_raw,
                            mode="cap",
                            cap_level=float(risk_cap),
                        )

                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
                        result['daily_state']      = pack_df(rv["daily_state_valve"])
                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
                        result['weight_curve']     = pack_series(rv["weights_valve"])
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
                        result['valve'] = {
                            "applied": True,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio_local
                        }

                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
                    else:
                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
                else:
                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                    result['valve'] = {
                        "applied": False,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio_local if ratio_local is not None else "N/A"
                    }
        elif strat_type == 'ensemble':
            # ä½¿ç”¨æ–°çš„ ensemble_runner é¿å…å¾ªç’°ä¾è³´
            try:
                from runners.ensemble_runner import run_ensemble_backtest
                from SSS_EnsembleTab import EnsembleParams, CostParams, RunConfig

                # æŠŠ SSSv096 çš„å·¢ç‹€åƒæ•¸æ”¤å¹³
                flat_params = {}
                flat_params.update(strat_params.get('params', {}))
                flat_params.update(strat_params.get('trade_cost', {}))
                flat_params['method'] = strat_params.get('method', 'majority')
                flat_params['ticker'] = ticker

                # ä½¿ç”¨æ¯”ä¾‹é–€æª»é¿å… N è®Šå‹•æ™‚å¤±çœŸ
                if 'majority_k' in flat_params and flat_params.get('method') == 'majority':
                    flat_params['majority_k_pct'] = 0.55
                    flat_params.pop('majority_k', None)
                    logger.info(f"[Ensemble] ä½¿ç”¨æ¯”ä¾‹é–€æª» majority_k_pct={flat_params['majority_k_pct']}")

                # å‰µå»ºé…ç½®
                ensemble_params = EnsembleParams(
                    floor=flat_params.get("floor", 0.2),
                    ema_span=flat_params.get("ema_span", 3),
                    delta_cap=flat_params.get("delta_cap", 0.3),
                    majority_k=flat_params.get("majority_k", 6),
                    min_cooldown_days=flat_params.get("min_cooldown_days", 1),
It looks like the metrics dictionary gets lost after applying the risk valve, affecting UI display. I'll check backtest_unified and UI functions to pinpoint where metrics should be recomputed.

I need to inspect the backtest_unified function, possibly in the SSS_EnsembleTab module.

root@5583bff288d6:/workspace/SSS096# rg "def backtest_unified" -n
SSSv096.py
953:def backtest_unified(
I see backtest_unified is in SSSv096.py. I'll check how it returns metrics.

return np.nan

    holding_periods = []
    entry_date = None

    for _, row in trade_df.iterrows():
        if row['type'] == 'buy':
            entry_date = row['trade_date']
        elif row['type'] == 'sell' and entry_date is not None:
            exit_date = row['trade_date']
            holding_days = (exit_date - entry_date).days
            holding_periods.append(holding_days)
            entry_date = None  # é‡ç½®

    return np.mean(holding_periods) if holding_periods else np.nan


def backtest_unified(
    df_ind: pd.DataFrame,
    strategy_type: str,
    params: Dict,
    buy_dates: Optional[List[pd.Timestamp]] = None,
    sell_dates: Optional[List[pd.Timestamp]] = None,
    discount: float = 0.30,
    trade_cooldown_bars: int = 3,
    bad_holding: bool = False,
    use_leverage: bool = False,
    lev_params: Optional[Dict] = None
) -> Dict:
    if not isinstance(df_ind, pd.DataFrame):
        logger.error(f"df_ind å¿…é ˆæ˜¯ä¸€å€‹ pandas.DataFrameï¼Œå»å‚³å…¥ {type(df_ind)}")
        return {'trades': [], 'trade_df': pd.DataFrame(), 'trades_df': pd.DataFrame(), 'signals_df': pd.DataFrame(), 'metrics':
[], 'equity_curve': pd.Series()}

    # è™•ç† ensemble ç­–ç•¥é¡å‹
    elif strategy_type == "ensemble":
        import traceback
        from contextlib import nullcontext

        status_ctx = st.status("åŸ·è¡Œ Ensemble å›æ¸¬ä¸­â€¦", state="running") if hasattr(st, "status") else nullcontext()
        try:
            with status_ctx:
                # å¾ Streamlit session_state å–å¾— ensemble åƒæ•¸
                method = st.session_state.get('ensemble_method', 'majority')
                floor  = st.session_state.get('ensemble_floor', 0.2)
                ema    = st.session_state.get('ensemble_ema', 3)
                delta  = st.session_state.get('ensemble_delta', 0.3)

                # ---- CostParamsï¼šUI ä»¥ bp è¨­å®šï¼Œå…§éƒ¨è½‰æˆ rate ----
                buy_fee_bp  = float(st.session_state.get('buy_fee_bp',  4.27))
                sell_fee_bp = float(st.session_state.get('sell_fee_bp', 4.27))
                sell_tax_bp = float(st.session_state.get('sell_tax_bp', 30.0))
                slip_bp     = float(st.session_state.get('slippage_bp', 0.0))

                # åœ¨ ensemble åˆ†æ”¯è£¡é¢å…ˆæ±ºå®š ticker_name
                ticker_name = (
                    params.get('ticker')
                    or (getattr(df_ind, 'name', None) if hasattr(df_ind, 'name') else None)
                    or 'UNKNOWN'
                )

                cost = CostParams(
                    buy_fee_bp   = buy_fee_bp,
The function seems to lose metrics when the risk valve is applied. I'll check the earlier code around lines 1115-1175 to understand how the result dictionary is constructed.

root@5583bff288d6:/workspace/SSS096# sed -n '1115,1175p' app_dash.py
                    else:
                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": "N/A",
                            "atr_ratio": "N/A"
                        }

                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
                    'trade_ledger': pack_df(backtest_result.ledger),
                    'daily_state_std': pack_df(backtest_result.daily_state),
                    'trade_ledger_std': pack_df(backtest_result.ledger)
                }

                logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: æ¬Šç›Šæ›²ç·šé•·åº¦={len(backtest_result.equity_curve)}, äº¤æ˜“æ•¸={len(backtest_result
.ledger) if backtest_result.ledger is not None and not backtest_result.ledger.empty else 0}")

            except Exception as e:
                logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
                # å‰µå»ºç©ºçš„çµæœ
                result = {
                    'trades': [],
                    'trade_df': pd.DataFrame(),
                    'trades_df': pd.DataFrame(),
                    'signals_df': pd.DataFrame(),
                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_rat
io': 0.0, 'num_trades': 0},
                    'equity_curve': pd.Series(1.0, index=df_raw.index)
                }

            # === ä¿®å¾© 3ï¼šæ·»åŠ èª¿è©¦æ—¥èªŒï¼Œæ ¸å°å­ç­–ç•¥é›†åˆæ˜¯å¦ä¸€è‡´ ===
            logger.info(f"[Ensemble] åŸ·è¡Œå®Œæˆï¼Œticker={ticker}, method={flat_params.get('method')}")
            if 'equity_curve' in result and hasattr(result['equity_curve'], 'shape'):
                logger.info(f"[Ensemble] æ¬Šç›Šæ›²ç·šé•·åº¦: {len(result['equity_curve'])}")
            if 'trade_df' in result and hasattr(result['trade_df'], 'shape'):
                logger.info(f"[Ensemble] äº¤æ˜“è¨˜éŒ„æ•¸é‡: {len(result['trade_df'])}")
        else:
            if strat_type == 'single':
                df_ind = compute_single(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen
"], strat_params["devwin"], smaa_source=smaa_src)
            elif strat_type == 'dual':
                df_ind = compute_dual(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen"]
, strat_params["short_win"], strat_params["long_win"], smaa_source=smaa_src)
            elif strat_type == 'RMA':
                df_ind = compute_RMA(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen"],
 strat_params["rma_len"], strat_params["dev_len"], smaa_source=smaa_src)
            if df_ind.empty:
                continue
            result = backtest_unified(df_ind, strat_type, strat_params, discount=discount, trade_cooldown_bars=cooldown, bad_hol
ding=bad_holding)

            # ç‚ºå…¶ä»–ç­–ç•¥é¡å‹æ·»åŠ  valve æ¨™è¨˜
            if global_apply:
                result['valve'] = {
                    "applied": False,  # å…¶ä»–ç­–ç•¥é¡å‹æš«æ™‚ä¸æ”¯æ´é¢¨éšªé–¥é–€
I see the code applies the risk valve before constructing the result dictionary, keeping the original stats intact. Maybe the backtest_result.stats is None, which could explain the issue with metrics later.

Iâ€™ll inspect the stats property in run_ensemble_backtest to check for serialization issues.

root@5583bff288d6:/workspace/SSS096# rg "class BacktestResult" -n
sss_core/schemas.py
11:class BacktestResult:
I'll open the file now to check its contents.

# sss_core/schemas.py
from __future__ import annotations
from dataclasses import dataclass
import pandas as pd
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

@dataclass
class BacktestResult:
    """çµ±ä¸€çš„å›æ¸¬è¼¸å‡ºçµæ§‹ï¼Œä¾›å–®ä¸€ç­–ç•¥èˆ‡ Ensemble éƒ½éµå®ˆ"""
    # æ›²ç·šé¡
    equity_curve: pd.Series           # æ¯æ—¥æ¬Šç›Šï¼ˆå¿…å¡«ï¼‰
    trades: pd.DataFrame              # æ¨™æº–æ¬„ä½ï¼štrade_date, type, price, delta_units, w_before, w_after, ...
    stats: Dict[str, Any]            # total_return, annual_return, max_drawdown, ...

    # å¯é¸æ¬„ä½
    daily_state: Optional[pd.DataFrame] = None  # å« equity/cash/w ç­‰ï¼ˆå¯é¸ï¼Œä½†å»ºè­°æä¾›ï¼‰
    ledger: Optional[pd.DataFrame] = None       # è‹¥æœ‰æ›´ç´°çš„æµæ°´å¸³ï¼ˆå¯é¸ï¼‰
    price_series: Optional[pd.Series] = None    # åƒ¹æ ¼åºåˆ—
    weight_curve: Optional[pd.Series] = None    # æ¬Šé‡åºåˆ—
    cash_curve: Optional[pd.Series] = None      # ç¾é‡‘åºåˆ—

    def __post_init__(self):
        """é©—è­‰å¿…è¦æ¬„ä½"""
        if self.equity_curve is None or len(self.equity_curve) == 0:
            raise ValueError("equity_curve æ˜¯å¿…å¡«æ¬„ä½ä¸”ä¸èƒ½ç‚ºç©º")
        if self.trades is None or len(self.trades) == 0:
            raise ValueError("trades æ˜¯å¿…å¡«æ¬„ä½ä¸”ä¸èƒ½ç‚ºç©º")
        if self.stats is None:
            raise ValueError("stats æ˜¯å¿…å¡«æ¬„ä½")

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼åºåˆ—åŒ–"""
        result = {
            'equity_curve': self.equity_curve,
            'trades': self.trades,
            'stats': self.stats
        }

        # æ·»åŠ å¯é¸æ¬„ä½
        if self.daily_state is not None:
            result['daily_state'] = self.daily_state
        if self.ledger is not None:
            result['ledger'] = self.ledger
        if self.price_series is not None:
            result['price_series'] = self.price_series
        if self.weight_curve is not None:
            result['weight_curve'] = self.weight_curve
        if self.cash_curve is not None:
            result['cash_curve'] = self.cash_curve

        return result


def pack_df(df: pd.DataFrame) -> str:
    """å°‡ DataFrame åºåˆ—åŒ–ç‚º JSON å­—ä¸²ï¼Œä½¿ç”¨ orient="split" + date_format="iso"

    Args:
        df: è¦åºåˆ—åŒ–çš„ DataFrame

    Returns:
        JSON å­—ä¸²ï¼Œç©º DataFrame å›å‚³ç©ºå­—ä¸²
    """
    if df is None or len(df) == 0:
        return ""
    return df.to_json(orient="split", date_format="iso")


def pack_series(s: pd.Series) -> str:
    """å°‡ Series åºåˆ—åŒ–ç‚º JSON å­—ä¸²ï¼Œä½¿ç”¨ orient="split" + date_format="iso"

    Args:
        s: è¦åºåˆ—åŒ–çš„ Series

    Returns:
        JSON å­—ä¸²ï¼Œç©º Series å›å‚³ç©ºå­—ä¸²
    """
    if s is None or len(s) == 0:
        return ""
    return s.to_json(orient="split", date_format="iso")
It looks like after applying the risk valve, the metrics dictionary gets wiped. I think the solution might be to recompute the metrics using compute_backtest_metrics or similar, ensuring backtest_result.stats stays intact. Also, _pack_result_for_store isn't processing metrics, so the dictionary should remain as is.

Metrics might disappear after risk valve, possibly due to overwriting or not repacking them after updates.

The issue might be from not updating backtest_result.stats after risk valve modifications, causing empty metrics.

The issue could be that backtest_result.stats is None, affecting metrics. I'll check ensemble_runner.py to confirm.

app_dash.py
955:                from runners.ensemble_runner import run_ensemble_backtest
1071:                backtest_result = run_ensemble_backtest(cfg)

runners/__init__.py
2:from .ensemble_runner import run_ensemble_backtest
4:__all__ = ['run_ensemble_backtest']

runners/ensemble_runner.py
13:def run_ensemble_backtest(cfg) -> BacktestResult:
root@5583bff288d6:/workspace/SSS096# sed -n '1,160p' runners/ensemble_runner.py
# runners/ensemble_runner.py
from __future__ import annotations
import logging
from typing import Optional
import pandas as pd

from sss_core.schemas import BacktestResult
from sss_core.normalize import normalize_trades_for_ui, normalize_daily_state

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

def run_ensemble_backtest(cfg) -> BacktestResult:
    """åŸ·è¡Œ Ensemble ç­–ç•¥ä¸¦å›å‚³æ¨™æº–åŒ–çš„ BacktestResult"""
    try:
        # å°å…¥ Ensemble ç›¸é—œæ¨¡çµ„
        from SSS_EnsembleTab import run_ensemble, EnsembleParams, CostParams, RunConfig

        # é‹è¡Œ ensemble ç­–ç•¥
        open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

        # æ¨™æº–åŒ–è³‡æ–™
        trades_ui = normalize_trades_for_ui(trades)
        daily_state_std = normalize_daily_state(daily_state)

        # æ§‹å»º BacktestResult
        result = BacktestResult(
            equity_curve=equity,
            daily_state=daily_state_std,
            trades=trades_ui,
            ledger=trade_ledger,
            stats=stats,
            price_series=open_px,
            weight_curve=w,
            cash_curve=daily_state_std['cash'] if daily_state_std is not None and 'cash' in daily_state_std.columns else None
        )

        logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: {method_name}, æ¬Šç›Šæ›²ç·šé•·åº¦={len(equity)}, äº¤æ˜“æ•¸={len(trade_ledger) if trade_ledger
is not None and not trade_ledger.empty else 0}")

        return result

    except Exception as e:
        logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
        # å‰µå»ºç©ºçš„çµæœ
        empty_df = pd.DataFrame()
        empty_series = pd.Series(dtype=float)

        return BacktestResult(
            equity_curve=empty_series,
            daily_state=empty_df,
            trades=empty_df,
            ledger=empty_df,
            stats={
                'total_return': 0.0,
                'annual_return': 0.0,
                'max_drawdown': 0.0,
                'sharpe_ratio': 0.0,
                'calmar_ratio': 0.0,
                'num_trades': 0
            }
        )
It seems metrics might not be included in the keys list for _pack_result_for_store.

root@5583bff288d6:/workspace/SSS096# rg "_unpack" -n app_dash.py
I'll look up df_from_pack to understand how the metrics are being retrieved.

root@5583bff288d6:/workspace/SSS096# rg "df_from_pack" -n
app_dash.py
141:def df_from_pack(data):
899:                        ds_std = df_from_pack(result.get('daily_state_std'))
905:                        ds = df_from_pack(result.get('daily_state'))
1209:            ds = df_from_pack(ds_raw)
1365:    df_raw = df_from_pack(data['df_raw'])  # ä½¿ç”¨ df_from_pack çµ±ä¸€è§£åŒ…
1414:                df = df_from_pack(cand) if isinstance(cand, str) else cand
1424:            daily_state_std = df_from_pack(result.get('daily_state_std'))
1426:                daily_state_std = df_from_pack(result.get('daily_state'))
1430:            trade_ledger_std = df_from_pack(result.get('trade_ledger_std'))
1432:                trade_ledger_std = df_from_pack(result.get('trade_ledger'))
1492:            ledger_df = df_from_pack(result.get('trade_ledger'))
1597:            daily_state = df_from_pack(result.get('daily_state'))
1799:            trade_df = df_from_pack(result.get('trade_df'))
1851:            trade_df = df_from_pack(result.get('trade_df'))
2114:    trade_df = df_from_pack(result.get('trade_df'))
2167:    trade_df = df_from_pack(result.get('trade_df'))
2386:    df_raw = df_from_pack(cache.get("df_raw"))
2387:    daily_state = df_from_pack(cache.get("daily_state"))
2514:    trade_df = df_from_pack(cache.get("trade_data"))
2515:    daily_state = df_from_pack(cache.get("daily_state"))
2795:    trade_df = df_from_pack(cache.get("trade_data"))
2796:    daily_state = df_from_pack(cache.get("daily_state"))
3102:        trade_data = df_from_pack(result["trade_ledger_std"])
3105:        trade_data = df_from_pack(result["trade_ledger"])
3108:        trade_data = df_from_pack(result["trade_df"])
3153:        daily_state = df_from_pack(result["daily_state_std"])
3155:        daily_state = df_from_pack(result["daily_state"])
3234:        trade_data = df_from_pack(best_result["trade_ledger_std"])
3237:        trade_data = df_from_pack(best_result["trade_ledger"])
3240:        trade_data = df_from_pack(best_result["trade_df"])
3285:        daily_state = df_from_pack(best_result["daily_state_std"])
3287:        daily_state = df_from_pack(best_result["daily_state"])
3340:        df_raw = df_from_pack(cache.get("df_raw"))
3341:        daily_state = df_from_pack(cache.get("daily_state"))
3364:        daily_state = df_from_pack(result.get("daily_state") or result.get("daily_state_std"))
3365:        df_raw = df_from_pack(backtest_data.get("df_raw"))
3776:            df_raw = df_from_pack(cache.get("df_raw"))
3777:            daily_state = df_from_pack(cache.get("daily_state"))
3791:            daily_state = df_from_pack(result.get("daily_state") or result.get("daily_state_std"))
3792:            df_raw = df_from_pack(backtest_data.get("df_raw"))

analysis/UI_INTEGRATION_GUIDE.md
63:åƒè€ƒ `df_from_pack` èˆ‡ `format_trade_like_df_for_display` çš„æ¬„ä½æ˜ å°„é‚è¼¯ï¼š
I'm wondering if the risk valve code affects backtest_result.stats or resets it later.

...
2621:        kpi("å¹³å‡æ¯æ®µ MDD(%)", f"{avg_mdd:.2f}" if pd.notna(avg_mdd) else "â€”"),
2622:        kpi("æˆåŠŸç‡(å…¨éƒ¨)", f"{succ_all*100:.1f}%" if pd.notna(succ_all) else "â€”"),
2623:        kpi("æˆåŠŸç‡(åŠ ç¢¼)", f"{succ_acc*100:.1f}%" if pd.notna(succ_acc) else "â€”"),
2624:        kpi("é¢¨éšªæ•ˆç‡", f"{risk_eff:.3f}" if pd.notna(risk_eff) else "â€”"),
2625:    ], style={"display":"flex","gap":"12px","flexWrap":"wrap","marginBottom":"10px"})
2627:    # ====== åˆ†çµ„ KPIï¼šåŠ ç¢¼ vs æ¸›ç¢¼ ======
2628:    def _group_metrics(mask):
2629:        if {"éšæ®µæ·¨è²¢ç»(%)","éšæ®µå…§MDD(%)"}.issubset(table.columns):
2630:            sub = table.loc[mask]
2631:            if sub.empty:
2632:                return None
2633:            a_net = sub["éšæ®µæ·¨è²¢ç»(%)"].mean()
2634:            a_mdd = sub["éšæ®µå…§MDD(%)"].mean()
2635:            succ  = (sub["éšæ®µæ·¨è²¢ç»(%)"] > 0).mean()
2636:            eff   = (a_net / abs(a_mdd)) if pd.notna(a_net) and pd.notna(a_mdd) and a_mdd != 0 else np.nan
2637:            return {"count": int(len(sub)), "avg_net": a_net, "avg_mdd": a_mdd, "succ": succ, "eff": eff}
2638:        return None
2640:    def _fmt(val, pct=False, dec=2):
2641:        if val is None or (isinstance(val, float) and (np.isnan(val) or np.isinf(val))):
2642:            return "â€”"
2643:        return f"{val*100:.1f}%" if pct else f"{val:.{dec}f}"
2645:    def group_row(title, m):
2646:        return html.Div([
2647:            html.Div(title, style={"fontWeight":"bold","marginRight":"12px","minWidth":"72px","alignSelf":"center"}),
2648:            kpi("æ®µæ•¸", f"{m['count']}" if m else "â€”"),
2649:            kpi("å¹³å‡æ·¨è²¢ç»(%)", _fmt(m['avg_net']) if m else "â€”"),
2650:            kpi("å¹³å‡MDD(%)",   _fmt(m['avg_mdd']) if m else "â€”"),
2651:            kpi("æˆåŠŸç‡",        _fmt(m['succ'], pct=True) if m else "â€”"),
2652:            kpi("é¢¨éšªæ•ˆç‡",      _fmt(m['eff'],  dec=3) if m else "â€”"),
2653:        ], style={"display":"flex","gap":"10px","flexWrap":"wrap","marginBottom":"8px"})
2655:    acc_metrics = dis_metrics = None
2656:    if "éšæ®µ" in table.columns:
2657:        mask_acc = table["éšæ®µ"].astype(str).str.contains("åŠ ç¢¼", na=False)
2658:        mask_dis = table["éšæ®µ"].astype(str).str.contains("æ¸›ç¢¼", na=False)
2659:        acc_metrics = _group_metrics(mask_acc)
2660:        dis_metrics = _group_metrics(mask_dis)
2662:    group_section = html.Div([
2663:        html.H6("åˆ†çµ„ KPIï¼ˆåŠ ç¢¼ vs æ¸›ç¢¼ï¼‰", style={"margin":"8px 0 6px 0"}),
2664:        group_row("åŠ ç¢¼æ®µ", acc_metrics),
2665:        group_row("æ¸›ç¢¼æ®µ", dis_metrics),
2666:    ], style={"marginTop":"4px"})
2668:    # ====== Top/Worst ä¾†æºåˆ‡æ›ï¼ˆå…¨éƒ¨ / åªåŠ ç¢¼ / åªæ¸›ç¢¼ï¼‰ ======
2669:    source_selector = html.Div([
2670:        html.Div("Top/Worst ä¾†æº", style={"marginRight":"8px", "alignSelf":"center"}),
2671:        dcc.RadioItems(
2672:            id="phase-source",
2673:            options=[
2674:                {"label": "å…¨éƒ¨",   "value": "all"},
2675:                {"label": "åŠ ç¢¼æ®µ", "value": "acc"},
2676:                {"label": "æ¸›ç¢¼æ®µ", "value": "dis"},
2678:            value="all",
2679:            inline=True,
2680:            inputStyle={"marginRight":"4px"},
2681:            labelStyle={"marginRight":"12px"}
2683:    ], style={"display":"flex","gap":"6px","alignItems":"center","margin":"6px 0 8px 0"})
2685:    # æ¬„ä½é †åºï¼ˆå®Œæ•´è¡¨ & Top/Worst å…±ç”¨ï¼‰
2686:    ordered = [c for c in ["éšæ®µ","é–‹å§‹æ—¥æœŸ","çµæŸæ—¥æœŸ","äº¤æ˜“ç­†æ•¸",
2687:                           "éšæ®µæ·¨è²¢ç»(%)","è³£å‡ºå ±é…¬ç¸½å’Œ(%)","éšæ®µå…§MDD(%)","æ˜¯å¦æˆåŠŸ"] if c in table.columns]
2688:    basis_col = "éšæ®µæ·¨è²¢ç»(%)" if "éšæ®µæ·¨è²¢ç»(%)" in table.columns else "è³£å‡ºå ±é…¬ç¸½å’Œ(%)"
2691:    full_table = dash_table.DataTable(
2692:        id="phase-datatable",
2693:        columns=[{"name": c, "id": c, "type": ("numeric" if c in num_cols else "text")} for c in ordered],
2694:        data=table[ordered].to_dict("records"),
2696:        page_action="native",
2697:        page_current=0,
2698:        page_size=100,            # é è¨­æ¯é  100ï¼Œè‹¥è¦æ”¹å¯åœ¨é€™è£¡
2700:        sort_action="native",
2701:        filter_action="native",
2703:        export_format="csv",
2704:        export_headers="display",
2706:        cell_selectable=True,
2707:        virtualization=False,     # é—œé–‰è™›æ“¬åŒ–ï¼Œé¿å…è¤‡è£½æ™‚åªè¤‡åˆ°å¯è¦–å€
2708:        fixed_rows={"headers": True},
2709:        style_table=style_table,
2710:        style_cell=style_cell,
2711:        style_header=style_header,
2712:        css=[{
2713:            "selector": ".dash-table-container .dash-spreadsheet-container .dash-spreadsheet-inner *",
2714:            "rule": "user-select: text; -webkit-user-select: text; -moz-user-select: text; -ms-user-select: text;"
2718:    # ====== dcc.Storeï¼šæä¾› Top/Worst å‹•æ…‹ callback ä½¿ç”¨ ======
2719:    store = dcc.Store(id="phase-table-store", data={
2720:        "records": table[ordered].to_dict("records"),
2721:        "ordered": ordered,
2722:        "basis": basis_col,
2723:        "has_stage": "éšæ®µ" in table.columns
2727:    def _subset(src):
2728:        df = table
2729:        if "éšæ®µ" not in df.columns:
2730:            return df
2731:        if src == "acc":
2732:            return df[df["éšæ®µ"].astype(str).str.contains("åŠ ç¢¼", na=False)]
2733:        if src == "dis":
2734:            return df[df["éšæ®µ"].astype(str).str.contains("æ¸›ç¢¼", na=False)]
2735:        return df
2736:    base = _subset("all")
2737:    top3   = base.nlargest(3, basis_col) if basis_col in base else base.head(3)
2738:    worst3 = base.nsmallest(3, basis_col) if basis_col in base else base.tail(3)
2740:    def simple_table(df, tbl_id):
2741:        return dash_table.DataTable(
2742:            id=tbl_id,
2743:            columns=[{"name": c, "id": c} for c in ordered],
2744:            data=df[ordered].to_dict("records"),
2745:            page_action="none",
2746:            style_table=style_table, style_cell=style_cell, style_header=style_header
2749:    top3_table = simple_table(top3, "phase-top-table")
2750:    worst3_table = simple_table(worst3, "phase-worst-table")
2753:    tools = html.Div([
2754:        html.Button("è¤‡è£½å…¨éƒ¨ï¼ˆCSVï¼‰", id="phase-copy-btn",
2755:                    style={"padding": "6px 10px", "borderRadius": "8px", "cursor": "pointer"}),
2756:        dcc.Clipboard(target_id="phase-csv-text", title="Copy", style={"marginLeft": "6px"}),
2757:        html.A("ä¸‹è¼‰ CSV", href=csv_data_url, download="trade_contribution.csv",
2758:               style={"marginLeft": "12px", "textDecoration": "none"})
2759:    ], style={"display": "flex", "alignItems": "center", "gap": "4px", "marginBottom": "8px"})
2761:    # éš±è—çš„ CSV æ–‡å­—ä¾†æºï¼ˆçµ¦ Clipboard ç”¨ï¼‰
2762:    csv_hidden = html.Pre(id="phase-csv-text", children=csv_text, style={"display": "none"})
2764:    # ====== Tabsï¼šæ˜“è®€ç‰ˆ / å®Œæ•´è¡¨æ ¼ ======
2765:    tabs = dcc.Tabs(id="phase-tabs", value="summary", children=[
2766:        dcc.Tab(label="æ˜“è®€ç‰ˆ", value="summary", children=[
2767:            kpi_bar,
2768:            group_section,
2769:            source_selector,
2770:            html.H6("æœ€è³ºçš„ 3 æ®µï¼ˆä¾ä¾†æºèˆ‡æ’åºæ¬„ï¼‰", style={"marginTop":"8px"}),
2771:            top3_table,
2772:            html.H6("æœ€è™§çš„ 3 æ®µï¼ˆä¾ä¾†æºèˆ‡æ’åºæ¬„ï¼‰", style={"marginTop":"16px"}),
...
3795:            return None
3798:        if df_raw is None or df_raw.empty or daily_state is None or daily_state.empty:
3799:            return None
3802:        c_open = "open" if "open" in df_raw.columns else _first_col(df_raw, ["Open","é–‹ç›¤åƒ¹"])
3803:        c_close = "close" if "close" in df_raw.columns else _first_col(df_raw, ["Close","æ”¶ç›¤åƒ¹"])
3804:        c_high  = "high" if "high" in df_raw.columns else _first_col(df_raw, ["High","æœ€é«˜åƒ¹"])
3805:        c_low   = "low"  if "low"  in df_raw.columns else _first_col(df_raw, ["Low","æœ€ä½åƒ¹"])
3807:        if c_open is None or c_close is None:
3808:            return None
3811:        open_px = pd.to_numeric(df_raw[c_open], errors="coerce").dropna()
3812:        open_px.index = pd.to_datetime(df_raw.index)
3813:        if "w" not in daily_state.columns:
3814:            return None
3816:        w = daily_state["w"].astype(float).reindex(open_px.index).ffill().fillna(0.0)
3818:        bench = pd.DataFrame({
3819:            "æ”¶ç›¤åƒ¹": pd.to_numeric(df_raw[c_close], errors="coerce"),
3820:        }, index=pd.to_datetime(df_raw.index))
3821:        if c_high and c_low:
3822:            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw[c_high], errors="coerce")
3823:            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw[c_low], errors="coerce")
3825:        # ä½¿ç”¨èˆ‡ generate_pareto_map ç›¸åŒçš„åƒæ•¸ç¯„åœå’Œé‚è¼¯
3826:        logger.info("=== é–‹å§‹ç”Ÿæˆ Pareto Map æ•¸æ“šç”¨æ–¼ CSV ä¸‹è¼‰ ===")
3827:        caps = np.round(np.arange(0.10, 1.00 + 1e-9, 0.05), 2)
3828:        atr_mults = np.round(np.arange(1.00, 2.00 + 1e-9, 0.05), 2)
3829:        logger.info(f"cap ç¯„åœ: {len(caps)} å€‹å€¼ï¼Œå¾ {caps[0]} åˆ° {caps[-1]}")
3830:        logger.info(f"ATR æ¯”å€¼ç¯„åœ: {len(atr_mults)} å€‹å€¼ï¼Œå¾ {atr_mults[0]} åˆ° {atr_mults[-1]}")
3831:        logger.info(f"ç¸½çµ„åˆæ•¸: {len(caps) * len(atr_mults)}")
3833:        pareto_data = []
3834:        tried = 0
3835:        succeeded = 0
3837:        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3838:        try:
3839:            from SSS_EnsembleTab import risk_valve_backtest
3840:            logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3841:        except Exception as e:
3842:            logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3843:            return None
3845:        logger.info("é–‹å§‹åŸ·è¡Œåƒæ•¸æƒæ...")
3846:        for cap_level in caps:
3847:            for atr_mult in atr_mults:
3848:                tried += 1
3849:                if tried % 50 == 0:  # æ¯50æ¬¡è¨˜éŒ„ä¸€æ¬¡é€²åº¦
3850:                    logger.info(f"é€²åº¦: {tried}/{len(caps) * len(atr_mults)} (cap={cap_level:.2f}, atr={atr_mult:.2f})")
3852:                try:
3853:                    out = risk_valve_backtest(
3854:                        open_px=open_px, w=w, cost=None, benchmark_df=bench,
3855:                        mode=(rv_mode or "cap"), cap_level=float(cap_level),
3856:                        slope20_thresh=0.0, slope60_thresh=0.0,
3857:                        atr_win=20, atr_ref_win=60, atr_ratio_mult=float(atr_mult)
3860:                    if out and "metrics" in out:
3861:                        m = out["metrics"]
3863:                        # è¨ˆç®— Pareto æŒ‡æ¨™
3864:                        equity_curve = out.get("daily_state_valve", {}).get("equity")
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)
3868:                        if metrics:
3869:                            pareto_data.append({
3870:                                'strategy': f'cap_{cap_level:.2f}_atr_{atr_mult:.2f}',
3871:                                'cap': cap_level,
3872:                                'atr_ratio': atr_mult,
3873:                                'max_drawdown': metrics['max_drawdown'],
3874:                                'pf': metrics['pf'],
3875:                                'right_tail_loss': metrics['right_tail_loss'],
3876:                                'risk_trigger_days': metrics['risk_trigger_days'],
3877:                                'pf_orig': m.get('pf_orig', 0.0),
3878:                                'pf_valve': m.get('pf_valve', 0.0),
3879:                                'mdd_orig': m.get('mdd_orig', 0.0),
3880:                                'mdd_valve': m.get('mdd_valve', 0.0),
3881:                                'right_tail_reduction': m.get('right_tail_reduction', 0.0)
3883:                            succeeded += 1
3885:                except Exception as e:
3886:                    logger.debug(f"åƒæ•¸çµ„åˆ cap={cap_level}, atr={atr_mult} è¨ˆç®—å¤±æ•—: {e}")
3887:                    continue
3889:        if not pareto_data:
3890:            logger.warning("æ²’æœ‰ç”Ÿæˆä»»ä½• Pareto æ•¸æ“š")
3891:            return None
3893:        logger.info(f"æˆåŠŸç”Ÿæˆ {succeeded} çµ„ Pareto æ•¸æ“š")
3895:        # è½‰æ›ç‚º DataFrame ä¸¦æº–å‚™ä¸‹è¼‰
3896:        df_pareto = pd.DataFrame(pareto_data)
3899:        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
3900:        filename = f"pareto_map_data_{timestamp}.csv"
3903:        return dcc.send_data_frame(
3904:            df_pareto.to_csv,
3905:            filename,
3906:            index=False,
3907:            encoding='utf-8-sig'  # æ”¯æ´ä¸­æ–‡
3910:    except Exception as e:
3911:        logger.exception(f"ä¸‹è¼‰ Pareto Map CSV å¤±æ•—ï¼š{e}")
3912:        return None
3914:# --------- æ–°å¢ï¼šå‹•æ…‹åˆ‡æ› Top/Worst çš„ callback ---------
3915:@app.callback(
3916:    [dash.Output("phase-top-table", "data"),
3917:     dash.Output("phase-worst-table", "data")],
3918:    [dash.Input("phase-source", "value"),
3919:     dash.Input("phase-table-store", "data")]
3921:def _update_top_worst(src, store):
3922:    if not store:
3923:        raise dash.exceptions.PreventUpdate
3924:    import pandas as pd
3925:    records = store.get("records", [])
3926:    ordered = store.get("ordered", [])
3927:    basis   = store.get("basis", None)
3928:    has_stage = store.get("has_stage", False)
3930:    df = pd.DataFrame(records)
3932:    if has_stage and "éšæ®µ" in df.columns:
3933:        if src == "acc":
3934:            df = df[df["éšæ®µ"].astype(str).str.contains("åŠ ç¢¼", na=False)]
3935:        elif src == "dis":
3936:            df = df[df["éšæ®µ"].astype(str).str.contains("æ¸›ç¢¼", na=False)]
3938:    if basis and basis in df.columns and not df.empty:
3939:        top3   = df.nlargest(3, basis)
3940:        worst3 = df.nsmallest(3, basis)
3941:    else:
3942:        top3   = df.head(3)
3943:        worst3 = df.tail(3)
3945:    return top3[ordered].to_dict("records"), worst3[ordered].to_dict("records")
3947:if __name__ == '__main__':
3949:    safe_startup()
3952:    app.run_server(
3953:        debug=True,
3954:        host='127.0.0.1',
3955:        port=8050,
3956:        threaded=True,
3957:        use_reloader=False  # é¿å…é‡è¼‰å™¨é€ æˆçš„ç·šç¨‹å•é¡Œ
I'm looking into why metrics disappear after applying the risk valve. It might be related to JSON serialization issues with numpy objects or the structure of backtest_result.stats after the risk valve is applied.

Looks like metrics might be missing after the risk valve application. Could be a type issue or result not returning correctly.

The metrics problem likely arises when result is reset, potentially due to improper handling after risk valve is applied. I'll check run_backtest in detail.

I need to verify how stats is handled in run_ensemble to see if conversion is missing.

root@5583bff288d6:/workspace/SSS096# rg "def run_ensemble(" -n SSS_EnsembleTab.py
rg: regex parse error:
    (?:def run_ensemble()
    ^
error: unclosed group
The error is due to parentheses, so I need to escape them properly.

root@5583bff288d6:/workspace/SSS096# rg "def run_ensemble" -n SSS_EnsembleTab.py
749:def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame
, pd.DataFrame]:
root@5583bff288d6:/workspace/SSS096# sed -n '749,830p' SSS_EnsembleTab.py
def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame, pd
.DataFrame]:
    """å›å‚³ï¼š(open åƒ¹), (æ¯æ—¥æ¬Šé‡ w), (äº¤æ˜“ç´€éŒ„ trades), (ç¸¾æ•ˆæŒ‡æ¨™ dict), (æ–¹æ³•åç¨±), (æ¬Šç›Šæ›²ç·š), (æ¯æ—¥ç‹€æ…‹ daily_state), (äº¤æ˜“æµ
æ°´å¸³ trade_ledger)"""
    # è®€åƒ¹ï¼ˆOpenï¼‰
    px_path = DATA_DIR / f"{cfg.ticker.replace(':','_')}_data_raw.csv"
    px = _read_market_csv_auto(px_path)

    # èª¿è©¦ä¿¡æ¯ï¼šåƒ¹æ ¼æ•¸æ“š
    logger.info(f"[Ensemble] åƒ¹è³‡æ–™å¤©æ•¸={len(px)}, é¦–æœ«={px.index.min()}~{px.index.max()}")

    # æ¨æ–·ç­–ç•¥åˆ—è¡¨
    if cfg.strategies is None:
        # å„ªå…ˆä½¿ç”¨ trades_from_results_*.csvï¼ˆ120æª”ç­–ç•¥ï¼‰
        strat_names = []
        file_map = {}  # ç­–ç•¥å -> æ–‡ä»¶è·¯å¾‘çš„æ˜ å°„

        # å…ˆæ‰¾ trades_from_results_*.csv
        trades_files = list(OUT_DIR.glob("trades_from_results_*.csv"))
        if trades_files:
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_from_results_*.csv æ–‡ä»¶ï¼ˆ120æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_from_results_", "")
                strat_names.append(name)
                file_map[name] = f
        else:
            # æ‰¾ä¸åˆ°å†ä½¿ç”¨èˆŠçš„ trades_*.csvï¼ˆ11æª”ç­–ç•¥ï¼‰
            trades_files = list(OUT_DIR.glob("trades_*.csv"))
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_*.csv æ–‡ä»¶ï¼ˆ11æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_", "")
                strat_names.append(name)
                file_map[name] = f

        if not strat_names:
            raise ValueError(f"åœ¨ {OUT_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½•äº¤æ˜“æ–‡ä»¶")

        cfg.strategies = strat_names
        cfg.file_map = file_map

    # è¼‰å…¥æŒå€‰çŸ©é™£
    pos_df = load_positions_matrix(OUT_DIR, px.index, cfg.strategies, cfg.file_map)

    if pos_df.empty:
        raise ValueError("ç„¡æ³•è¼‰å…¥ä»»ä½•ç­–ç•¥çš„æŒå€‰æ•¸æ“š")

    N = pos_df.shape[1]  # ç­–ç•¥æ•¸é‡
    logger.info(f"[Ensemble] è¼‰å…¥ {N} å€‹ç­–ç•¥çš„æŒå€‰æ•¸æ“š")

    # è™•ç† majority_k åƒæ•¸
    if cfg.method.lower() == "majority":
        if cfg.majority_k_pct is not None:
            # ä½¿ç”¨æ¯”ä¾‹é–€æª»
            k_req = int(math.ceil(N * cfg.majority_k_pct))
            logger.info(f"[Ensemble] ä½¿ç”¨æ¯”ä¾‹é–€æª» majority_k_pct={cfg.majority_k_pct}, N={N}, è¨ˆç®—å¾—åˆ° K={k_req}")
        else:
            # ä½¿ç”¨å›ºå®š K å€¼
            k_req = cfg.params.majority_k
            logger.info(f"[Ensemble] ä½¿ç”¨å›ºå®šé–€æª» majority_k={k_req}, N={N}")

        # æ–¹æ¡ˆ Aï¼šå¤¾æ“ åˆ°åˆæ³•ç¯„åœ
        k_eff = max(1, min(int(k_req), N))

        # æ–¹æ¡ˆ Bï¼šè‹¥ k>n æ¡ç”¨å‹•æ…‹å¤šæ•¸æ±ºï¼ˆå»ºè­°ï¼‰ï¼šceil(N*0.5)
        if k_req > N:
            logger.warning(f"majority_k({k_req}) > N({N}); fallback to ceil(N/2)={math.ceil(N*0.5)}")
            k_eff = int(math.ceil(N*0.5))

        # ç”¨ dataclasses.replace è¦†å¯«åƒæ•¸å¾Œå†ç®—æ¬Šé‡èˆ‡å‘½å
        import dataclasses
        cfg.params = dataclasses.replace(cfg.params, majority_k=k_eff)

        # æª¢æŸ¥ N æ˜¯å¦è¶³å¤ ï¼ˆå¯é¸ä½†å»ºè­°ï¼‰
        if N < 8:
            logger.warning(f"ç­–ç•¥æ•¸é‡ N={N} < 8ï¼Œå¯èƒ½å½±éŸ¿ ensemble æ•ˆæœã€‚å»ºè­°ç¢ºä¿æœ‰è¶³å¤ çš„å­ç­–ç•¥ã€‚")

    # èª¿è©¦ä¿¡æ¯ï¼šå­ç­–ç•¥å¤šå€‰çµ±è¨ˆ
    S = pos_df.sum(axis=1)
    logger.info(f"[Ensemble] å¤šé ­è¨ˆæ•¸Såˆ†ä½ˆ: mean={S.mean():.2f}, 1%={S.quantile(0.01):.2f}, 99%={S.quantile(0.99):.2f}")

    # æ¬Šé‡
    if cfg.method.lower() == "majority":
The issue might be that stats is a pandas Series, not a dictionary.

After the risk valve, I need to recompute stats using the updated data to avoid empty metrics.

root@5583bff288d6:/workspace/SSS096# rg "stats" -n app_dash.py
1130:                    'metrics': backtest_result.stats,
1876:            detailed_stats = calculate_holding_periods(trade_df)
1888:                'å¹³å‡æŒæœ‰å¤©æ•¸': f"{detailed_stats['avg_holding_days']:.1f}",
1889:                'è³£å¾Œè²·å¹³å‡å¤©æ•¸': f"{detailed_stats['avg_sell_to_buy_days']:.1f}",
1890:                'ç›®å‰ç‹€æ…‹': detailed_stats['current_status'],
1891:                'è·é›¢ä¸Šæ¬¡æ“ä½œå¤©æ•¸': f"{detailed_stats['days_since_last_action']}"
2196:def calculate_strategy_detailed_stats(trade_df, df_raw):
2993:        summary_stats = html.Div([
3008:            summary_stats,
The metrics might not be packed correctly. Let's simulate _pack_result_for_store to check.

I think the risk valve branch doesn't recompute metrics. Let's test by computing them post-valve.

root@5583bff288d6:/workspace/SSS096# rg "calculate_pareto_metrics" -n
app_dash.py
3547:def calculate_pareto_metrics(equity_curve, trade_df):
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)
It's odd that there's no general metrics function. Let's look for calc_all_metrics or similar.

root@5583bff288d6:/workspace/SSS096# rg "metrics" -n sss_core
sss_core/__init__.py
8:    plot_performance_metrics,
22:    'plot_performance_metrics',

sss_core/plotting.py
182:def plot_performance_metrics(stats: dict, title: str = "ç¸¾æ•ˆæŒ‡æ¨™",
186:    key_metrics = ['total_return', 'annual_return', 'max_drawdown', 'sharpe_ratio']
192:    for metric, display_name in zip(key_metrics, display_names):
It looks like the metrics may be overwritten after the risk valve. I'll check _pack_any.

root@5583bff288d6:/workspace/SSS096# rg "def _pack_any" -n app_dash.py
236:def _pack_any(x):
root@5583bff288d6:/workspace/SSS096# sed -n '236,244p' app_dash.py
def _pack_any(x):
    import pandas as pd
    if isinstance(x, pd.DataFrame):
        return pack_df(x)          # orient="split" + date_format="iso"
    if isinstance(x, pd.Series):
        return pack_series(x)      # orient="split" + date_format="iso"
    return x

def _pack_result_for_store(result: dict) -> dict:
The bug may occur when applying the risk valve to all strategies, causing metrics to vanish. The issue could be related to overwriting the result variable or serialization errors when storing numpy values.

I suspect the bug might occur because result is referenced before being defined in the risk valve block. I'll check if result = {} appears earlier in the function.

Looks like result isn't defined earlier in the function, which could be causing the bug. I should check if the code is inside a try block that might catch errors.

root@5583bff288d6:/workspace/SSS096# sed -n '735,820p' app_dash.py
            logger.info("ç­‰å¾…æ•¸æ“šè¼‰å…¥ï¼šæœªé¸æ“‡è‚¡ç¥¨ä»£è™Ÿæˆ–æ—¥æœŸ")
            return html.Div([
                html.Small("ğŸŸ¡ ç­‰å¾…æ•¸æ“šè¼‰å…¥", style={"color":"#ffc107","fontWeight":"bold"}),
                html.Br(),
                html.Small("è«‹é¸æ“‡è‚¡ç¥¨ä»£è™Ÿå’Œæ—¥æœŸ", style={"color":"#666","fontSize":"10px"})
            ])
    except Exception as e:
        logger.error(f"é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–°å¤±æ•—: {e}")
        return html.Div([
            html.Small("ğŸŸ¡ è¨ˆç®—ä¸­...", style={"color":"#ffc107","fontWeight":"bold"}),
            html.Br(),
            html.Small(f"éŒ¯èª¤: {str(e)}", style={"color":"#666","fontSize":"10px"})
        ])

# --------- åŸ·è¡Œå›æ¸¬ä¸¦å­˜åˆ° Store ---------
@app.callback(
    Output('backtest-store', 'data'),
    [
        Input('run-btn', 'n_clicks'),
        Input('auto-run', 'value'),
        Input('ticker-dropdown', 'value'),
        Input('start-date', 'value'),
        Input('end-date', 'value'),
        Input('discount-slider', 'value'),
        Input('cooldown-bars', 'value'),
        Input('bad-holding', 'value'),
        Input('global-apply-switch', 'value'),
        Input('risk-cap-input', 'value'),
        Input('atr-ratio-threshold', 'value'),
        Input('force-valve-trigger', 'value'),
        Input('strategy-dropdown', 'value'),
        Input({'type': 'param-input', 'param': ALL}, 'value'),
        Input({'type': 'param-input', 'param': ALL}, 'id'),
    ],
    State('backtest-store', 'data')
)
def run_backtest(n_clicks, auto_run, ticker, start_date, end_date, discount, cooldown, bad_holding, global_apply, risk_cap, atr_
ratio, force_trigger, strategy, param_values, param_ids, stored_data):
    # === èª¿è©¦æ—¥èªŒï¼ˆåƒ…åœ¨ DEBUG ç´šåˆ¥æ™‚é¡¯ç¤ºï¼‰===
    logger.debug(f"run_backtest è¢«èª¿ç”¨ - n_clicks: {n_clicks}, auto_run: {auto_run}, trigger: {ctx.triggered_id}")

    # ç§»é™¤è‡ªå‹•å¿«å–æ¸…ç†ï¼Œé¿å…å¤šç”¨æˆ·è¡çª
    # è®“ joblib.Memory è‡ªå‹•ç®¡ç†å¿«å–ï¼Œåªåœ¨éœ€è¦æ™‚æ‰‹å‹•æ¸…ç†
    if n_clicks is None and not auto_run:
        logger.debug(f"æ—©æœŸè¿”å›ï¼šn_clicks={n_clicks}, auto_run={auto_run}")
        return stored_data

    # è¼‰å…¥æ•¸æ“š
    df_raw, df_factor = load_data(ticker, start_date, end_date, "Self")
    if df_raw.empty:
        logger.warning(f"ç„¡æ³•è¼‰å…¥ {ticker} çš„æ•¸æ“š")
        return {"error": f"ç„¡æ³•è¼‰å…¥ {ticker} çš„æ•¸æ“š"}

    ctx_trigger = ctx.triggered_id

    # åªåœ¨ auto-run ç‚º True æˆ–æŒ‰éˆ•è¢«é»æ“Šæ™‚é‹ç®—
    if not auto_run and ctx_trigger != 'run-btn':
        logger.debug(f"è·³éå›æ¸¬ï¼šauto_run={auto_run}, ctx_trigger={ctx_trigger}")
        return stored_data

    logger.info(f"é–‹å§‹åŸ·è¡Œå›æ¸¬ - ticker: {ticker}, ç­–ç•¥æ•¸: {len(strategy_names)}")
    results = {}

    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
    valve_triggered = False
    atr_ratio_current = None

    for strat in strategy_names:
        # åªä½¿ç”¨ param_presets ä¸­çš„åƒæ•¸
        strat_params = param_presets[strat].copy()
        strat_type = strat_params["strategy_type"]
        smaa_src = strat_params.get("smaa_source", "Self")

        # ç‚ºæ¯å€‹ç­–ç•¥è¼‰å…¥å°æ‡‰çš„æ•¸æ“š
        df_raw, df_factor = load_data(ticker, start_date, end_date if end_date else None, smaa_source=smaa_src)

        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        logger.info(f"[{strat}] é¢¨éšªé–¥é–€é–‹é—œç‹€æ…‹: global_apply={global_apply}, é¡å‹={type(global_apply)}")
        if global_apply:
            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")

            # è¨ˆç®— ATR æ¯”å€¼ï¼ˆä½¿ç”¨æœ€æ–°æ•¸æ“šï¼Œåƒ…ç”¨æ–¼æ—¥èªŒé¡¯ç¤ºï¼‰
            try:
                atr_20 = calculate_atr(df_raw, 20)
                atr_60 = calculate_atr(df_raw, 60)

                # ç¢ºä¿ ATR æ•¸æ“šæœ‰æ•ˆ
I suspect the result variable is being referenced before definition, causing potential bugs.

The valve info is lost after the new result dict is built without it.

Let's check if result['valve'] is reassigned after line 1125.

root@5583bff288d6:/workspace/SSS096# sed -n '1125,1165p' app_dash.py
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
                    'trade_ledger': pack_df(backtest_result.ledger),
                    'daily_state_std': pack_df(backtest_result.daily_state),
                    'trade_ledger_std': pack_df(backtest_result.ledger)
                }

                logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: æ¬Šç›Šæ›²ç·šé•·åº¦={len(backtest_result.equity_curve)}, äº¤æ˜“æ•¸={len(backtest_result
.ledger) if backtest_result.ledger is not None and not backtest_result.ledger.empty else 0}")

            except Exception as e:
                logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
                # å‰µå»ºç©ºçš„çµæœ
                result = {
                    'trades': [],
                    'trade_df': pd.DataFrame(),
                    'trades_df': pd.DataFrame(),
                    'signals_df': pd.DataFrame(),
                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_rat
io': 0.0, 'num_trades': 0},
                    'equity_curve': pd.Series(1.0, index=df_raw.index)
                }

            # === ä¿®å¾© 3ï¼šæ·»åŠ èª¿è©¦æ—¥èªŒï¼Œæ ¸å°å­ç­–ç•¥é›†åˆæ˜¯å¦ä¸€è‡´ ===
            logger.info(f"[Ensemble] åŸ·è¡Œå®Œæˆï¼Œticker={ticker}, method={flat_params.get('method')}")
            if 'equity_curve' in result and hasattr(result['equity_curve'], 'shape'):
                logger.info(f"[Ensemble] æ¬Šç›Šæ›²ç·šé•·åº¦: {len(result['equity_curve'])}")
            if 'trade_df' in result and hasattr(result['trade_df'], 'shape'):
                logger.info(f"[Ensemble] äº¤æ˜“è¨˜éŒ„æ•¸é‡: {len(result['trade_df'])}")
        else:
            if strat_type == 'single':
                df_ind = compute_single(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen
"], strat_params["devwin"], smaa_source=smaa_src)
            elif strat_type == 'dual':
                df_ind = compute_dual(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen"]
, strat_params["short_win"], strat_params["long_win"], smaa_source=smaa_src)
I think metrics vanish after applying the risk valve. I'll recompute metrics post-transformation.

root@5583bff288d6:/workspace/SSS096# rg "calculate_metrics" -n
analysis/ROEAv4.py
25:from SSSv095a1 import calculate_metrics, load_data_wrapper
407:    all_metric_cols = list(calculate_metrics([], pd.DataFrame()).keys())

SSSv096.py
40:    "compute_backtest_for_periods", "calculate_metrics",
824:def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series
= None) -> Dict:
1295:    metrics = calculate_metrics(trades, df_ind, equity_curve)
I'll open lines 824 to 835 in SSSv096.py to check out calculate_metrics.

root@5583bff288d6:/workspace/SSS096# sed -n '824,870p' SSSv096.py
def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series = No
ne) -> Dict:
    """
    è¨ˆç®—å›æ¸¬ç¸¾æ•ˆæŒ‡æ¨™.
    Args:
        trades: äº¤æ˜“è¨˜éŒ„,åŒ…å«æ—¥æœŸå’Œå ±é…¬ç‡.
        df_ind: æŒ‡æ¨™æ•¸æ“š DataFrame,åŒ…å«äº¤æ˜“æ—¥ç´¢å¼•.
        equity_curve: å…¨éƒ¨å›æ¸¬æœŸé–“çš„æ¯æ—¥è³‡ç”¢æ·¨å€¼ï¼ˆå¯é¸, è‹¥è¦è¨ˆç®—æŒæœ‰æœŸé–“MMDéœ€æä¾›ï¼‰
    Returns:
        Dict: åŒ…å«ç¸½å›å ±ç‡ã€å¹´åŒ–å›å ±ç‡ã€æœ€å¤§å›æ’¤ç­‰æŒ‡æ¨™.
    """
    metrics = {
        'total_return': 0.0,
        'annual_return': 0.0,
        'max_drawdown': 0.0,  # é€™è£¡ä¹‹å¾Œæœƒç›´æ¥ç”¨ max_mmd è¦†è“‹
        'max_drawdown_duration': 0,
        'calmar_ratio': np.nan,
        'num_trades': 0,
        'win_rate': 0.0,
        'avg_win': np.nan,
        'avg_loss': np.nan,
        'payoff_ratio': np.nan,
        'sharpe_ratio': np.nan,
        'sortino_ratio': np.nan,
        'max_consecutive_wins': 0,# æ–°å¢
        'max_consecutive_losses': 0,# æ–°å¢
        'avg_holding_period': np.nan,# æ–°å¢
        'annualized_volatility': np.nan,# æ–°å¢
        'profit_factor': np.nan,# æ–°å¢
        # 'avg_mmd': np.nan, # ä¸å†éœ€è¦
        # 'max_mmd': np.nan, # ä¸å†éœ€è¦å–®ç¨æ¬„ä½
    }
    if not trades:
        return metrics

    trade_metrics = pd.DataFrame(trades, columns=['entry_date', 'return', 'exit_date']).set_index('exit_date')
    trade_metrics['equity'] = (1 + trade_metrics['return']).cumprod()
    roll_max = trade_metrics['equity'].cummax()
    daily_drawdown = trade_metrics['equity'] / roll_max - 1

    # åŸºæœ¬æŒ‡æ¨™
    metrics['total_return'] = trade_metrics['equity'].iloc[-1] - 1
    years = max((trade_metrics.index[-1] - trade_metrics.index[0]).days / 365.25, 1)
    metrics['annual_return'] = (1 + metrics['total_return']) ** (1 / years) - 1

    # æ—¢æœ‰æ—¥å…§å›æ’¤ï¼ˆnumpy é™£åˆ—ï¼‰
    dd_np = (daily_drawdown < 0).astype(bool)
Iâ€™ll use existing functions to recompute metrics or preserve them from backtest_result.stats.

I need to check if run_ensemble_backtest causes stats to be a Series instead of dict.

root@5583bff288d6:/workspace/SSS096# rg "stats" -n SSS_EnsembleTab.py | head -n 40
691:def perf_stats(equity: pd.Series, w: pd.Series) -> Dict[str, float]:
871:    stats = perf_stats(equity, w)
874:    logger.info(f"[Ensemble] ç¸¾æ•ˆæ‘˜è¦: ç¸½å ±é…¬={stats.get('total_return', 0):.4f}, å¹´åŒ–={stats.get('annual_return', 0):.4f},
æœ€å¤§å›æ’¤={stats.get('max_drawdown', 0):.4f}")
895:        f.write(f"ç¸¾æ•ˆ: {stats}\n")
902:    return px["open"], w, trades_ui, stats, method_name, equity, daily_state, trade_ledger_ui
904:def save_outputs(method_name: str, open_px: pd.Series, w: pd.Series, trades: pd.DataFrame, stats: Dict[str, float], equity:
pd.Series = None, cost: CostParams = None, daily_state: pd.DataFrame = None, trade_ledger: pd.DataFrame = None):
943:        "total_return": stats.get("total_return"),
944:        "annual_return": stats.get("annual_return"),
945:        "max_drawdown": stats.get("max_drawdown"),
946:        "calmar_ratio": stats.get("calmar_ratio"),
947:        "sharpe_ratio": stats.get("sharpe_ratio"),
948:        "sortino_ratio": stats.get("sortino_ratio"),
949:        "time_in_market": stats.get("time_in_market"),
950:        "turnover_py": stats.get("turnover_py"),
951:        "num_trades": stats.get("num_trades"),
1009:    open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)
1012:    save_outputs(method_name, open_px, w, trades, stats, equity, cost, daily_state, trade_ledger)
1015:    print(f"ç¸¾æ•ˆ: {stats}")
1082:                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)
1090:                st.metric("ç¸½å ±é…¬ç‡", f"{stats.get('total_return', 0):.2%}")
1091:                st.metric("å¹´åŒ–å ±é…¬ç‡", f"{stats.get('annual_return', 0):.2%}")
1093:                st.metric("æœ€å¤§å›æ’¤", f"{stats.get('max_drawdown', 0):.2%}")
1094:                st.metric("å¤æ™®æ¯”ç‡", f"{stats.get('sharpe_ratio', 0):.2f}")
1096:                st.metric("å¡ç‘ªæ¯”ç‡", f"{stats.get('calmar_ratio', 0):.2f}")
1097:                st.metric("äº¤æ˜“æ¬¡æ•¸", stats.get('num_trades', 0))
1219:            save_outputs(method_name, open_px, w, trades, stats, equity, cost, daily_state, trade_ledger)
I'm looking into why metrics might be replaced with 'avg_holding_period' during result handling.

root@5583bff288d6:/workspace/SSS096# rg "avg_holding_period" -n app_dash.py
1545:            metrics['avg_holding_period'] = avg_holding
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1573:                elif k in ["max_drawdown_duration", "avg_holding_period"]:
I'll check around line 1545 to examine how avg_holding_period is integrated into the metrics.

root@5583bff288d6:/workspace/SSS096# sed -n '1528,1554p' app_dash.py

            if ledger_src is not None and not ledger_src.empty:
                # ç‚ºäº† 100% ä¿è­‰ weight_change å‡ºç¾ï¼Œå…ˆç¢ºä¿æ¬Šé‡æ¬„ä½
                ledger_src = _ensure_weight_columns(ledger_src)
                # ä½¿ç”¨æ–°çš„çµ±ä¸€æ ¼å¼åŒ–å‡½å¼
                ledger_display = format_trade_like_df_for_display(ledger_src)
                ledger_columns = [{"name": i, "id": i} for i in ledger_display.columns]
                ledger_data = ledger_display.to_dict('records')
            else:
                ledger_columns = []
                ledger_data = []

            metrics = result.get('metrics', {})
            tooltip = f"{strategy} ç­–ç•¥èª¬æ˜"
            param_display = {k: v for k, v in param_presets[strategy].items() if k != "strategy_type"}
            param_str = ", ".join(f"{k}: {v}" for k, v in param_display.items())
            avg_holding = calculate_holding_periods(trade_df)
            metrics['avg_holding_period'] = avg_holding

            label_map = {
                "total_return": "ç¸½å›å ±ç‡",
                "annual_return": "å¹´åŒ–å›å ±ç‡",
                "win_rate": "å‹ç‡",
                "max_drawdown": "æœ€å¤§å›æ’¤",
                "max_drawdown_duration": "å›æ’¤æŒçºŒ",
                "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
                "sharpe_ratio": "å¤æ™®æ¯”ç‡",
It looks like result is saved with results[strat] = result. I'll search for this.

root@5583bff288d6:/workspace/SSS096# rg "results\[strat\]" -n app_dash.py
1320:        results[strat] = result
root@5583bff288d6:/workspace/SSS096# sed -n '1310,1325p' app_dash.py
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)
I think the issue with empty metrics happens after modifying resultâ€”metrics get lost when risk valve triggers unintended changes. There's also a bug referencing result before its definition in the risk valve block.

The missing metrics might happen when risk valve triggers, overwriting or mutating result unexpectedly.

Iâ€™ll investigate results.get(strategy) calls that might modify results, beyond displaying at line 1397.

root@5583bff288d6:/workspace/SSS096# sed -n '1388,1408p' app_dash.py
        bg_color = '#003366'
        legend_font_color = '#ffe066'
        legend_bgcolor = '#002244'
        legend_bordercolor = '#ffe066'

    if tab == "backtest":
        # å‰µå»ºç­–ç•¥å›æ¸¬çš„å­é ç±¤
        strategy_tabs = []
        for strategy in strategy_names:
            result = results.get(strategy)
            if not result:
                continue

            # === çµ±ä¸€å…¥å£ï¼šè®€å–äº¤æ˜“è¡¨ã€æ—¥ç‹€æ…‹ã€æ¬Šç›Šæ›²ç·š ===
            # è®€äº¤æ˜“è¡¨çš„çµ±ä¸€å…¥å£ï¼šå…ˆç”¨æ¨™æº–éµï¼Œå† fallback
            trade_df = None
            candidates = [
                result.get('trades'),      # å…¨å±€è¦†å¯«å¾Œæ¨™æº–éµ
                result.get('trades_ui'),   # èˆŠæ ¼å¼ï¼ˆè‹¥é‚„å­˜åœ¨ï¼‰
                result.get('trade_df'),    # æŸäº›ç­–ç•¥è‡ªå¸¶
            ]
Iâ€™ll ensure correct metric preservation after applying the risk valve and initialize data properly.

I need to confirm the structure of the trade ledger from risk_valve_backtest before calculating metrics.

root@5583bff288d6:/workspace/SSS096# sed -n '1359,1387p' SSS_EnsembleTab.py
def risk_valve_backtest(open_px: pd.Series,
                        w: pd.Series,
                        cost,
                        benchmark_df: pd.DataFrame,
                        mode: str = "cap",
                        cap_level: float = 0.5,
                        slope20_thresh: float = 0.0,
                        slope60_thresh: float = 0.0,
                        atr_win: int = 20,
                        atr_ref_win: int = 60,
                        atr_ratio_mult: float = 1.3) -> dict:
    """å›å‚³ï¼šåŸå§‹èˆ‡é–¥é–€ç‰ˆæœ¬çš„ç¸¾æ•ˆã€å³å°¾å‰Šæ¸›åº¦ã€ä»¥åŠå…©ç‰ˆ equity/ledger"""
    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
                                     atr_win, atr_ref_win, atr_ratio_mult)
    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)

    # åŸå§‹ç‰ˆæœ¬
    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
    # é–¥é–€ç‰ˆæœ¬
    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)

    # äº¤æ˜“çµ±è¨ˆï¼ˆåªç”¨è³£å‡ºåˆ—ï¼›ç¬¦åˆ re.txt å°å¯¦ç¾æç›Šçš„å£å¾‘ï¼‰
    r1 = _sell_returns_pct_from_ledger(ledger1)
    r2 = _sell_returns_pct_from_ledger(ledger2)
    pf1 = (r1[r1>0].sum() / abs(r1[r1<0].sum())) if (r1[r1<0].sum()!=0) else np.inf
    pf2 = (r2[r2>0].sum() / abs(r2[r2<0].sum())) if (r2[r2<0].sum()!=0) else np.inf
    mdd1 = _mdd_from_daily_equity(daily_state1["equity"])
    mdd2 = _mdd_from_daily_equity(daily_state2["equity"])
I'll scroll through from line 1387 to dive deeper into the ledger.

root@5583bff288d6:/workspace/SSS096# sed -n '1387,1410p' SSS_EnsembleTab.py

    # å³å°¾å‰Šæ¸›ï¼šæ¯”è¼ƒå…©ç‰ˆçš„ã€Œ>P90 çš„æ­£å ±é…¬ç¸½å’Œã€
    def right_tail_sum(x: pd.Series):
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)

    return {
        "signals": sig,
        "weights_orig": w,
        "weights_valve": w2,
        "daily_state_orig": daily_state1,
        "daily_state_valve": daily_state2,
        "trade_ledger_orig": ledger1,
        "trade_ledger_valve": ledger2,
        "metrics": {
            "pf_orig": float(pf1), "pf_valve": float(pf2),
            "mdd_orig": float(mdd1), "mdd_valve": float(mdd2),
            "right_tail_sum_orig": float(rt1), "right_tail_sum_valve": float(rt2),
            "right_tail_reduction": float(right_tail_reduction)
        }
Iâ€™m exploring how to compute metrics based on trade ledger or equity curve. I'll check perf_stats.

root@5583bff288d6:/workspace/SSS096# sed -n '691,711p' SSS_EnsembleTab.py
def perf_stats(equity: pd.Series, w: pd.Series) -> Dict[str, float]:
    """è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
    if len(equity) < 2:
        return {}

    # è¨ˆç®—æ—¥å ±é…¬ç‡
    r = equity.pct_change().dropna()

    # åŸºæœ¬æŒ‡æ¨™
    total_return = (equity.iloc[-1] / equity.iloc[0]) - 1
    annual_return = total_return * (252 / len(equity))

    # æœ€å¤§å›æ’¤
    cummax = equity.cummax()
    drawdown = (equity - cummax) / cummax
    max_drawdown = drawdown.min()

    # é¢¨éšªèª¿æ•´æŒ‡æ¨™
    sharpe_ratio = r.mean() / r.std() * np.sqrt(252) if r.std() > 0 else 0
    calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown != 0 else 0
    sortino_ratio = r.mean() / r[r < 0].std() * np.sqrt(252) if len(r[r < 0]) > 0 and r[r < 0].std() > 0 else 0
root@5583bff288d6:/workspace/SSS096# sed -n '711,730p' SSS_EnsembleTab.py
    sortino_ratio = r.mean() / r[r < 0].std() * np.sqrt(252) if len(r[r < 0]) > 0 and r[r < 0].std() > 0 else 0

    # å…¶ä»–æŒ‡æ¨™
    time_in_market = (w > 0.5).mean()
    turnover_py = w.diff().abs().sum() / len(w) * 252

    return {
        'total_return': total_return,
        'annual_return': annual_return,
        'max_drawdown': max_drawdown,
        'sharpe_ratio': sharpe_ratio,
        'calmar_ratio': calmar_ratio,
        'sortino_ratio': sortino_ratio,
        'time_in_market': time_in_market,
        'turnover_py': turnover_py,
        'num_trades': len(w.diff()[w.diff() != 0])
    }

# ---------------------------------------------------------------------
# ä¸»æµç¨‹ï¼šè®€åƒ¹ã€è®€å­ç­–ç•¥éƒ¨ä½ -> ç”Ÿæˆæ¬Šé‡ -> æ¬Šç›Šèˆ‡äº¤æ˜“ -> è¼¸å‡º
After applying the risk valve, I'll recompute stats using perf_stats to ensure metrics exist.

I need to confirm AGENTS instructions for Chinese comments and modification records in app_dash.py.

root@5583bff288d6:/workspace/SSS096# head -n 20 app_dash.py
import dash
from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.graph_objects as go
import json
import io
from dash.dependencies import ALL
import shutil
import os
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from analysis import config as cfg
import yfinance as yf
import logging
import numpy as np
from urllib.parse import quote as urlparse

# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
The instructions ask for a date-stamped record and file path at the top of modified files. I'll check other files for examples.

root@5583bff288d6:/workspace/SSS096# rg "ä¿®æ”¹ç´€éŒ„" -n
AGENTS.md
93:- ä¿®æ”¹ç´€éŒ„éœ€åŠ å…¥æ—¥æœŸæ™‚é–“æˆ³è¨˜
I'm adding date/time and risk valve handling in the backtest. I'll adjust the code flow.

Metrics must be recomputed after applying the risk valve in both ssma_turn and ensemble branches.

Iâ€™ll recompute metrics after applying risk valve and ensure theyâ€™re serializable for ssma_turn.

Iâ€™ll apply fixes for both branches, adjust imports, and recompute metrics as necessary.

root@5583bff288d6:/workspace/SSS096# which pwsh
root@5583bff288d6:/workspace/SSS096# which powershell
PowerShell isn't available, but I'll try using pwsh to execute the script and record errors.

root@5583bff288d6:/workspace/SSS096# sed -n '1070,1135p' app_dash.py
                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
                backtest_result = run_ensemble_backtest(cfg)

                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
                if global_apply and valve_triggered:
                    from SSS_EnsembleTab import risk_valve_backtest
                    bench = df_raw  # å·²å« open/high/low/close/volume

                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")

                    rv = risk_valve_backtest(
                        open_px=backtest_result.price_series,
                        w=backtest_result.weight_curve,
                        cost=cost_params,
                        benchmark_df=bench,
                        mode="cap",
                        cap_level=float(risk_cap),
                    )
                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
                    backtest_result.daily_state = rv["daily_state_valve"]
                    backtest_result.ledger = rv["trade_ledger_valve"]
                    backtest_result.weight_curve = rv["weights_valve"]
                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")

                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
                    if "metrics" in rv:
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['metr
ics'].get('pf_valve', 'N/A'):.2f}")
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={rv['
metrics'].get('mdd_valve', 'N/A'):.2f}%")

                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
                    result['valve'] = {
                        "applied": True,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio
                    }
                else:
                    if global_apply:
                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio if ratio is not None else "N/A"
                        }
                    else:
                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": "N/A",
                            "atr_ratio": "N/A"
                        }

                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
I need to fix the result['valve'] issue and adjust ssma_turn from 909â€“977.

root@5583bff288d6:/workspace/SSS096# sed -n '909,960p' app_dash.py
                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
                        )

                        rv = risk_valve_backtest(
                            open_px=open_px,
                            w=w_series,
                            cost=cost_params,
                            benchmark_df=df_raw,
                            mode="cap",
                            cap_level=float(risk_cap),
                        )

                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
                        result['daily_state']      = pack_df(rv["daily_state_valve"])
                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
                        result['weight_curve']     = pack_series(rv["weights_valve"])
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
                        result['valve'] = {
                            "applied": True,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio_local
                        }

                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
                    else:
                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
                else:
                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                    result['valve'] = {
                        "applied": False,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio_local if ratio_local is not None else "N/A"
                    }
        elif strat_type == 'ensemble':
            # ä½¿ç”¨æ–°çš„ ensemble_runner é¿å…å¾ªç’°ä¾è³´
            try:
                from runners.ensemble_runner import run_ensemble_backtest
                from SSS_EnsembleTab import EnsembleParams, CostParams, RunConfig

                # æŠŠ SSSv096 çš„å·¢ç‹€åƒæ•¸æ”¤å¹³
                flat_params = {}
                flat_params.update(strat_params.get('params', {}))
Iâ€™ll import perf_stats inside the function, update imports, and compute metrics after risk valve. I'll patch the file now.

root@5583bff288d6:/workspace/SSS096# sed -n '100,120p' app_dash.py
        return pd.Series(index=df.index, dtype=float)

def calculate_equity_curve(open_px, w, cap, atr_ratio):
    """è¨ˆç®—æ¬Šç›Šæ›²ç·š"""
    try:
        # ç°¡åŒ–çš„æ¬Šç›Šæ›²ç·šè¨ˆç®—
        # é€™è£¡ä½¿ç”¨é–‹ç›¤åƒ¹å’Œæ¬Šé‡çš„ä¹˜ç©ä¾†æ¨¡æ“¬æ¬Šç›Šè®ŠåŒ–
        equity = (open_px * w * cap).cumsum()
        return equity
    except Exception as e:
        logger.warning(f"æ¬Šç›Šæ›²ç·šè¨ˆç®—å¤±æ•—: {e}")
        return None

def calculate_trades_from_equity(equity_curve, open_px, w, cap, atr_ratio):
    """å¾æ¬Šç›Šæ›²ç·šè¨ˆç®—äº¤æ˜“è¨˜éŒ„"""
    try:
        if equity_curve is None or equity_curve.empty:
            return None

        # ç°¡åŒ–çš„äº¤æ˜“è¨˜éŒ„ç”Ÿæˆ
        # é€™è£¡æ ¹æ“šæ¬Šé‡è®ŠåŒ–ä¾†è­˜åˆ¥äº¤æ˜“
I'll add a modification record comment at the top and apply necessary code changes.

root@5583bff288d6:/workspace/SSS096# sed -n '1,20p' app_dash.py
import dash
from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.graph_objects as go
import json
import io
from dash.dependencies import ALL
import shutil
import os
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from analysis import config as cfg
import yfinance as yf
import logging
import numpy as np
from urllib.parse import quote as urlparse

# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
I noticed the lack of a modification record, so I'll go ahead and add one.

root@5583bff288d6:/workspace/SSS096# apply_patch<<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -import dash
> -from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
> -import dash_bootstrap_components as dbc
> -import pandas as pd
> -import plotly.graph_objects as go
> -import json
> -import io
> -from dash.dependencies import ALL
> -import shutil
> -import os
> -from datetime import datetime, timedelta
> -from pathlib import Path
> -import joblib
> -from analysis import config as cfg
> -import yfinance as yf
> -import logging
> -import numpy as np
> -from urllib.parse import quote as urlparse
> -
> -# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
> +import dash
> +from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
> +import dash_bootstrap_components as dbc
> +import pandas as pd
> +import plotly.graph_objects as go
> +import json
> +import io
> +from dash.dependencies import ALL
> +import shutil
> +import os
> +from datetime import datetime, timedelta
> +from pathlib import Path
> +import joblib
> +from analysis import config as cfg
> +import yfinance as yf
> +import logging
> +import numpy as np
> +from urllib.parse import quote as urlparse
> +
> +#app_dash.py
> +# ä¿®æ”¹ç´€éŒ„:
> +# 2025-08-19: ä¿®æ­£é¢¨éšªé–¥é–€å¥—ç”¨å¾Œç¸¾æ•ˆæŒ‡æ¨™éºå¤±å•é¡Œï¼Œä¸¦åœ¨çµæœæ§‹å»ºå¾ŒåŠ å…¥é–¥é–€è³‡è¨Š
> +
> +# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
The patch worked! Now, I'll adjust the ensemble branch risk valve as planned.

root@5583bff288d6:/workspace/SSS096# apply_patch<<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
> -                backtest_result = run_ensemble_backtest(cfg)
> -
å‡ºä¸€è‡´
+                    backtest_result.daily_state = rv["daily_state_valve"]
+                    backtest_result.ledger = rv["trade_ledger_valve"]
+                    backtest_r> -                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
> -                if global_apply and valve_triggered:
      > -                    from SSS_EnsembleTab import risk_valve_backtest
> -                    bench = df_raw  # å·²å« open/high/low/close/volume
f"[{strat}] > -
> -                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
> -
> -                    rv = risk_valve_backtest(
> -                        open_px=backtest_result.price_series,
é¢¨éšªé–¥> -                        w=backtest_result.weight_curve,
> -                        cost=cost_params,
et('pf_valve> -                        benchmark_df=bench,
> -                        mode="cap",
> -                        cap_level=float(risk_cap),
mdd_orig', 'N/A'):> -                    )
> -                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
> -                    backtest_result.daily_state = rv["daily_state_valve"]
    > -                    backtest_result.ledger = rv["trade_ledger_valve"]
> -                    backtest_result.weight_curve = rv["weights_valve"]
> -                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
> -                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
> -
> -                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
> -                    if "metrics" in rv:
> -                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['m
etrics'].get('pf_valve', 'N/A'):.2f}")
> -                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={r
v['metrics'].get('mdd_valve', 'N/A'):.2f}%")
> -
> -                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
> -                    result['valve'] = {
{
> -                        "applied": True,
> -                        "cap": float(risk_cap),
backtest_result> -                        "atr_ratio": ratio
> -                    }
> -                else:
.trades),
@@
          > -                    if global_apply:
> -                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
> -                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
> -                        result['valve'] = {
)
+       > -                            "applied": False,
o is no> -                            "cap": float(risk_cap),
valve'] = valve_> -                            "atr_ratio": ratio if ratio is not None else "N/A",
> -                        }
> -                    else:
> -                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
> -                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
> -                        result['valve'] = {
> -                            "applied": False,
> -                            "cap": "N/A",
> -                            "atr_ratio": "N/A",
> -                        }
> -
> -                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
> -                result = {
> +                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
> +                backtest_result = run_ensemble_backtest(cfg)
> +
> +                valve_info = None
> +                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
> +                if global_apply and valve_triggered:
> +                    from SSS_EnsembleTab import risk_valve_backtest, perf_stats
> +                    bench = df_raw  # å·²å« open/high/low/close/volume
> +
> +                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
> +
> +                    rv = risk_valve_backtest(
> +                        open_px=backtest_result.price_series,
> +                        w=backtest_result.weight_curve,
> +                        cost=cost_params,
> +                        benchmark_df=bench,
> +                        mode="cap",
> +                        cap_level=float(risk_cap),
> +                    )
> +                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
> +                    backtest_result.daily_state = rv["daily_state_valve"]
> +                    backtest_result.ledger = rv["trade_ledger_valve"]
> +                    backtest_result.weight_curve = rv["weights_valve"]
> +                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
> +                    backtest_result.stats = perf_stats(backtest_result.equity_curve, backtest_result.weight_curve)
> +                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
> +
> +                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
> +                    if "metrics" in rv:
> +                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['m
etrics'].get('pf_valve', 'N/A'):.2f}")
> +                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={r
v['metrics'].get('mdd_valve', 'N/A'):.2f}%")
> +
> +                    valve_info = {
> +                        "applied": True,
> +                        "cap": float(risk_cap),
> +                        "atr_ratio": ratio,
> +                    }
> +                else:
> +                    if global_apply:
> +                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
> +                        valve_info = {
> +                            "applied": False,
> +                            "cap": float(risk_cap),
> +                            "atr_ratio": ratio if ratio is not None else "N/A",
> +                        }
> +                    else:
> +                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
> +                        valve_info = {
> +                            "applied": False,
> +                            "cap": "N/A",
> +                            "atr_ratio": "N/A",
> +                        }
> +
> +                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
> +                result = {
>                      'trades': [],
>                      'trade_df': pack_df(backtest_result.trades),
>                      'trades_df': pack_df(backtest_result.trades),
> @@
>                      'daily_state_std': pack_df(backtest_result.daily_state),
> -                    'trade_ledger_std': pack_df(backtest_result.ledger)
> -                }
> +                    'trade_ledger_std': pack_df(backtest_result.ledger)
> +                }
> +                if valve_info is not None:
> +                    result['valve'] = valve_info
> *** End Patch
> PATCH
Failed to find expected lines ["                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ", "                backtest_result = run_ensembl
e_backtest(cfg)", "", "                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP", "                if global_apply a
nd valve_triggered:", "                    from SSS_EnsembleTab import risk_valve_backtest", "                    bench = df_raw
  # å·²å« open/high/low/close/volume", "", "                    logger.info(f\"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, rat
io={ratio:.4f}\")", "", "                    rv = risk_valve_backtest(", "                        open_px=backtest_result.price_
series,", "                        w=backtest_result.weight_curve,", "                        cost=cost_params,", "
           benchmark_df=bench,", "                        mode=\"cap\",", "                        cap_level=float(risk_cap),",
"                    )", "                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´", "                    backtest_result.daily_state
= rv[\"daily_state_valve\"]", "                    backtest_result.ledger = rv[\"trade_ledger_valve\"]", "                    ba
cktest_result.weight_curve = rv[\"weights_valve\"]", "                    backtest_result.equity_curve = rv[\"daily_state_valve\
"][\"equity\"]", "                    logger.info(f\"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰\")", "", "
                 # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š", "                    if \"metrics\" in rv:", "                        logger.inf
o(f\"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['metrics'].get('pf_valve', 'N/A'):.2f}
\")", "                        logger.info(f\"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥
é–€={rv['metrics'].get('mdd_valve', 'N/A'):.2f}%\")", "", "                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰", "
        result['valve'] = {", "                        \"applied\": True,", "                        \"cap\": float(risk_cap),",
 "                        \"atr_ratio\": ratio", "                    }", "                else:", "                    if globa
l_apply:", "                        logger.info(f\"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸\")", "                        # çµ¦
UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰", "                        result['valve'] = {", "                            \"applied\": False,", "
                     \"cap\": float(risk_cap),", "                            \"atr_ratio\": ratio if ratio is not None else \"N
/A\",", "                        }", "                    else:", "                        logger.info(f\"[{strat}] âšª å…¨å±€é¢¨éšªé–¥
é–€æœªå•Ÿç”¨\")", "                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰", "                        result['valve'] = {", "
              \"applied\": False,", "                            \"cap\": \"N/A\",", "                            \"atr_ratio\":
 \"N/A\",", "                        }", "", "                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§", "                result = {", "
              'trades': [],", "                    'trade_df': pack_df(backtest_result.trades),", "                    'trades_d
f': pack_df(backtest_result.trades),"] in app_dash.py
The lines didn't match, so I'll patch using smaller segments to ensure accuracy.

root@5583bff288d6:/workspace/SSS096# sed -n '1070,1120p' app_dash.py
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
                    if ratio is None:
                        ratio = 1.5  # è¨­å®šä¸€å€‹é è¨­å€¼ç”¨æ–¼é¡¯ç¤º

                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
                backtest_result = run_ensemble_backtest(cfg)

                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
                if global_apply and valve_triggered:
                    from SSS_EnsembleTab import risk_valve_backtest
                    bench = df_raw  # å·²å« open/high/low/close/volume

                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")

                    rv = risk_valve_backtest(
                        open_px=backtest_result.price_series,
                        w=backtest_result.weight_curve,
                        cost=cost_params,
                        benchmark_df=bench,
                        mode="cap",
                        cap_level=float(risk_cap),
                    )
                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
                    backtest_result.daily_state = rv["daily_state_valve"]
                    backtest_result.ledger = rv["trade_ledger_valve"]
                    backtest_result.weight_curve = rv["weights_valve"]
                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")

                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
                    if "metrics" in rv:
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['metr
ics'].get('pf_valve', 'N/A'):.2f}")
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={rv['
metrics'].get('mdd_valve', 'N/A'):.2f}%")

                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
                    result['valve'] = {
                        "applied": True,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio
                    }
                else:
                    if global_apply:
                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio if ratio is not None else "N/A"
                        }
                    else:
                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
root@5583bff288d6:/workspace/SSS096# sed -n '1120,1168p' app_dash.py
                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": "N/A",
                            "atr_ratio": "N/A"
                        }

                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
                    'trade_ledger': pack_df(backtest_result.ledger),
                    'daily_state_std': pack_df(backtest_result.daily_state),
                    'trade_ledger_std': pack_df(backtest_result.ledger)
                }

                logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: æ¬Šç›Šæ›²ç·šé•·åº¦={len(backtest_result.equity_curve)}, äº¤æ˜“æ•¸={len(backtest_result
.ledger) if backtest_result.ledger is not None and not backtest_result.ledger.empty else 0}")

            except Exception as e:
                logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
                # å‰µå»ºç©ºçš„çµæœ
                result = {
                    'trades': [],
                    'trade_df': pd.DataFrame(),
                    'trades_df': pd.DataFrame(),
                    'signals_df': pd.DataFrame(),
                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_rat
io': 0.0, 'num_trades': 0},
                    'equity_curve': pd.Series(1.0, index=df_raw.index)
                }

            # === ä¿®å¾© 3ï¼šæ·»åŠ èª¿è©¦æ—¥èªŒï¼Œæ ¸å°å­ç­–ç•¥é›†åˆæ˜¯å¦ä¸€è‡´ ===
            logger.info(f"[Ensemble] åŸ·è¡Œå®Œæˆï¼Œticker={ticker}, method={flat_params.get('method')}")
            if 'equity_curve' in result and hasattr(result['equity_curve'], 'shape'):
                logger.info(f"[Ensemble] æ¬Šç›Šæ›²ç·šé•·åº¦: {len(result['equity_curve'])}")
            if 'trade_df' in result and hasattr(result['trade_df'], 'shape'):
                logger.info(f"[Ensemble] äº¤æ˜“è¨˜éŒ„æ•¸é‡: {len(result['trade_df'])}")
        else:
            if strat_type == 'single':
                df_ind = compute_single(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen
"], strat_params["devwin"], smaa_source=smaa_src)
            elif strat_type == 'dual':
Time to create a patch with the lines we have, preserving the necessary context.

root@5583bff288d6:/workspace/SSS096# apply_patch<<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
 rv["da> @@
> -                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
> -                backtest_result = run_ensemble_backtest(cfg)
> -
> -                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
> -                if global_apply and valve_triggered:
   b> -                    from SSS_EnsembleTab import risk_valve_backtest
> -                    bench = df_raw  # å·²å« open/high/low/close/volume
> -
> -                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
> -
> -                    rv = risk_valve_backtest(
> -                        open_px=backtest_result.price_series,
> -                        w=backtest_result.weight_curve,
2> -                        cost=cost_params,
> -                        benchmark_df=bench,
> -                        mode="cap",
> -                        cap_level=float(risk_cap),
/A'):.2> -                    )
t> -                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
           va> -                    backtest_result.daily_state = rv["daily_state_valve"]
,
+                   > -                    backtest_result.ledger = rv["trade_ledger_valve"]
: rati> -                    backtest_result.weight_curve = rv["weights_valve"]
  if> -                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
> -                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
"applied": > -
Fa> -                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
isk_cap),
+    > -                    if "metrics" in rv:
> -                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['m
etrics'].get('pf_valve', 'N/A'):.2f}")
> -                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={r
v['metrics'].get('mdd_valve', 'N/A'):.2f}%")
+    > -
  > -                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
> -                    result['valve'] = {
   result = {
  > -                        "applied": True,
> -                        "cap": float(risk_cap),
> -                        "atr_ratio": ratio
> -                    }
> -                else:

            > -                    if global_apply:
er': pack_df(backt> -                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
ate),
-> -                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
test_re> -                        result['valve'] = {

+            > -                            "applied": False,
> -                            "cap": float(risk_cap),
v> -                            "atr_ratio": ratio if ratio is not None else "N/A"
o
*** End> -                        }
> -                    else:
> -                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
> -                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
> -                        result['valve'] = {
> -                            "applied": False,
> -                            "cap": "N/A",
> -                            "atr_ratio": "N/A"
> -                        }
> -
> -                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
> -                result = {
> +                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
> +                backtest_result = run_ensemble_backtest(cfg)
> +
> +                valve_info = None
> +                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
> +                if global_apply and valve_triggered:
> +                    from SSS_EnsembleTab import risk_valve_backtest, perf_stats
> +                    bench = df_raw  # å·²å« open/high/low/close/volume
> +
> +                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
> +
> +                    rv = risk_valve_backtest(
> +                        open_px=backtest_result.price_series,
> +                        w=backtest_result.weight_curve,
> +                        cost=cost_params,
> +                        benchmark_df=bench,
> +                        mode="cap",
> +                        cap_level=float(risk_cap),
> +                    )
> +                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
> +                    backtest_result.daily_state = rv["daily_state_valve"]
> +                    backtest_result.ledger = rv["trade_ledger_valve"]
> +                    backtest_result.weight_curve = rv["weights_valve"]
> +                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
> +                    backtest_result.stats = perf_stats(backtest_result.equity_curve, backtest_result.weight_curve)
> +                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
> +
> +                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
> +                    if "metrics" in rv:
> +                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['m
etrics'].get('pf_valve', 'N/A'):.2f}")
> +                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={r
v['metrics'].get('mdd_valve', 'N/A'):.2f}%")
> +
> +                    valve_info = {
> +                        "applied": True,
> +                        "cap": float(risk_cap),
> +                        "atr_ratio": ratio,
> +                    }
> +                else:
> +                    if global_apply:
> +                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
> +                        valve_info = {
> +                            "applied": False,
> +                            "cap": float(risk_cap),
> +                            "atr_ratio": ratio if ratio is not None else "N/A",
> +                        }
> +                    else:
> +                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
> +                        valve_info = {
> +                            "applied": False,
> +                            "cap": "N/A",
> +                            "atr_ratio": "N/A",
> +                        }
> +
> +                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
> +                result = {
>                      'trades': [],
>                      'trade_df': pack_df(backtest_result.trades),
>                      'trades_df': pack_df(backtest_result.trades),
> @@
>                      'trade_ledger': pack_df(backtest_result.ledger),
>                      'daily_state_std': pack_df(backtest_result.daily_state),
> -                    'trade_ledger_std': pack_df(backtest_result.ledger)
> -                }
> +                    'trade_ledger_std': pack_df(backtest_result.ledger)
> +                }
> +                if valve_info is not None:
> +                    result['valve'] = valve_info
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
I'll patch the ssma_turn branch to recompute metrics around line 909.

root@5583bff288d6:/workspace/SSS096# apply_patch<<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -                        rv = risk_valve_backtest(
> -                            open_px=open_px,
> -                            w=w_series,
> -                            cost=cost_params,
> -                            benchmark_df=df_raw,
> -                            mode="cap",
> -                            cap_level=float(risk_cap),
> -                        )
> -
> -                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
> -                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
> -                        result['daily_state']      = pack_df(rv["daily_state_valve"])
> -                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
> -                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
> -                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
> -                        result['weight_curve']     = pack_series(rv["weights_valve"])
> -                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
> -                        result['valve'] = {
> -                            "applied": True,
> -                            "cap": float(risk_cap),
> -                            "atr_ratio": ratio_local
> -                        }
> -
> -                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
> +                        rv = risk_valve_backtest(
> +                            open_px=open_px,
> +                            w=w_series,
> +                            cost=cost_params,
> +                            benchmark_df=df_raw,
> +                            mode="cap",
> +                            cap_level=float(risk_cap),
> +                        )
> +
> +                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
> +                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
> +                        result['daily_state']      = pack_df(rv["daily_state_valve"])
> +                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
> +                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
> +                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
> +                        result['weight_curve']     = pack_series(rv["weights_valve"])
> +                        result['metrics']          = perf_stats(rv["daily_state_valve"]["equity"], rv["weights_valve"])
> +                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
> +                        result['valve'] = {
> +                            "applied": True,
> +                            "cap": float(risk_cap),
> +                            "atr_ratio": ratio_local
> +                        }
> +
> +                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
I'll add perf_stats to the import line and check around line 898 for context.

root@5583bff288d6:/workspace/SSS096# sed -n '888,918p' app_dash.py
                except Exception:
                    pass

                if force_trigger:
                    valve_triggered_local = True
                    if ratio_local is None:
                        ratio_local = 1.5

                if valve_triggered_local:
                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
                    # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
                    open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
                    # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
                    w_series = None
                    try:
                        ds_std = df_from_pack(result.get('daily_state_std'))
                        if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
                            w_series = ds_std['w']
                    except Exception:
                        pass
                    if w_series is None:
                        ds = df_from_pack(result.get('daily_state'))
                        if ds is not None and not ds.empty and 'w' in ds.columns:
                            w_series = ds['w']

                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
I'll update the import to include perf_stats. Time to patch it!

root@5583bff288d6:/workspace/SSS096# apply_patch<<'PATCH'
> *** Begin Patch
> *** Update File: app_dash.py
> @@
> -                if valve_triggered_local:
> -                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
> +                if valve_triggered_local:
> +                    from SSS_EnsembleTab import risk_valve_backtest, CostParams, perf_stats
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
I added perf_stats to ensemble and fixed patches, but need to check syntax and run tests.

root@5583bff288d6:/workspace/SSS096# python -m py_compile app_dash.py
I'm moving forward to execute the quick_check script as no compile errors showed up.

root@5583bff288d6:/workspace/SSS096# pwsh -ExecutionPolicy Bypass -File tools/quick_check.ps1
bash: command not found: pwsh
The command didn't work, but I'll try running git status to check the status.

root@5583bff288d6:/workspace/SSS096# git status --short
 M app_dash.py
?? __pycache__/
I need to delete the __pycache__ directory before committing.

root@5583bff288d6:/workspace/SSS096# rm -r __pycache__
root@5583bff288d6:/workspace/SSS096# git status --short
 M app_dash.py
I'll commit now, making sure to include a message in Traditional Chinese as instructed.

root@5583bff288d6:/workspace/SSS096# git commit -am "ä¿®å¾©é¢¨éšªé–¥é–€å¥—ç”¨å¾Œç¸¾æ•ˆæŒ‡æ¨™éºå¤±å•é¡Œ" && git status --short
[work c6468b6] ä¿®å¾©é¢¨éšªé–¥é–€å¥—ç”¨å¾Œç¸¾æ•ˆæŒ‡æ¨™éºå¤±å•é¡Œ
 1 file changed, 23 insertions(+), 17 deletions(-)

root@5583bff288d6:/workspace/SSS096# git status --short
I'll open the lines where I made changes, such as new metrics and modifications to app_dash.py.

root@5583bff288d6:/workspace/SSS096# nl -ba app_dash.py | sed -n '1,120p'
     1  import dash
     2  from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
     3  import dash_bootstrap_components as dbc
     4  import pandas as pd
     5  import plotly.graph_objects as go
     6  import json
     7  import io
     8  from dash.dependencies import ALL
     9  import shutil
    10  import os
    11  from datetime import datetime, timedelta
    12  from pathlib import Path
    13  import joblib
    14  from analysis import config as cfg
    15  import yfinance as yf
    16  import logging
    17  import numpy as np
    18  from urllib.parse import quote as urlparse
    19
    20  #app_dash.py
    21  # ä¿®æ”¹ç´€éŒ„:
    22  # 2025-08-19: ä¿®æ­£é¢¨éšªé–¥é–€å¥—ç”¨å¾Œç¸¾æ•ˆæŒ‡æ¨™éºå¤±å•é¡Œï¼Œä¸¦åœ¨çµæœæ§‹å»ºå¾ŒåŠ å…¥é–¥é–€è³‡è¨Š
    23
    24  # é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
    25  from analysis.logging_config import init_logging
    26  import os
    27
    28  # å¼·åˆ¶å•Ÿç”¨æª”æ¡ˆæ—¥èªŒï¼Œä¸ä¾è³´ç’°å¢ƒè®Šæ•¸
    29  os.environ["SSS_CREATE_LOGS"] = "1"
    30
    31  # ç›´æ¥å‘¼å« init_loggingï¼Œå®ƒæœƒä½¿ç”¨æ­£ç¢ºçš„æ—¥èªŒç›®éŒ„
    32  init_logging(enable_file=True)
    33  logger = logging.getLogger("SSS.App")
    34
    35  # è¨­å®šç‚º DEBUG ç´šåˆ¥ï¼Œè©³ç´°èª¿è©¦è³‡è¨Šæœƒå¯«å…¥æ—¥èªŒæª”æ¡ˆ
    36  logger.setLevel(logging.DEBUG)
    37  logger.info("å·²å•Ÿç”¨è©³ç´°èª¿è©¦æ¨¡å¼ - èª¿è©¦è³‡è¨Šå°‡å¯«å…¥æ—¥èªŒæª”æ¡ˆ")
    38  logger.info(f"æ—¥èªŒç›®éŒ„: {os.path.abspath('analysis/log')}")
    39
    40  # ATR è¨ˆç®—å‡½æ•¸
    41  def calculate_atr(df, window):
    42      """è¨ˆç®— ATR (Average True Range)"""
    43      try:
    44          # æ”¯æ´å¤šç¨®æ¬„ä½åç¨±æ ¼å¼
    45          high_col = None
    46          low_col = None
    47          close_col = None
    48
    49          # æª¢æŸ¥è‹±æ–‡æ¬„ä½åç¨±
    50          if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:
    51              high_col = 'high'
    52              low_col = 'low'
    53              close_col = 'close'
    54          # æª¢æŸ¥ä¸­æ–‡æ¬„ä½åç¨±
    55          elif 'æœ€é«˜åƒ¹' in df.columns and 'æœ€ä½åƒ¹' in df.columns and 'æ”¶ç›¤åƒ¹' in df.columns:
    56              high_col = 'æœ€é«˜åƒ¹'
    57              low_col = 'æœ€ä½åƒ¹'
    58              close_col = 'æ”¶ç›¤åƒ¹'
    59          # æª¢æŸ¥å…¶ä»–å¯èƒ½çš„æ¬„ä½åç¨±
    60          elif 'open' in df.columns and 'close' in df.columns:
    61              # å¦‚æœæ²’æœ‰é«˜ä½åƒ¹ï¼Œç”¨é–‹ç›¤åƒ¹å’Œæ”¶ç›¤åƒ¹è¿‘ä¼¼
    62              high_col = 'open'
    63              low_col = 'close'
    64              close_col = 'close'
    65
    66          if high_col and low_col and close_col:
    67              # æœ‰é«˜ä½åƒ¹æ™‚ï¼Œè¨ˆç®— True Range
    68              high = df[high_col]
    69              low = df[low_col]
    70              close = df[close_col]
    71
    72              # ç¢ºä¿æ•¸æ“šç‚ºæ•¸å€¼å‹
    73              high = pd.to_numeric(high, errors='coerce')
    74              low = pd.to_numeric(low, errors='coerce')
    75              close = pd.to_numeric(close, errors='coerce')
    76
    77              tr1 = high - low
    78              tr2 = abs(high - close.shift(1))
    79              tr3 = abs(low - close.shift(1))
    80
    81              true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    82              atr = true_range.rolling(window=window).mean()
    83          else:
    84              # åªæœ‰æ”¶ç›¤åƒ¹æ™‚ï¼Œç”¨åƒ¹æ ¼è®ŠåŒ–è¿‘ä¼¼
    85              if close_col:
    86                  close = pd.to_numeric(df[close_col], errors='coerce')
    87              elif 'close' in df.columns:
    88                  close = pd.to_numeric(df['close'], errors='coerce')
    89              else:
    90                  logger.warning("æ‰¾ä¸åˆ°å¯ç”¨çš„åƒ¹æ ¼æ¬„ä½ä¾†è¨ˆç®— ATR")
    91                  return pd.Series(index=df.index, dtype=float)
    92
    93              price_change = close.diff().abs()
    94              atr = price_change.rolling(window=window).mean()
    95
    96          # æª¢æŸ¥è¨ˆç®—çµæœ
    97          if atr is None or atr.empty or atr.isna().all():
    98              logger.warning(f"ATR è¨ˆç®—çµæœç„¡æ•ˆï¼Œwindow={window}")
    99              return pd.Series(index=df.index, dtype=float)
   100
   101          return atr
   102      except Exception as e:
   103          logger.warning(f"ATR è¨ˆç®—å¤±æ•—: {e}")
   104          return pd.Series(index=df.index, dtype=float)
   105
   106  def calculate_equity_curve(open_px, w, cap, atr_ratio):
   107      """è¨ˆç®—æ¬Šç›Šæ›²ç·š"""
   108      try:
   109          # ç°¡åŒ–çš„æ¬Šç›Šæ›²ç·šè¨ˆç®—
   110          # é€™è£¡ä½¿ç”¨é–‹ç›¤åƒ¹å’Œæ¬Šé‡çš„ä¹˜ç©ä¾†æ¨¡æ“¬æ¬Šç›Šè®ŠåŒ–
   111          equity = (open_px * w * cap).cumsum()
   112          return equity
   113      except Exception as e:
   114          logger.warning(f"æ¬Šç›Šæ›²ç·šè¨ˆç®—å¤±æ•—: {e}")
   115          return None
   116
   117  def calculate_trades_from_equity(equity_curve, open_px, w, cap, atr_ratio):
   118      """å¾æ¬Šç›Šæ›²ç·šè¨ˆç®—äº¤æ˜“è¨˜éŒ„"""
   119      try:
   120          if equity_curve is None or equity_curve.empty:
I'll grab lines starting from 897 to capture the updated metrics recomputation.

root@5583bff288d6:/workspace/SSS096# nl -ba app_dash.py | sed -n '888,960p'
   888                  except Exception:
   889                      pass
   890
   891                  if force_trigger:
   892                      valve_triggered_local = True
   893                      if ratio_local is None:
   894                          ratio_local = 1.5
   895
   896                  if valve_triggered_local:
   897                      from SSS_EnsembleTab import risk_valve_backtest, CostParams, perf_stats
   898                      # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
   899                      open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
   900                      # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
   901                      w_series = None
   902                      try:
   903                          ds_std = df_from_pack(result.get('daily_state_std'))
   904                          if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
   905                              w_series = ds_std['w']
   906                      except Exception:
   907                          pass
   908                      if w_series is None:
   909                          ds = df_from_pack(result.get('daily_state'))
   910                          if ds is not None and not ds.empty and 'w' in ds.columns:
   911                              w_series = ds['w']
   912
   913                      if w_series is not None:
   914                          # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
   915                          trade_cost = strat_params.get('trade_cost', {})
   916                          cost_params = CostParams(
   917                              buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
   918                              sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
   919                              sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
   920                          )
   921
   922                          rv = risk_valve_backtest(
   923                              open_px=open_px,
   924                              w=w_series,
   925                              cost=cost_params,
   926                              benchmark_df=df_raw,
   927                              mode="cap",
   928                              cap_level=float(risk_cap),
   929                          )
   930
   931                          # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
   932                          result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
   933                          result['daily_state']      = pack_df(rv["daily_state_valve"])
   934                          result['daily_state_std']  = pack_df(rv["daily_state_valve"])
   935                          result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
   936                          result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
   937                          result['weight_curve']     = pack_series(rv["weights_valve"])
   938                          result['metrics']          = perf_stats(rv["daily_state_valve"]["equity"], rv["weights_valve"])
   939                          # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
   940                          result['valve'] = {
   941                              "applied": True,
   942                              "cap": float(risk_cap),
   943                              "atr_ratio": ratio_local
   944                          }
   945
   946                          logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
   947                      else:
   948                          logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
   949                  else:
   950                      logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
   951                      # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
   952                      result['valve'] = {
   953                          "applied": False,
   954                          "cap": float(risk_cap),
   955                          "atr_ratio": ratio_local if ratio_local is not None else "N/A"
   956                      }
   957          elif strat_type == 'ensemble':
   958              # ä½¿ç”¨æ–°çš„ ensemble_runner é¿å…å¾ªç’°ä¾è³´
   959              try:
   960                  from runners.ensemble_runner import run_ensemble_backtest
I'll capture the lines where the ensemble branch modifications occur.

root@5583bff288d6:/workspace/SSS096# nl -ba app_dash.py | sed -n '1060,1150p'
  1060                              logger.warning(f"[{strat}] Ensemble ATR æ•¸æ“šä¸è¶³")
  1061                      else:
  1062                          logger.warning(f"[{strat}] Ensemble ATR è¨ˆç®—çµæœç‚ºç©º")
  1063
  1064                  except Exception as e:
  1065                      logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— Ensemble ATR æ¯”å€¼: {e}")
  1066                      logger.warning(f"[{strat}] éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
  1067
  1068                  # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
  1069                  if force_trigger:
  1070                      valve_triggered = True
  1071                      logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
  1072                      if ratio is None:
  1073                          ratio = 1.5  # è¨­å®šä¸€å€‹é è¨­å€¼ç”¨æ–¼é¡¯ç¤º
  1074
  1075                  # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
  1076                  backtest_result = run_ensemble_backtest(cfg)
  1077
  1078                  valve_info = None
  1079                  # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
  1080                  if global_apply and valve_triggered:
  1081                      from SSS_EnsembleTab import risk_valve_backtest, perf_stats
  1082                      bench = df_raw  # å·²å« open/high/low/close/volume
  1083
  1084                      logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
  1085
  1086                      rv = risk_valve_backtest(
  1087                          open_px=backtest_result.price_series,
  1088                          w=backtest_result.weight_curve,
  1089                          cost=cost_params,
  1090                          benchmark_df=bench,
  1091                          mode="cap",
  1092                          cap_level=float(risk_cap),
  1093                      )
  1094                      # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
  1095                      backtest_result.daily_state = rv["daily_state_valve"]
  1096                      backtest_result.ledger = rv["trade_ledger_valve"]
  1097                      backtest_result.weight_curve = rv["weights_valve"]
  1098                      backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
  1099                      backtest_result.stats = perf_stats(backtest_result.equity_curve, backtest_result.weight_curve)
  1100                      logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
  1101
  1102                      # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
  1103                      if "metrics" in rv:
  1104                          logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={
rv['metrics'].get('pf_valve', 'N/A'):.2f}")
  1105                          logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥
é–€={rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
  1106
  1107                      valve_info = {
  1108                          "applied": True,
  1109                          "cap": float(risk_cap),
  1110                          "atr_ratio": ratio,
  1111                      }
  1112                  else:
  1113                      if global_apply:
  1114                          logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
  1115                          valve_info = {
  1116                              "applied": False,
  1117                              "cap": float(risk_cap),
  1118                              "atr_ratio": ratio if ratio is not None else "N/A",
  1119                          }
  1120                      else:
  1121                          logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
  1122                          valve_info = {
  1123                              "applied": False,
  1124                              "cap": "N/A",
  1125                              "atr_ratio": "N/A",
  1126                          }
  1127
  1128                  # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
  1129                  result = {
  1130                      'trades': [],
  1131                      'trade_df': pack_df(backtest_result.trades),
  1132                      'trades_df': pack_df(backtest_result.trades),
  1133                      'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type'
: 'action'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
  1134                      'metrics': backtest_result.stats,
  1135                      'equity_curve': pack_series(backtest_result.equity_curve),
  1136                      'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else
 "",
  1137                      'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not Non
e else pack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
  1138                      'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not Non
e else pack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
  1139                      'daily_state': pack_df(backtest_result.daily_state),
  1140                      'trade_ledger': pack_df(backtest_result.ledger),
  1141                      'daily_state_std': pack_df(backtest_result.daily_state),
  1142                      'trade_ledger_std': pack_df(backtest_result.ledger)
  1143                  }
  1144                  if valve_info is not None:
  1145                      result['valve'] = valve_info
  1146
  1147                  logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: æ¬Šç›Šæ›²ç·šé•·åº¦={len(backtest_result.equity_curve)}, äº¤æ˜“æ•¸={len(backtes
t_result.ledger) if backtest_result.ledger is not None and not backtest_result.ledger.empty else 0}")
  1148
  1149              except Exception as e:
  1150                  logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
I'm finalizing the PR title and message, summarizing the user report and changes applied.

Iâ€™ll summarize the fix, testing, and include citations for SSMA and ensemble branches.

3.
Configuring language runtimes...
# Python: 3.12
# Node.js: v20 (default: v22)
default -> 20 (-> v20.19.4)
Now using node v20.19.4 (npm v10.9.3)
# Ruby: 3.4.4 (default: 3.2.3)
mise ~/.config/mise/config.toml tools: ruby@3.4.4
# Rust: 1.88.0 (default: 1.89.0)
info: using existing install for '1.88.0-x86_64-unknown-linux-gnu'
info: default toolchain set to '1.88.0-x86_64-unknown-linux-gnu'

  1.88.0-x86_64-unknown-linux-gnu unchanged - rustc 1.88.0 (6b00bc388 2025-06-23)

# Go: go1.24.3 (default: go1.24.3)
# Swift: 6.1 (default: 6.1)
# PHP: 8.4 (default: 8.4)
Running auto setup...
Scanning repository (up to depth 3) for dependency configurations...
No known dependency configuration files found up to depth 3.
å¾Œé¢ç‰ˆæœ¬ä¸æ˜¯æ–°å¢äº†é¢¨éšªé–¥é–€å¥—ç”¨åœ¨å„ç­–ç•¥å—
ä½†ä¸çŸ¥é“ç‚ºä»€éº¼å¥—ç”¨å¾Œ å„æŒ‡æ¨™æœƒåªå‰©ä¸‹å¹³å‡æŒå€‰å¤©æ•¸,å…¶ä»–æŒ‡æ¨™éƒ½æ¶ˆå¤±äº† è«‹ä¿®å¾©
Opening a new session to explore the repo and locate AGENTS.md, then I'll investigate further.

ls
I think the shell isn't showing results since it's non-interactive. I'll adjust timing and retry.

root@9a3d3c80743e:/workspace# ls
SSS096
Iâ€™ll change into the SSS096 directory now. Letâ€™s get started!

root@9a3d3c80743e:/workspace# cd SSS096
root@9a3d3c80743e:/workspace/SSS096# ls
AGENTS.md                           data                      run_workflow_example.py
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md  debug_enhanced_data.py    runners
ENSEMBLE_WORKFLOW.md                ensemble_wrapper.py       setup.sh
LOGGING_SYSTEM_UPDATE_SUMMARY.md    extract_params.py         sss_backtest_outputs
PORTFOLIO_LEDGER_README.md          leverage.py               sss_core
SSS_EnsembleTab.py                  list.txt                  test
SSSv096.py                          list_folder_structure.py  tools
analysis                            presets                   utils_payload.py
app_dash.py                         ruff.toml                 version_history.py
assets                              run_enhanced_debug.py     workflow_summary_20250812_023903.txt
config.yaml                         run_enhanced_ensemble.py  workflow_summary_20250818_040656.txt
convert_results_to_trades.py        run_workflow.py
I'll open AGENTS.md to review its instructions before making any changes.

convert_results_to_trades.py        run_workflow.py
root@9a3d3c80743e:/workspace/SSS096# cat AGENTS.md
# SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

SSS096 æ˜¯ä¸€å€‹è‚¡ç¥¨ç­–ç•¥å›æ¸¬èˆ‡åˆ†æç³»çµ±ï¼Œä¸»è¦åŒ…å«ï¼š
- ç­–ç•¥å›æ¸¬å¼•æ“ï¼ˆSSSv096.pyï¼‰
- Web UI ç•Œé¢ï¼ˆapp_dash.pyï¼‰
- å¢å¼·åˆ†ææ¨¡çµ„ï¼ˆanalysis/ï¼‰
- Ensemble ç­–ç•¥åŸ·è¡Œï¼ˆrunners/ï¼‰
- æ•¸æ“šè™•ç†èˆ‡è½‰æ›å·¥å…·

## ğŸ¯ å·¥ä½œé‡é»å€åŸŸ

### æ ¸å¿ƒæª”æ¡ˆ
- `SSSv096.py` - ä¸»è¦ç­–ç•¥å›æ¸¬å¼•æ“
- `app_dash.py` - Web UI ä¸»æ‡‰ç”¨
- `ensemble_wrapper.py` - Ensemble ç­–ç•¥åŒ…è£å™¨
- `analysis/` - åˆ†ææ¨¡çµ„ç›®éŒ„

### é¿å…ä¿®æ”¹çš„æª”æ¡ˆ
- `tools/quick_check.ps1` - è‡ªå‹•åŒ–æª¢æŸ¥è…³æœ¬ï¼ˆé™¤éå¿…è¦ï¼‰
- å·²æ¨™è¨˜ç‚º "past/" çš„èˆŠç‰ˆæœ¬æª”æ¡ˆ
- ç·¨è­¯å¾Œçš„ `.pyc` æª”æ¡ˆ

## ğŸ”§ é–‹ç™¼ç’°å¢ƒè¨­å®š

### Codex ç’°å¢ƒè¨­ç½®ï¼ˆæ¨è–¦ï¼‰
```bash
# åŸ·è¡Œè‡ªå‹•è¨­ç½®è…³æœ¬
chmod +x setup.sh
./setup.sh

# æ¸¬è©¦è¨­ç½®æ˜¯å¦æˆåŠŸ
python test_setup.py
```

### Python ç’°å¢ƒï¼ˆæ‰‹å‹•è¨­ç½®ï¼‰
```bash
# å®‰è£ä¾è³´å¥—ä»¶
pip install pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components

# æª¢æŸ¥ Python ç‰ˆæœ¬ï¼ˆå»ºè­° 3.8+ï¼‰
python --version
```

### ä»£ç†è¨­ç½®ï¼ˆå¦‚æœé‡åˆ° 403 éŒ¯èª¤ï¼‰
```bash
# æª¢æŸ¥ä»£ç†è­‰æ›¸
echo $CODEX_PROXY_CERT

# é…ç½® pip ä½¿ç”¨ä»£ç†
pip config set global.cert "$CODEX_PROXY_CERT"
pip config set global.trusted-host "proxy:8080"
```

### å°ˆæ¡ˆçµæ§‹å°èˆª
```bash
# å¿«é€ŸæŸ¥çœ‹ç›®éŒ„çµæ§‹
python list_folder_structure.py

# æŸ¥çœ‹ç‰¹å®šç›®éŒ„å…§å®¹
ls analysis/
ls runners/
```

## ğŸ§ª æ¸¬è©¦èˆ‡é©—è­‰

### å¿«é€Ÿæª¢æŸ¥
```bash
# åŸ·è¡Œè‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆé‡è¦ï¼ï¼‰
powershell -ExecutionPolicy Bypass -File tools\quick_check.ps1
```

### å›æ¸¬æ¸¬è©¦
```bash
# åŸ·è¡Œå–®ä¸€ç­–ç•¥å›æ¸¬
python SSSv096.py --strategy RMA_Factor --param_preset op.json

# åŸ·è¡Œ Ensemble ç­–ç•¥
python run_enhanced_ensemble.py --method majority --top_k 5
```

### UI æ¸¬è©¦
```bash
# å•Ÿå‹• Web UI
python app_dash.py
```

## ğŸ“ ç¨‹å¼ç¢¼è¦ç¯„

### è¨»è§£èˆ‡è¼¸å‡º
- **ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**é€²è¡Œè¨»è§£å’Œè¼¸å‡º
- ä¿®æ”¹ç´€éŒ„éœ€åŠ å…¥æ—¥æœŸæ™‚é–“æˆ³è¨˜
- è·¯å¾‘èªªæ˜æ ¼å¼ï¼š`#å­è³‡æ–™å¤¾/æª”æ¡ˆå`

### æ—¥èªŒè¨˜éŒ„
- ä½¿ç”¨ `analysis/logging_config.py` ä¸­çš„æ—¥èªŒå™¨
- é‡è¦æ“ä½œéœ€è¨˜éŒ„åˆ°æ—¥èªŒæª”æ¡ˆ
- éŒ¯èª¤è™•ç†éœ€åŒ…å«è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯

### è³‡æ–™æ ¼å¼
- æ—¥æœŸæ¬„ä½çµ±ä¸€ä½¿ç”¨ ISO æ ¼å¼ï¼š`YYYY-MM-DD`
- æ•¸å€¼æ¬„ä½ä½¿ç”¨ float é¡å‹
- é¿å…ä½¿ç”¨ä¸­æ–‡æ¬„ä½åç¨±ï¼ˆé™¤éå¿…è¦ï¼‰

## ğŸ” é™¤éŒ¯æŒ‡å—

### å¸¸è¦‹å•é¡Œ
1. **æ¨¡çµ„å°å…¥å¤±æ•—**ï¼šæª¢æŸ¥ `sys.path` å’Œç›¸å°å°å…¥
2. **æ•¸æ“šæ ¼å¼éŒ¯èª¤**ï¼šé©—è­‰ CSV æª”æ¡ˆçµæ§‹å’Œæ¬„ä½åç¨±
3. **è¨˜æ†¶é«”ä¸è¶³**ï¼šæª¢æŸ¥å¤§æ•¸æ“šé›†çš„è™•ç†æ–¹å¼

### é™¤éŒ¯å·¥å…·
```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(level=logging.DEBUG)

# æª¢æŸ¥æ•¸æ“šçµæ§‹
print(df.info())
print(df.head())
```

## ğŸ“Š æ•¸æ“šè™•ç†è¦ç¯„

### è¼¸å…¥æ•¸æ“š
- æ”¯æ´ CSVã€Excelã€JSON æ ¼å¼
- å¿…è¦æ¬„ä½ï¼šäº¤æ˜“æ—¥æœŸã€æ¬Šé‡è®ŠåŒ–ã€ç›ˆè™§%
- å¯é¸æ¬„ä½ï¼šäº¤æ˜“é¡å‹ã€åƒ¹æ ¼ã€æˆäº¤é‡

### è¼¸å‡ºæ•¸æ“š
- çµ±ä¸€ Schemaï¼šequityã€tradesã€daily_stateã€trade_ledger
- é¿å… KeyError å’Œæ¬„ä½ç¼ºå¤±
- æ”¯æ´å¤šç¨®è¼¸å‡ºæ ¼å¼

## ğŸš€ éƒ¨ç½²èˆ‡ç¶­è­·

### æª”æ¡ˆç®¡ç†
- å®šæœŸæ¸…ç†èˆŠçš„æ—¥èªŒå’Œå¿«å–æª”æ¡ˆ
- å‚™ä»½é‡è¦çš„é…ç½®å’Œçµæœæª”æ¡ˆ
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶è¿½è¹¤è®Šæ›´

### æ€§èƒ½å„ªåŒ–
- å¤§æ•¸æ“šé›†ä½¿ç”¨å¿«å–æ©Ÿåˆ¶
- é¿å…é‡è¤‡è¨ˆç®—
- ä½¿ç”¨é©ç•¶çš„æ•¸æ“šçµæ§‹

## âš ï¸ æ³¨æ„äº‹é …

### å®‰å…¨è€ƒé‡
- ä¸è¦ç¡¬ç·¨ç¢¼ API é‡‘é‘°
- é©—è­‰æ‰€æœ‰ç”¨æˆ¶è¼¸å…¥
- ä¿è­·æ•æ„Ÿæ•¸æ“š

### ç›¸å®¹æ€§
- ç¶­æŒèˆ‡ç¾æœ‰å·¥ä½œæµç¨‹çš„ç›¸å®¹æ€§
- æ¸¬è©¦æ‰€æœ‰æ•´åˆé»
- é¿å…ç ´å£ç¾æœ‰åŠŸèƒ½

## ğŸš€ Codex ç’°å¢ƒè¨­ç½®

### è‡ªå‹•è¨­ç½®
```bash
# åŸ·è¡Œè¨­ç½®è…³æœ¬
./setup.sh

# è…³æœ¬æœƒè‡ªå‹•ï¼š
# 1. é…ç½®ä»£ç†è¨­ç½®
# 2. å®‰è£æ‰€æœ‰å¿…è¦ä¾è³´
# 3. å‰µå»ºå›é€€æ—¥èªŒç³»çµ±
# 4. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```

### æ‰‹å‹•è¨­ç½®ï¼ˆå¦‚æœè‡ªå‹•è¨­ç½®å¤±æ•—ï¼‰
```bash
# 1. é…ç½®ä»£ç†
export PIP_CERT="$CODEX_PROXY_CERT"
export NODE_EXTRA_CA_CERTS="$CODEX_PROXY_CERT"

# 2. å®‰è£æ ¸å¿ƒä¾è³´
pip install --no-cache-dir pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components yfinance pyyaml joblib

# 3. å®‰è£åˆ†æå¥—ä»¶
pip install --no-cache-dir scikit-learn scipy statsmodels plotly kaleido

# 4. å‰µå»ºå¿…è¦ç›®éŒ„
mkdir -p analysis/log analysis/cache cache log results sss_backtest_outputs
```

### å¸¸è¦‹å•é¡Œè§£æ±º
- **pip install 403 éŒ¯èª¤**ï¼šæª¢æŸ¥ `$CODEX_PROXY_CERT` ç’°å¢ƒè®Šæ•¸
- **joblib å°å…¥å¤±æ•—**ï¼šä½¿ç”¨ `logging_config_fallback.py` å›é€€ç‰ˆæœ¬
- **æ¨¡çµ„è·¯å¾‘å•é¡Œ**ï¼šè¨­ç½® `PYTHONPATH` ç’°å¢ƒè®Šæ•¸

## ğŸ“ æŠ€è¡“æ”¯æ´

### å•é¡Œå›å ±
- æä¾›å®Œæ•´çš„éŒ¯èª¤è¨Šæ¯å’Œå †ç–Šè¿½è¹¤
- åŒ…å«é‡ç¾æ­¥é©Ÿå’Œç’°å¢ƒä¿¡æ¯
- æª¢æŸ¥ç›¸é—œçš„æ—¥èªŒæª”æ¡ˆ

### æ–‡æª”æ›´æ–°
- ä¿®æ”¹åŠŸèƒ½æ™‚åŒæ­¥æ›´æ–°ç›¸é—œæ–‡æª”
- ä½¿ç”¨æ¸…æ™°çš„ç¯„ä¾‹å’Œèªªæ˜
- ä¿æŒæ–‡æª”çš„æ™‚æ•ˆæ€§

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€å¾Œæ›´æ–°ï¼š** 2025-08-18
**é©ç”¨æ–¼ï¼š** SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—
I need to locate where the risk valve is applied in the code. Let's search!

root@9a3d3c80743e:/workspace/SSS096# rg "é¢¨éšªé–¥" -n
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md
36:- åœ¨ç¹ªè£½é¢¨éšªé–¥é–€æ™‚åºåœ–å‰æª¢æŸ¥å¿…è¦æ¬„ä½

analysis/UI_INTEGRATION_GUIDE.md
11:1. é¢¨éšªé–¥é–€å›æ¸¬
260:- **è‡ªå®šç¾©åŸºæº–æ•¸æ“š**ï¼šä¸Šå‚³åŸºæº–æ•¸æ“šé€²è¡Œé¢¨éšªé–¥é–€åˆ†æ

analysis/ENHANCED_ANALYSIS_README.md
11:1. **é¢¨éšªé–¥é–€å›æ¸¬** - åœ¨æ¥µç«¯è¡Œæƒ…ä¸‹æš«åœåŠ ç¢¼æ©Ÿåˆ¶
17:### 1. é¢¨éšªé–¥é–€å›æ¸¬ (`risk_valve_backtest`)
141:- é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åºåœ–
184:### 1. é¢¨éšªé–¥é–€ç­–ç•¥

analysis/enhanced_trade_analysis.py
4:æ•´åˆé¢¨éšªé–¥é–€å›æ¸¬ã€äº¤æ˜“è²¢ç»æ‹†è§£ã€åŠ ç¢¼æ¢¯åº¦å„ªåŒ–
97:        é¢¨éšªé–¥é–€å›æ¸¬
109:        print("=== é¢¨éšªé–¥é–€å›æ¸¬åˆ†æ ===")
112:        # æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼
115:        # è¨ˆç®—é¢¨éšªé–¥é–€æ•ˆæœ
121:        """æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼"""
123:            print("è­¦å‘Šï¼šç¼ºå°‘åŸºæº–æ•¸æ“šï¼Œç„¡æ³•é€²è¡Œé¢¨éšªé–¥é–€å›æ¸¬")
179:        # é¢¨éšªé–¥é–€è§¸ç™¼æ¢ä»¶
184:        # ç¶œåˆé¢¨éšªé–¥é–€
194:        """è¨ˆç®—é¢¨éšªé–¥é–€å°ç¸¾æ•ˆçš„å½±éŸ¿"""
200:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'æ—¥æœŸ' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
204:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
207:        # æ‰¾å‡ºé¢¨éšªé–¥é–€è§¸ç™¼çš„æœŸé–“
243:        print(f"é¢¨éšªé–¥é–€è§¸ç™¼æœŸé–“æ•¸ï¼š{len(risk_periods)}")
555:        # 1. é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº
559:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
561:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
575:                               color='red', s=50, alpha=0.8, label='é¢¨éšªé–¥é–€è§¸ç™¼')
578:                ax1.set_title('é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº')
637:    print("2. åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬ï¼šanalyzer.risk_valve_backtest()")

SSS_EnsembleTab.py
1287:# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
1292:    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š

app_dash.py
445:                        html.Label("é¢¨éšªé–¥é–€ CAP", style={"fontSize":"12px","color":"#888"}),
455:                        dbc.Checkbox(id='force-valve-trigger', value=False, label="å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼ˆæ¸¬è©¦ç”¨ï¼‰", style={"font
Size":"11px","color":"#dc3545"}),
456:                        html.Small("ğŸ’¡ å‹¾é¸å¾Œå°‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼Œç”¨æ–¼æ¸¬è©¦åŠŸèƒ½", style={"color":"#dc3545","fontSize":"10px"})
460:                    # === é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤ºå€åŸŸ ===
464:                        html.Small("ğŸ”’ é¢¨éšªé–¥é–€èªªæ˜:", style={"color":"#28a745","fontWeight":"bold","fontSize":"11px"}),
573:# --------- é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ---------
587:    """å‹•æ…‹æ›´æ–°é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤º"""
588:    logger.info(f"=== é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ===")
598:        logger.info("é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
600:            html.Small("ğŸ”´ é¢¨éšªé–¥é–€æœªå•Ÿç”¨", style={"color":"#dc3545","fontWeight":"bold"}),
655:                            # åˆ¤æ–·æ˜¯å¦éœ€è¦è§¸ç™¼é¢¨éšªé–¥é–€
658:                            # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
661:                                logger.info(f"å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
663:                            # è¨˜éŒ„é¢¨éšªé–¥é–€ç‹€æ…‹åˆ°æ—¥èªŒ
665:                            logger.info(f"é¢¨éšªé–¥é–€é–€æª»: {atr_ratio}, ç•¶å‰æ¯”å€¼: {atr_ratio_current:.4f}")
666:                            logger.info(f"é¢¨éšªé–¥é–€è§¸ç™¼: {'æ˜¯' if valve_triggered else 'å¦'}")
667:                            logger.info(f"é¢¨éšªé–¥é–€ç‹€æ…‹: {'ğŸ”´ è§¸ç™¼' if valve_triggered else 'ğŸŸ¢ æ­£å¸¸'}")
680:                                    html.Small(f"{status_icon} é¢¨éšªé–¥é–€ç‹€æ…‹: {status_text}",
742:        logger.error(f"é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–°å¤±æ•—: {e}")
797:    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
810:        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
811:        logger.info(f"[{strat}] é¢¨éšªé–¥é–€é–‹é—œç‹€æ…‹: global_apply={global_apply}, é¡å‹={type(global_apply)}")
813:            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")
836:                        logger.warning(f"[{strat}] ATR(60) å€¼ç•°å¸¸: {atr_60_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
840:                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
851:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
856:            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")
864:            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
870:            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
941:                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
943:                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
945:                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
981:                # è¨»è§£æ‰åŸæœ¬çš„ç„¡æ¢ä»¶é¢¨éšªé–¥é–€èª¿æ•´ï¼ˆæœƒé€ æˆ floor æ–¹å‘éŒ¯èª¤ï¼‰
983:                #     logger.info(f"[{strat}] Ensemble ç­–ç•¥æ‡‰ç”¨é¢¨éšªé–¥é–€: åŸå§‹ delta_cap={ensemble_params.delta_cap}, floor={
ensemble_params.floor}")
1020:                # --- æ–°å¢ï¼šåªåœ¨ ATR è§¸ç™¼æ™‚å•Ÿç”¨é¢¨éšªé–¥é–€ ---
1047:                                # å¢åŠ é¢¨éšªé–¥é–€è§¸ç™¼çš„è©³ç´°è³‡è¨Š
1049:                                    logger.info(f"[{strat}] ğŸ”´ é¢¨éšªé–¥é–€è§¸ç™¼ï¼ATRæ¯”å€¼({ratio:.4f}) > é–€æª»({atr_ratio})")
1051:                                    logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼ŒATRæ¯”å€¼({ratio:.4f}) <= é–€æª»({atr_ratio})")
1063:                # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
1066:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
1078:                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
1093:                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
1095:                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1108:                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
1116:                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
1175:                    "applied": False,  # å…¶ä»–ç­–ç•¥é¡å‹æš«æ™‚ä¸æ”¯æ´é¢¨éšªé–¥é–€
1205:        # === å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨ï¼ˆèˆ‡å¢å¼·åˆ†æä¸€è‡´ï¼‰ ===
1211:                logger.warning(f"[{strat}] daily_state ä¸å« 'w'ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1217:                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— ATR20/60ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1316:                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
1318:                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")
1701:            # === è¨ˆç®—é¢¨éšªé–¥é–€å¾½ç« å…§å®¹ ===
1713:                "ğŸ›¡ï¸ é¢¨éšªé–¥é–€ï¼š" + valve_badge_text + ((" | " + " | ".join(valve_badge_extra)) if valve_badge_extra else ""),
1968:                html.Summary("é¢¨éšªé–¥é–€å›æ¸¬"),
1981:                    html.Button("åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬", id="run-rv", n_clicks=0, style={"marginTop":"8px"})
2369:# --------- å¢å¼·åˆ†æ Callbackï¼šé¢¨éšªé–¥é–€å›æ¸¬ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---------
2434:        return f"é¢¨éšªé–¥é–€å›æ¸¬åŸ·è¡Œå¤±æ•—: {e}", no_update, no_update
3488:        return go.Figure(), "âŒ ç„¡æ³•å¾é¢¨éšªé–¥é–€å›æ¸¬çš„åƒæ•¸çµ„åˆä¸­å–å¾—è³‡æ–™"
I spotted the issue with missing metrics after applying the risk valve. I'll locate the function calculating those metrics.

root@9a3d3c80743e:/workspace/SSS096# rg "å¹³å‡æŒ" -n
app_dash.py
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1888:                'å¹³å‡æŒæœ‰å¤©æ•¸': f"{detailed_stats['avg_holding_days']:.1f}",
2213:    # è¨ˆç®—å¹³å‡æŒæœ‰å¤©æ•¸ï¼ˆè²·å…¥åˆ°è³£å‡ºçš„å¤©æ•¸ï¼‰

SSSv096.py
927:    è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸.
933:        float: å¹³å‡æŒå€‰å¤©æ•¸.
1828:            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1864:    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
1867:    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
2293:            # é¡¯ç¤ºç­–ç•¥æ‘˜è¦,å‚³å…¥ trade_df ä»¥è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸

analysis/optuna_16.py
10:v10-12  æ”¯æ´å–®ä¸€/éš¨æ©Ÿ/ä¾åºæ•¸æ“šæºæ¨¡å¼,æ–°å¢å¹³å‡æŒå€‰å¤©æ•¸,ç›¸é—œä¿‚æ•¸åˆ†æ,ä¸¦è‡ªå‹•è¼¸å‡ºç†±åœ–,è©¦é©—çµæœèˆ‡æœ€ä½³åƒæ•¸è‡ªå‹•è¼¸å‡º CSV/JSON
254:        # è®¡ç®—å¹³å‡æŒä»“å¤©æ•°

analysis/OSv3.py
82:        'avg_hold_days': 'å¹³å‡æŒå€‰å¤©æ•¸ï¼šå¹³å‡æ¯æ¬¡äº¤æ˜“çš„æŒå€‰æ™‚é–“',
I'll look at the metrics calculation section in SSSv096 around line 1828.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1800,1870p' SSSv096.py
    # å…ˆæŠŠ metrics çš„ key/value è½‰æˆã€Œé¡¯ç¤ºç”¨çš„ labelã€å’Œã€Œæ ¼å¼åŒ–å¾Œçš„ value å­—ä¸²ã€
    items = []
    for k, v in metrics.items():
        # ä¾æ“š k æ±ºå®šè¦ä¸è¦ä»¥ç™¾åˆ†æ¯”ã€æˆ–å°æ•¸ã€æˆ–ç´”æ–‡å­—ä¾†æ ¼å¼åŒ–
        if k in ["total_return", "annual_return", "win_rate", "max_drawdown", "annualized_volatility", "avg_win", "avg_loss"]:
            txt = f"{v:.2%}" if pd.notna(v) else ""
        elif k in ["calmar_ratio", "sharpe_ratio", "sortino_ratio", "payoff_ratio", "profit_factor"]:
            txt = f"{v:.2f}" if pd.notna(v) else ""
        elif k in ["max_drawdown_duration", "avg_holding_period"]:
            txt = f"{v:.1f} å¤©" if pd.notna(v) else ""
        elif k in ["num_trades", "max_consecutive_wins", "max_consecutive_losses"]:
            txt = str(int(v)) if pd.notna(v) else ""
        else:
            # å…¶ä»–å°±å…ˆç›¡é‡ç•¶ç´”æ–‡å­—é¡¯ç¤º
            txt = f"{v}"
        # æŠŠå­—å…¸ key â†’ ä¸­æ–‡é¡¯ç¤º label
        label_map = {
            "total_return": "ç¸½å›å ±ç‡",
            "annual_return": "å¹´åŒ–å›å ±ç‡",
            "win_rate": "å‹ç‡",
            "max_drawdown": "æœ€å¤§å›æ’¤",
            "max_drawdown_duration": "å›æ’¤æŒçºŒ",
            "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
            "sharpe_ratio": "å¤æ™®æ¯”ç‡",
            "sortino_ratio": "ç´¢æè«¾æ¯”ç‡",
            "payoff_ratio": "ç›ˆè™§æ¯”",
            "profit_factor": "ç›ˆè™§å› å­",
            "num_trades": "äº¤æ˜“æ¬¡æ•¸",
            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
            "annualized_volatility": "å¹´åŒ–æ³¢å‹•ç‡",
            "max_consecutive_wins": "æœ€å¤§é€£çºŒç›ˆåˆ©",
            "max_consecutive_losses": "æœ€å¤§é€£çºŒè™§æ",
            "avg_win": "å¹³å‡ç›ˆåˆ©",
            "avg_loss": "å¹³å‡è™§æ",
        }
        label = label_map.get(k, k)
        items.append((label, txt))

    # é–‹å§‹ç”¢ç”Ÿ HTML:å¤–å±¤ä¸€å€‹ flex container,å…§å±¤æ¯çµ„(æŒ‡æ¨™+æ•¸å€¼) éƒ½æ˜¯ flex item
    html = """
<div style="display:flex;flex-wrap:wrap;gap:12px;align-items:flex-start;">
"""
    for label, val in items:
        html += f"""
  <div style="flex:0 1 150px;border:1px solid #444;border-radius:4px;padding:8px 12px;background:#1a1a1a;">
    <div style="font-size:14px;color:#aaa;">{label}</div>
    <div style="font-size:20px;font-weight:bold;color:#fff;margin-top:4px;">{val}</div>
  </div>
"""
    html += "</div>"

    # å»æ‰å¤šé¤˜ç¸®æ’,é¿å…é–‹é ­ç©ºç™½è¢«è§£è®€æˆ code block
    html = textwrap.dedent(html)

    st.markdown(html, unsafe_allow_html=True)

def display_strategy_summary(strategy: str, params: Dict, metrics: Dict, smaa_source: str, trade_df: pd.DataFrame):
    """
    é¡¯ç¤ºç­–ç•¥åƒæ•¸èˆ‡å›æ¸¬ç¸¾æ•ˆæ‘˜è¦,ä½¿ç”¨ HTML Flexbox å¡ç‰‡å±•ç¤º.
    """
    # åƒæ•¸å±•ç¤º
    param_display = {k: v for k, v in params.items() if k != "strategy_type"}
    st.write("**åƒæ•¸è¨­å®š**: " + ", ".join(f"{k}: {v}" for k, v in param_display.items()))

    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
    avg_holding_period = calculate_holding_periods(trade_df)

    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
    metrics['avg_holding_period'] = avg_holding_period

    # ç¸¾æ•ˆæŒ‡æ¨™å±•ç¤º
I'm looking at the code where the risk valve might be affecting the metrics, possibly overwriting them.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1070,1120p' SSS_EnsembleTab.py
            )

            cfg = RunConfig(
                ticker=ticker,
                method=method,
                params=params,
                cost=cost,
                majority_k_pct=majority_k_pct
            )

            # é‹è¡Œ ensemble
            with st.spinner("åŸ·è¡Œä¸­..."):
                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

            # é¡¯ç¤ºçµæœ
            st.success(f"åŸ·è¡Œå®Œæˆ: {method_name}")

            # ç¸¾æ•ˆæŒ‡æ¨™
            col6, col7, col8 = st.columns(3)
            with col6:
                st.metric("ç¸½å ±é…¬ç‡", f"{stats.get('total_return', 0):.2%}")
                st.metric("å¹´åŒ–å ±é…¬ç‡", f"{stats.get('annual_return', 0):.2%}")
            with col7:
                st.metric("æœ€å¤§å›æ’¤", f"{stats.get('max_drawdown', 0):.2%}")
                st.metric("å¤æ™®æ¯”ç‡", f"{stats.get('sharpe_ratio', 0):.2f}")
            with col8:
                st.metric("å¡ç‘ªæ¯”ç‡", f"{stats.get('calmar_ratio', 0):.2f}")
                st.metric("äº¤æ˜“æ¬¡æ•¸", stats.get('num_trades', 0))

            # === æ–°å¢ï¼šäº¤æ˜“æ˜ç´°é¡¯ç¤º ===
            st.subheader("äº¤æ˜“æ˜ç´° (trade_ledger)")
            if trade_ledger is not None and not trade_ledger.empty:
                # ä½¿ç”¨ normalize_trades_for_ui æ¨™æº–åŒ–äº¤æ˜“æ˜ç´°
                trades_ui = normalize_trades_for_ui(trade_ledger)

                # === æ–°å¢ï¼šåˆªé™¤è²»ç”¨æ¬„ä½ï¼ˆåƒ…UIå±¤ç§»é™¤é¡¯ç¤ºï¼‰ ===
                cols_to_hide = ['fee_buy','fee_sell','sell_tax','tax','shares_before','shares_after']
                trades_ui = trades_ui.drop(columns=[c for c in cols_to_hide if c in trades_ui.columns], errors='ignore')

                # === æ–°å¢ï¼šçµ±ä¸€æ•¸å­—æ ¼å¼ ===
                # åƒ¹æ ¼æ ¼å¼åŒ–
                if 'price' in trades_ui.columns:
                    trades_ui['price'] = trades_ui['price'].apply(
                        lambda x: f"{x:,.2f}" if pd.notnull(x) else ""
                    )

                # é‡‘é¡/æ•¸é‡/æ¬Šé‡ç­‰æ ¼å¼åŒ–
                for col in ['exec_notional','weight_change','w_before','w_after','delta_units',
                            'cash_after','equity_after','position_value']:
                    if col in trades_ui.columns:
                        trades_ui[col] = trades_ui[col].apply(
Let me look for the run_ensemble function to check its relevance.

root@9a3d3c80743e:/workspace/SSS096# rg "run_ensemble" -n
run_workflow.py
109:def run_ensemble(config, args, method):
222:        results['Majority Ensemble'] = run_ensemble(config, args, 'majority')
225:        results['Proportional Ensemble'] = run_ensemble(config, args, 'proportional')

runners/__init__.py
2:from .ensemble_runner import run_ensemble_backtest
4:__all__ = ['run_ensemble_backtest']

runners/ensemble_runner.py
13:def run_ensemble_backtest(cfg) -> BacktestResult:
17:        from SSS_EnsembleTab import run_ensemble, EnsembleParams, CostParams, RunConfig
20:        open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

PORTFOLIO_LEDGER_README.md
78:### 1. åœ¨ `run_ensemble()` ä¸­è‡ªåŠ¨ç”Ÿæˆ

run_enhanced_ensemble.py
185:def run_ensemble_with_strategies(method: str,
291:                    result = run_ensemble_with_strategies(method, strategies, params, cost_params, trades_dir)
418:            final_result = run_ensemble_with_strategies(
437:        result = run_ensemble_with_strategies(

app_dash.py
955:                from runners.ensemble_runner import run_ensemble_backtest
1071:                backtest_result = run_ensemble_backtest(cfg)

analysis/OSv3.py
720:def run_ensemble_strategy(params, ticker="00631L.TW"):
1384:    if st.button("ğŸš€ è¿è¡Œ Ensemble ç­–ç•¥", type="secondary", key="run_ensemble"):
1399:                equity_curve, trades, stats = run_ensemble_strategy(ensemble_params)

SSS_EnsembleTab.py
749:def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame
, pd.DataFrame]:
1009:    open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)
1082:                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

SSSv096.py
33:from SSS_EnsembleTab import RunConfig, EnsembleParams, CostParams, run_ensemble
1017:                open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(cfg)

ensemble_wrapper.py
6:é€™æ˜¯ä¸€å€‹è–„åŒ…è£ï¼Œä¸å¾©åˆ¶è¨ˆç®—æ ¸å¿ƒï¼Œç›´æ¥èª¿ç”¨ç¾æœ‰çš„ run_ensemble å‡½æ•¸ã€‚
18:from SSS_EnsembleTab import run_ensemble, RunConfig, EnsembleParams, CostParams
166:            open_px, w, trades, stats, method_name, equity, daily_state, trade_ledger = run_ensemble(config)
I'll focus on the run_ensemble function in SSS_EnsembleTab.py to find risk valve usage.

root@9a3d3c80743e:/workspace/SSS096# rg "def run_ensemble" -n SSS_EnsembleTab.py
749:def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame
, pd.DataFrame]:
root@9a3d3c80743e:/workspace/SSS096# sed -n '749,900p' SSS_EnsembleTab.py
def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame, pd
.DataFrame]:
    """å›å‚³ï¼š(open åƒ¹), (æ¯æ—¥æ¬Šé‡ w), (äº¤æ˜“ç´€éŒ„ trades), (ç¸¾æ•ˆæŒ‡æ¨™ dict), (æ–¹æ³•åç¨±), (æ¬Šç›Šæ›²ç·š), (æ¯æ—¥ç‹€æ…‹ daily_state), (äº¤æ˜“æµ
æ°´å¸³ trade_ledger)"""
    # è®€åƒ¹ï¼ˆOpenï¼‰
    px_path = DATA_DIR / f"{cfg.ticker.replace(':','_')}_data_raw.csv"
    px = _read_market_csv_auto(px_path)

    # èª¿è©¦ä¿¡æ¯ï¼šåƒ¹æ ¼æ•¸æ“š
    logger.info(f"[Ensemble] åƒ¹è³‡æ–™å¤©æ•¸={len(px)}, é¦–æœ«={px.index.min()}~{px.index.max()}")

    # æ¨æ–·ç­–ç•¥åˆ—è¡¨
    if cfg.strategies is None:
        # å„ªå…ˆä½¿ç”¨ trades_from_results_*.csvï¼ˆ120æª”ç­–ç•¥ï¼‰
        strat_names = []
        file_map = {}  # ç­–ç•¥å -> æ–‡ä»¶è·¯å¾‘çš„æ˜ å°„

        # å…ˆæ‰¾ trades_from_results_*.csv
        trades_files = list(OUT_DIR.glob("trades_from_results_*.csv"))
        if trades_files:
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_from_results_*.csv æ–‡ä»¶ï¼ˆ120æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_from_results_", "")
                strat_names.append(name)
                file_map[name] = f
        else:
            # æ‰¾ä¸åˆ°å†ä½¿ç”¨èˆŠçš„ trades_*.csvï¼ˆ11æª”ç­–ç•¥ï¼‰
            trades_files = list(OUT_DIR.glob("trades_*.csv"))
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_*.csv æ–‡ä»¶ï¼ˆ11æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_", "")
                strat_names.append(name)
                file_map[name] = f

        if not strat_names:
            raise ValueError(f"åœ¨ {OUT_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½•äº¤æ˜“æ–‡ä»¶")

        cfg.strategies = strat_names
        cfg.file_map = file_map

    # è¼‰å…¥æŒå€‰çŸ©é™£
    pos_df = load_positions_matrix(OUT_DIR, px.index, cfg.strategies, cfg.file_map)

    if pos_df.empty:
        raise ValueError("ç„¡æ³•è¼‰å…¥ä»»ä½•ç­–ç•¥çš„æŒå€‰æ•¸æ“š")

    N = pos_df.shape[1]  # ç­–ç•¥æ•¸é‡
    logger.info(f"[Ensemble] è¼‰å…¥ {N} å€‹ç­–ç•¥çš„æŒå€‰æ•¸æ“š")

    # è™•ç† majority_k åƒæ•¸
    if cfg.method.lower() == "majority":
        if cfg.majority_k_pct is not None:
            # ä½¿ç”¨æ¯”ä¾‹é–€æª»
            k_req = int(math.ceil(N * cfg.majority_k_pct))
            logger.info(f"[Ensemble] ä½¿ç”¨æ¯”ä¾‹é–€æª» majority_k_pct={cfg.majority_k_pct}, N={N}, è¨ˆç®—å¾—åˆ° K={k_req}")
        else:
            # ä½¿ç”¨å›ºå®š K å€¼
            k_req = cfg.params.majority_k
            logger.info(f"[Ensemble] ä½¿ç”¨å›ºå®šé–€æª» majority_k={k_req}, N={N}")

        # æ–¹æ¡ˆ Aï¼šå¤¾æ“ åˆ°åˆæ³•ç¯„åœ
        k_eff = max(1, min(int(k_req), N))

        # æ–¹æ¡ˆ Bï¼šè‹¥ k>n æ¡ç”¨å‹•æ…‹å¤šæ•¸æ±ºï¼ˆå»ºè­°ï¼‰ï¼šceil(N*0.5)
        if k_req > N:
            logger.warning(f"majority_k({k_req}) > N({N}); fallback to ceil(N/2)={math.ceil(N*0.5)}")
            k_eff = int(math.ceil(N*0.5))

        # ç”¨ dataclasses.replace è¦†å¯«åƒæ•¸å¾Œå†ç®—æ¬Šé‡èˆ‡å‘½å
        import dataclasses
        cfg.params = dataclasses.replace(cfg.params, majority_k=k_eff)

        # æª¢æŸ¥ N æ˜¯å¦è¶³å¤ ï¼ˆå¯é¸ä½†å»ºè­°ï¼‰
        if N < 8:
            logger.warning(f"ç­–ç•¥æ•¸é‡ N={N} < 8ï¼Œå¯èƒ½å½±éŸ¿ ensemble æ•ˆæœã€‚å»ºè­°ç¢ºä¿æœ‰è¶³å¤ çš„å­ç­–ç•¥ã€‚")

    # èª¿è©¦ä¿¡æ¯ï¼šå­ç­–ç•¥å¤šå€‰çµ±è¨ˆ
    S = pos_df.sum(axis=1)
    logger.info(f"[Ensemble] å¤šé ­è¨ˆæ•¸Såˆ†ä½ˆ: mean={S.mean():.2f}, 1%={S.quantile(0.01):.2f}, 99%={S.quantile(0.99):.2f}")

    # æ¬Šé‡
    if cfg.method.lower() == "majority":
        w = weights_majority(pos_df, cfg.params)
        # å‘½åä¹Ÿç”¨èª¿æ•´å¾Œçš„ k_eff èˆ‡ N
        method_name = f"Majority_{k_eff}_of_{N}"
    elif cfg.method.lower() == "proportional":
        w = weights_proportional(pos_df, cfg.params)
        method_name = f"Proportional_N{N}"
    else:
        raise ValueError("method å¿…é ˆæ˜¯ 'majority' æˆ– 'proportional'")

    # èª¿è©¦ä¿¡æ¯ï¼šæ¬Šé‡çµ±è¨ˆ
    w_raw = (S >= cfg.params.majority_k).astype(float) if cfg.method.lower() == 'majority' else (S / max(N, 1))
    logger.info(f"[Ensemble] w_raw(min/mean/max)={w_raw.min():.2f}/{w_raw.mean():.2f}/{w_raw.max():.2f}")
    logger.info(f"[Ensemble] w_smooth(min/mean/max)={w.min():.2f}/{w.mean():.2f}/{w.max():.2f}")

    # æ¬Šç›Šèˆ‡äº‹ä»¶ï¼ˆOpenâ†’Openï¼‰
    equity, trades, daily_state, trade_ledger = equity_open_to_open(px["open"], w, cfg.cost, start_equity=1.0)

    # ä»¥é–‹ç›¤åƒ¹æ¨¡æ“¬è³‡ç”¢æ›²ç·š
    open_px = px['open'].copy()  # ä½ ç¾æœ‰çš„åƒ¹æ ¼åˆ—
    daily_state, trade_ledger = build_portfolio_ledger(
        open_px=open_px,
        w=w,                    # ä½ è¦è½åœ°çš„æœ€çµ‚æ¬Šé‡åºåˆ—ï¼ˆå·²ç¶“é floor/ema/delta_cap/cooldownï¼‰
        cost=cfg.cost,          # ä½ çš„ CostParams
        initial_capital=1_000_000.0,
        lot_size=None            # è‹¥è¦æ•´è‚¡å°±çµ¦ 1000 or 1ï¼Œå¦å‰‡ None å…è¨±å°æ•¸
    )

    # çµ±ä¸€æ¬„ä½æ ¼å¼ï¼ˆé¿å…ä¹‹å¾Œåºåˆ—åŒ–éº»ç…©ï¼‰
    daily_state = daily_state.copy()
    daily_state.index = pd.to_datetime(daily_state.index)
    daily_state.index.name = 'date'
    for col in ['equity', 'cash', 'position_value', 'invested_pct', 'cash_pct', 'w']:
        if col not in daily_state.columns:
            daily_state[col] = np.nan

    # èª¿è©¦ä¿¡æ¯ï¼šOpenâ†’Open å ±é…¬çµ±è¨ˆ
    r_oo = (px['open'].shift(-1) / px['open'] - 1).dropna()
    logger.info(f"[Ensemble] r_oo mean={r_oo.mean():.4f}, std={r_oo.std():.4f}, count={len(r_oo)}")

    # ç¸¾æ•ˆæŒ‡æ¨™
    stats = perf_stats(equity, w)

    # èª¿è©¦ä¿¡æ¯ï¼šç¸¾æ•ˆæ‘˜è¦
    logger.info(f"[Ensemble] ç¸¾æ•ˆæ‘˜è¦: ç¸½å ±é…¬={stats.get('total_return', 0):.4f}, å¹´åŒ–={stats.get('annual_return', 0):.4f}, æœ€å¤§
å›æ’¤={stats.get('max_drawdown', 0):.4f}")

    # æ¨™æº–åŒ–äº¤æ˜“æ˜ç´°ç‚ºçµ±ä¸€å¥‘ç´„
    trades_ui = normalize_trades_for_ui(trades)
    trade_ledger_ui = normalize_trades_for_ui(trade_ledger)

    # èª¿è©¦ä¿¡æ¯ï¼šæ¨™æº–åŒ–å¾Œçš„äº¤æ˜“æ˜ç´°
    logger.info(f"[Ensemble] æ¨™æº–åŒ–å¾Œ trades_ui æ¬„ä½: {list(trades_ui.columns)}")
    logger.info(f"[Ensemble] æ¨™æº–åŒ–å¾Œ trade_ledger_ui æ¬„ä½: {list(trade_ledger_ui.columns)}")
    logger.info(f"[Ensemble] äº¤æ˜“ç­†æ•¸: trades={len(trades_ui)}, trade_ledger={len(trade_ledger_ui)}")

    # ä¿å­˜èª¿è©¦ä¿¡æ¯åˆ°æ–‡ä»¶
    debug_path = OUT_DIR / f"ensemble_debug_{method_name}.txt"
    with open(debug_path, 'w', encoding='utf-8') as f:
        f.write(f"Ensemble èª¿è©¦ä¿¡æ¯\n")
        f.write(f"================\n")
        f.write(f"æ–¹æ³•: {method_name}\n")
        f.write(f"ç­–ç•¥æ•¸é‡: {N}\n")
        f.write(f"åƒæ•¸: {cfg.params}\n")
        f.write(f"æˆæœ¬: {cfg.cost}\n")
        f.write(f"æ¬Šé‡çµ±è¨ˆ: min={w.min():.4f}, mean={w.mean():.4f}, max={w.max():.4f}\n")
        f.write(f"ç¸¾æ•ˆ: {stats}\n")
        f.write(f"äº¤æ˜“æ˜ç´°æ¬„ä½: trades={list(trades_ui.columns)}, trade_ledger={list(trade_ledger_ui.columns)}\n")
        f.write(f"äº¤æ˜“ç­†æ•¸: trades={len(trades_ui)}, trade_ledger={len(trade_ledger_ui)}\n")

    logger.info(f"[Ensemble] èª¿è©¦ä¿¡æ¯å·²ä¿å­˜åˆ°: {debug_path}")
I noticed the function doesn't reference the risk valve. I should double-check where it might apply.

root@9a3d3c80743e:/workspace/SSS096# rg "é¢¨éšªé–¥é–€" SSS_EnsembleTab.py
1287:# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
1292:    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š
root@9a3d3c80743e:/workspace/SSS096# sed -n '1287,1335p' SSS_EnsembleTab.py
# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
def apply_valve_to_weights(w: pd.Series,
                           risk_trigger: pd.Series,
                           mode: str = "cap",   # "cap" æˆ– "ban_add"
                           cap_level: float = 0.5) -> pd.Series:
    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š
       - cap: é¢¨éšªæ—¥å°‡ w é™åœ¨ cap_level ä»¥ä¸‹
       - ban_add: é¢¨éšªæ—¥ç¦æ­¢ã€Œæé«˜ã€å€‰ä½ï¼ˆå…è¨±æ¸›å€‰ï¼‰ï¼Œç”¨è¿­ä»£æ³•ç¢ºä¿å–®èª¿"""
    w = w.copy().reindex(risk_trigger.index).ffill().fillna(0.0)
    out = w.copy()

    if mode == "cap":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        out[mask] = np.minimum(out[mask], cap_level)

    elif mode == "ban_add":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        for i in range(1, len(out)):
            if mask.iloc[i] and (out.iloc[i] > out.iloc[i-1]):
                out.iloc[i] = out.iloc[i-1]  # ç¦æ­¢å¢åŠ 
    else:
        raise ValueError("mode éœ€ç‚º 'cap' æˆ– 'ban_add'")
    return out.clip(0.0, 1.0)


# === PATCH: ç”± trade_ledger å»ºç«‹ã€Œæ¯ç­†ç›ˆè™§%ã€èˆ‡å¿«ç…§ï¼ˆå¼•ç”¨ re.txt åšæ³•ï¼‰===
def build_trade_snapshots_from_ledger(trade_ledger: pd.DataFrame) -> pd.DataFrame:
    """ä»¥äº¤æ˜“å¾Œæ¬Šç›Š/ç¾é‡‘ç‚ºè³‡ç”¢å¿«ç…§ï¼Œè¨ˆç®—æ¯ç­†ç›ˆè™§%ï¼ˆåªåœ¨æœ‰è®Šå‹•çš„äº¤æ˜“æ—¥è¨˜ä¸€ç­†ï¼‰"""
    df = trade_ledger.copy()
    # æ¬„åå°é½Š
    if "date" in df.columns: df.rename(columns={"date": "äº¤æ˜“æ—¥æœŸ"}, inplace=True)
    if "type" in df.columns: df.rename(columns={"type": "äº¤æ˜“é¡å‹"}, inplace=True)
    if "equity_after" in df.columns: df.rename(columns={"equity_after": "äº¤æ˜“å¾Œæ¬Šç›Š"}, inplace=True)
    if "cash_after" in df.columns: df.rename(columns={"cash_after": "äº¤æ˜“å¾Œç¾é‡‘"}, inplace=True)
    if "open" in df.columns: df.rename(columns={"open": "é–‹ç›¤åƒ¹"}, inplace=True)
    if "w_before" in df.columns: df.rename(columns={"w_before": "äº¤æ˜“å‰æ¬Šé‡"}, inplace=True)
    if "w_after" in df.columns: df.rename(columns={"w_after": "äº¤æ˜“å¾Œæ¬Šé‡"}, inplace=True)

    df["äº¤æ˜“æ—¥æœŸ"] = pd.to_datetime(df["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
    df = df.dropna(subset=["äº¤æ˜“æ—¥æœŸ"]).sort_values("äº¤æ˜“æ—¥æœŸ").reset_index(drop=True)

    df["ç¸½è³‡ç”¢"] = pd.to_numeric(df.get("äº¤æ˜“å¾Œæ¬Šç›Š", 0), errors="coerce").fillna(0) + \
                   pd.to_numeric(df.get("äº¤æ˜“å¾Œç¾é‡‘", 0), errors="coerce").fillna(0)
    df["å‰æ¬¡ç¸½è³‡ç”¢"] = df["ç¸½è³‡ç”¢"].shift(1)
    df["æ¯ç­†ç›ˆè™§"] = df["ç¸½è³‡ç”¢"] - df["å‰æ¬¡ç¸½è³‡ç”¢"]
    df["æ¯ç­†ç›ˆè™§%"] = (df["æ¯ç­†ç›ˆè™§"] / df["å‰æ¬¡ç¸½è³‡ç”¢"]).replace([np.inf, -np.inf], np.nan) * 100
    df.loc[df.index.min(), ["æ¯ç­†ç›ˆè™§", "æ¯ç­†ç›ˆè™§%"]] = 0.0
    return df
Iâ€™ll search for apply_valve_to_weights to check if it affects the metrics variable.

root@9a3d3c80743e:/workspace/SSS096# rg "apply_valve_to_weights" -n
SSS_EnsembleTab.py
1288:def apply_valve_to_weights(w: pd.Series,
1373:    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)
Iâ€™ll check between lines 1350 and 1385, which are related to the risk valve example.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1345,1385p' SSS_EnsembleTab.py
    if "äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š" in tl.columns and "äº¤æ˜“å¾Œæ¬Šç›Š" in tl.columns:
        delta = (tl["äº¤æ˜“å¾Œæ¬Šç›Š"] - tl["äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š"]) / tl["äº¤æ˜“å¾Œé–‹ç›¤æ¬Šç›Š"]
        tl["ç›ˆè™§%"] = delta * 100.0
    elif "ç›ˆè™§%" not in tl.columns:
        tl["ç›ˆè™§%"] = 0.0
    mask_sell = tl["äº¤æ˜“é¡å‹"].astype(str).str.lower().eq("sell")
    return pd.to_numeric(tl.loc[mask_sell, "ç›ˆè™§%"], errors="coerce").dropna()

def _mdd_from_daily_equity(equity: pd.Series) -> float:
    eq = pd.to_numeric(equity, errors="coerce").dropna()
    if eq.empty: return 0.0
    dd = eq / eq.cummax() - 1.0
    return float(dd.min())

def risk_valve_backtest(open_px: pd.Series,
                        w: pd.Series,
                        cost,
                        benchmark_df: pd.DataFrame,
                        mode: str = "cap",
                        cap_level: float = 0.5,
                        slope20_thresh: float = 0.0,
                        slope60_thresh: float = 0.0,
                        atr_win: int = 20,
                        atr_ref_win: int = 60,
                        atr_ratio_mult: float = 1.3) -> dict:
    """å›å‚³ï¼šåŸå§‹èˆ‡é–¥é–€ç‰ˆæœ¬çš„ç¸¾æ•ˆã€å³å°¾å‰Šæ¸›åº¦ã€ä»¥åŠå…©ç‰ˆ equity/ledger"""
    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
                                     atr_win, atr_ref_win, atr_ratio_mult)
    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)

    # åŸå§‹ç‰ˆæœ¬
    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
    # é–¥é–€ç‰ˆæœ¬
    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)

    # äº¤æ˜“çµ±è¨ˆï¼ˆåªç”¨è³£å‡ºåˆ—ï¼›ç¬¦åˆ re.txt å°å¯¦ç¾æç›Šçš„å£å¾‘ï¼‰
    r1 = _sell_returns_pct_from_ledger(ledger1)
    r2 = _sell_returns_pct_from_ledger(ledger2)
    pf1 = (r1[r1>0].sum() / abs(r1[r1<0].sum())) if (r1[r1<0].sum()!=0) else np.inf
    pf2 = (r2[r2>0].sum() / abs(r2[r2<0].sum())) if (r2[r2<0].sum()!=0) else np.inf
    mdd1 = _mdd_from_daily_equity(daily_state1["equity"])
The user reports missing metrics after applying the risk valve. I'll check relevant functions.

root@9a3d3c80743e:/workspace/SSS096# rg "å…¨å±€é¢¨éšªé–¥é–€" -n
app_dash.py
797:    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
810:        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
813:            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")
856:            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")
1116:                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
1205:        # === å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨ï¼ˆèˆ‡å¢å¼·åˆ†æä¸€è‡´ï¼‰ ===
1211:                logger.warning(f"[{strat}] daily_state ä¸å« 'w'ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1217:                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— ATR20/60ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1316:                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
The user mentioned metrics disappearing after applying the risk valve. I'll check SSSv096 for any issues.

root@9a3d3c80743e:/workspace/SSS096# rg "é¢¨éšªé–¥é–€" SSSv096.py
I think the user might be seeing issues with metrics after applying the risk valve. Let's check for the function recalculating metrics in app_dash after the valve is applied, especially around the 'avg_holding_days' or similar metrics.

root@9a3d3c80743e:/workspace/SSS096# rg "metrics" -n app_dash.py
1096:                    if "metrics" in rv:
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1130:                    'metrics': backtest_result.stats,
1151:                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calma
r_ratio': 0.0, 'num_trades': 0},
1302:                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
1540:            metrics = result.get('metrics', {})
1545:            metrics['avg_holding_period'] = avg_holding
1568:            for k, v in metrics.items():
1729:                dbc.Row(metric_cards, style={"flex-wrap": "wrap"}, className='metrics-cards-row'),
1731:                dcc.Graph(figure=fig1, config={'displayModeBar': True}, className='main-metrics-graph'),
1878:            metrics = result['metrics']
1881:                'ç¸½å›å ±ç‡': f"{metrics.get('total_return', 0):.2%}",
1882:                'å¹´åŒ–å›å ±ç‡': f"{metrics.get('annual_return', 0):.2%}",
1883:                'æœ€å¤§å›æ’¤': f"{metrics.get('max_drawdown', 0):.2%}",
1884:                'å¡ç‘ªæ¯”ç‡': f"{metrics.get('calmar_ratio', 0):.2f}",
1885:                'äº¤æ˜“æ¬¡æ•¸': metrics.get('num_trades', 0),
1886:                'å‹ç‡': f"{metrics.get('win_rate', 0):.2%}",
1887:                'ç›ˆè™§æ¯”': f"{metrics.get('payoff_ratio', 0):.2f}",
2436:    m = out["metrics"]
2628:    def _group_metrics(mask):
2655:    acc_metrics = dis_metrics = None
2659:        acc_metrics = _group_metrics(mask_acc)
2660:        dis_metrics = _group_metrics(mask_dis)
2664:        group_row("åŠ ç¢¼æ®µ", acc_metrics),
2665:        group_row("æ¸›ç¢¼æ®µ", dis_metrics),
3447:                if not isinstance(out, dict) or "metrics" not in out:
3451:                m = out["metrics"]
3547:def calculate_pareto_metrics(equity_curve, trade_df):
3860:                    if out and "metrics" in out:
3861:                        m = out["metrics"]
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)
3868:                        if metrics:
3873:                                'max_drawdown': metrics['max_drawdown'],
3874:                                'pf': metrics['pf'],
3875:                                'right_tail_loss': metrics['right_tail_loss'],
3876:                                'risk_trigger_days': metrics['risk_trigger_days'],
I'm checking around where the risk valve is applied to see if it overwrites 'metrics'.

root@9a3d3c80743e:/workspace/SSS096# rg "valve" -n app_dash.py
455:                        dbc.Checkbox(id='force-valve-trigger', value=False, label="å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼ˆæ¸¬è©¦ç”¨ï¼‰", style={"font
Size":"11px","color":"#dc3545"}),
461:                    html.Div(id='risk-valve-status', style={"marginTop":"8px","padding":"8px","backgroundColor":"#f8f9fa","b
orderRadius":"4px","border":"1px solid #dee2e6"}),
575:    Output('risk-valve-status', 'children'),
580:        Input('force-valve-trigger', 'value'),
586:def update_risk_valve_status(global_apply, risk_cap, atr_ratio, force_trigger, ticker, start_date, end_date):
656:                            valve_triggered = atr_ratio_current > atr_ratio
660:                                valve_triggered = True
666:                            logger.info(f"é¢¨éšªé–¥é–€è§¸ç™¼: {'æ˜¯' if valve_triggered else 'å¦'}")
667:                            logger.info(f"é¢¨éšªé–¥é–€ç‹€æ…‹: {'ğŸ”´ è§¸ç™¼' if valve_triggered else 'ğŸŸ¢ æ­£å¸¸'}")
669:                            status_color = "#dc3545" if valve_triggered else "#28a745"
670:                            status_icon = "ğŸ”´" if valve_triggered else "ğŸŸ¢"
671:                            status_text = "è§¸ç™¼" if valve_triggered else "æ­£å¸¸"
764:        Input('force-valve-trigger', 'value'),
798:    valve_triggered = False
850:                    valve_triggered = True
873:                valve_triggered_local = False
883:                            valve_triggered_local = (ratio_local >= atr_ratio)  # å»ºè­°ä½¿ç”¨ >=
888:                    valve_triggered_local = True
892:                if valve_triggered_local:
893:                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
918:                        rv = risk_valve_backtest(
928:                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
929:                        result['daily_state']      = pack_df(rv["daily_state_valve"])
930:                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
931:                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
932:                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
933:                        result['weight_curve']     = pack_series(rv["weights_valve"])
935:                        result['valve'] = {
947:                    result['valve'] = {
1021:                valve_triggered = False
1044:                                valve_triggered = (ratio >= atr_ratio)
1045:                                logger.info(f"[{strat}] Ensemble ATR æ¯”å€¼: {ratio:.4f} (é–€æª»={atr_ratio}) -> è§¸ç™¼={valve_tr
iggered}")
1048:                                if valve_triggered:
1065:                    valve_triggered = True
1074:                if global_apply and valve_triggered:
1075:                    from SSS_EnsembleTab import risk_valve_backtest
1080:                    rv = risk_valve_backtest(
1089:                    backtest_result.daily_state = rv["daily_state_valve"]
1090:                    backtest_result.ledger = rv["trade_ledger_valve"]
1091:                    backtest_result.weight_curve = rv["weights_valve"]
1092:                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1101:                    result['valve'] = {
1110:                        result['valve'] = {
1118:                        result['valve'] = {
1172:            # ç‚ºå…¶ä»–ç­–ç•¥é¡å‹æ·»åŠ  valve æ¨™è¨˜
1174:                result['valve'] = {
1180:                result['valve'] = {
1238:                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
1240:                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
1264:                        result_cap = risk_valve_backtest(
1273:                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
1278:                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFram
e()))} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")
1281:                        if 'trade_ledger_valve' in result_cap:
1282:                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
1283:                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
1284:                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
1286:                        if 'daily_state_valve' in result_cap:
1287:                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
1288:                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])
1290:                        if 'weights_valve' in result_cap:
1291:                            result['weight_curve'] = pack_series(result_cap['weights_valve'])
1294:                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
1296:                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
1299:                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity
'))
1307:                        result['_risk_valve_applied'] = True
1308:                        result['_risk_valve_params'] = {
1702:            valve = results.get(strategy, {}).get('valve', {}) or {}
1703:            valve_badge_text = ("å·²å¥—ç”¨" if valve.get("applied") else "æœªå¥—ç”¨")
1704:            valve_badge_extra = []
1705:            if isinstance(valve.get("cap"), (int, float)):
1706:                valve_badge_extra.append(f"CAP={valve['cap']:.2f}")
1707:            if isinstance(valve.get("atr_ratio"), (int, float)):
1708:                valve_badge_extra.append(f"ATRæ¯”å€¼={valve['atr_ratio']:.2f}")
1709:            elif valve.get("atr_ratio") == "forced":
1710:                valve_badge_extra.append("å¼·åˆ¶è§¸ç™¼")
1712:            valve_badge = html.Span(
1713:                "ğŸ›¡ï¸ é¢¨éšªé–¥é–€ï¼š" + valve_badge_text + ((" | " + " | ".join(valve_badge_extra)) if valve_badge_extra else ""),
1716:                    "color": ("#dc3545" if valve.get("applied") else "#6c757d"),
1719:            ) if valve else html.Span("")
1725:                    valve_badge
2426:        from SSS_EnsembleTab import risk_valve_backtest
2427:        out = risk_valve_backtest(
2443:        html.Code(f"PF: åŸå§‹ {m['pf_orig']:.2f} â†’ é–¥é–€ {m['pf_valve']:.2f}"), html.Br(),
2444:        html.Code(f"MDD: åŸå§‹ {m['mdd_orig']:.2%} â†’ é–¥é–€ {m['mdd_valve']:.2%}"), html.Br(),
2445:        html.Code(f"å³å°¾ç¸½å’Œ(>P90 æ­£å ±é…¬): åŸå§‹ {m['right_tail_sum_orig']:.2f} â†’ é–¥é–€ {m['right_tail_sum_valve']:.2f} (â†“{m[
'right_tail_reduction']:.2f})"), html.Br(),
2452:    eq2 = out["daily_state_valve"]["equity"]
2458:        "valve": {"color": "#ff7f0e", "dash": "dot"},
2469:        mode="lines", line=dict(color=palette["valve"]["color"], width=2, dash=palette["valve"]["dash"]),
2482:        mode="lines", line=dict(color=palette["valve"]["color"], width=2, dash=palette["valve"]["dash"]),
3424:    # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3426:        from SSS_EnsembleTab import risk_valve_backtest
3427:        logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3429:        logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3430:        return go.Figure(), f"âŒ ç„¡æ³•åŒ¯å…¥ risk_valve_backtest: {e}"
3440:                out = risk_valve_backtest(
3456:                pf = float(m.get("pf_valve", np.nan))
3457:                mdd = float(m.get("mdd_valve", np.nan))
3458:                rt_sum_valve = float(m.get("right_tail_sum_valve", np.nan))
3460:                rt_reduction = float(m.get("right_tail_reduction", np.nan)) if m.get("right_tail_reduction") is not None el
se (rt_sum_orig - rt_sum_valve if np.isfinite(rt_sum_orig) and np.isfinite(rt_sum_valve) else np.nan)
3468:                    "right_tail_sum_valve": rt_sum_valve,
3524:        customdata=dfp[["right_tail_sum_valve","right_tail_sum_orig"]].values,
3837:        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3839:            from SSS_EnsembleTab import risk_valve_backtest
3840:            logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3842:            logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3853:                    out = risk_valve_backtest(
3864:                        equity_curve = out.get("daily_state_valve", {}).get("equity")
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
3878:                                'pf_valve': m.get('pf_valve', 0.0),
3880:                                'mdd_valve': m.get('mdd_valve', 0.0),
I'm checking where metrics might be overwritten after applying the risk valve. Let's dig deeper around 1130-1151.

root@9a3d3c80743e:/workspace/SSS096# sed -n '824,1170p' app_dash.py

                    # æª¢æŸ¥æ¨£æœ¬æ•¸é‡æ˜¯å¦è¶³å¤ 
                    min_samples_20, min_samples_60 = 30, 60  # è‡³å°‘éœ€è¦ 30 å’Œ 60 å€‹æ¨£æœ¬
                    if len(atr_20_valid) < min_samples_20 or len(atr_60_valid) < min_samples_60:
                        logger.warning(f"[{strat}] ATR æ¨£æœ¬ä¸è¶³ï¼Œ20æœŸ:{len(atr_20_valid)}/{min_samples_20}, 60æœŸ:{len(atr_60_val
id)}/{min_samples_60}")
                        continue

                    atr_20_latest = atr_20_valid.iloc[-1]
                    atr_60_latest = atr_60_valid.iloc[-1]

                    # æª¢æŸ¥ ATR å€¼æ˜¯å¦åˆç†
                    if atr_60_latest <= 0 or not np.isfinite(atr_60_latest):
                        logger.warning(f"[{strat}] ATR(60) å€¼ç•°å¸¸: {atr_60_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
                        continue

                    if atr_20_latest <= 0 or not np.isfinite(atr_20_latest):
                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
                        continue

                    atr_ratio_current = atr_20_latest / atr_60_latest
                    logger.info(f"[{strat}] æœ€æ–°ATRæ¯”å€¼: {atr_ratio_current:.4f} (20æœŸ:{atr_20_latest:.4f}, 60æœŸ:{atr_60_latest:
.4f})")
                else:
                    logger.warning(f"[{strat}] ATR è¨ˆç®—çµæœç‚ºç©º")

                # å¼·åˆ¶è§¸ç™¼æ™‚è¨­ç½®æ¨™è¨˜
                if force_trigger:
                    valve_triggered = True
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")

            except Exception as e:
                logger.warning(f"[{strat}] ATR è¨ˆç®—å¤±æ•—: {e}")
        else:
            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")

        if strat_type == 'ssma_turn':
            calc_keys = ['linlen', 'factor', 'smaalen', 'prom_factor', 'min_dist', 'buy_shift', 'exit_shift', 'vol_window', 'sig
nal_cooldown_days', 'quantile_win']
            ssma_params = {k: v for k, v in strat_params.items() if k in calc_keys}
            backtest_params = ssma_params.copy()
            backtest_params['stop_loss'] = strat_params.get('stop_loss', 0.0)

            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
            df_ind, buy_dates, sell_dates = compute_ssma_turn_combined(df_raw, df_factor, **ssma_params, smaa_source=smaa_src)
            if df_ind.empty:
                continue
            result = backtest_unified(df_ind, strat_type, backtest_params, buy_dates, sell_dates, discount=discount, trade_coold
own_bars=cooldown, bad_holding=bad_holding)

            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
            if global_apply:
                # åˆ¤æ–·æ˜¯å¦è¦è§¸ç™¼ï¼ˆèˆ‡ä½ çš„ ATR æª¢æŸ¥æˆ–å¼·åˆ¶è§¸ç™¼ä¸€è‡´ï¼‰
                valve_triggered_local = False
                ratio_local = None
                try:
                    atr_20 = calculate_atr(df_raw, 20)
                    atr_60 = calculate_atr(df_raw, 60)
                    if not atr_20.empty and not atr_60.empty:
                        a20 = atr_20.dropna().iloc[-1]
                        a60 = atr_60.dropna().iloc[-1]
                        if a60 > 0:
                            ratio_local = float(a20 / a60)
                            valve_triggered_local = (ratio_local >= atr_ratio)  # å»ºè­°ä½¿ç”¨ >=
                except Exception:
                    pass

                if force_trigger:
                    valve_triggered_local = True
                    if ratio_local is None:
                        ratio_local = 1.5

                if valve_triggered_local:
                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
                    # å–å¾— open åƒ¹ï¼›df_raw æ¬„ä½åç¨±æ˜¯å°å¯«
                    open_px = df_raw['open'] if 'open' in df_raw.columns else df_raw['close']
                    # å¾å›æ¸¬è¼¸å‡ºæŠ“ wï¼ˆå…ˆç”¨æ¨™æº–åŒ– daily_stateï¼Œå¦‚æœæ²’æœ‰å°±ç”¨åŸ daily_stateï¼‰
                    w_series = None
                    try:
                        ds_std = df_from_pack(result.get('daily_state_std'))
                        if ds_std is not None and not ds_std.empty and 'w' in ds_std.columns:
                            w_series = ds_std['w']
                    except Exception:
                        pass
                    if w_series is None:
                        ds = df_from_pack(result.get('daily_state'))
                        if ds is not None and not ds.empty and 'w' in ds.columns:
                            w_series = ds['w']

                    if w_series is not None:
                        # äº¤æ˜“æˆæœ¬ï¼ˆèˆ‡ Ensemble åˆ†æ”¯ä¸€è‡´ï¼‰
                        trade_cost = strat_params.get('trade_cost', {})
                        cost_params = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0))
                        )

                        rv = risk_valve_backtest(
                            open_px=open_px,
                            w=w_series,
                            cost=cost_params,
                            benchmark_df=df_raw,
                            mode="cap",
                            cap_level=float(risk_cap),
                        )

                        # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´ï¼ˆå’Œ Ensemble åˆ†æ”¯å°é½Šï¼‰
                        result['equity_curve']     = pack_series(rv["daily_state_valve"]["equity"])
                        result['daily_state']      = pack_df(rv["daily_state_valve"])
                        result['daily_state_std']  = pack_df(rv["daily_state_valve"])
                        result['trade_ledger']     = pack_df(rv["trade_ledger_valve"])
                        result['trade_ledger_std'] = pack_df(rv["trade_ledger_valve"])
                        result['weight_curve']     = pack_series(rv["weights_valve"])
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆä¸‹å€‹å°ç¯€æœƒç”¨åˆ°ï¼‰
                        result['valve'] = {
                            "applied": True,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio_local
                        }

                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
                    else:
...

                except Exception as e:
                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— Ensemble ATR æ¯”å€¼: {e}")
                    logger.warning(f"[{strat}] éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")

                # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
                if force_trigger:
                    valve_triggered = True
                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
                    if ratio is None:
                        ratio = 1.5  # è¨­å®šä¸€å€‹é è¨­å€¼ç”¨æ–¼é¡¯ç¤º

                # ä½¿ç”¨æ–°çš„ ensemble_runner åŸ·è¡Œ
                backtest_result = run_ensemble_backtest(cfg)

                # è‹¥å…¨å±€é–‹é—œé–‹å•Ÿä¸”é”è§¸ç™¼æ¢ä»¶ï¼Œæ‰åœ¨æ¬Šé‡åºåˆ—ä¸Šå¥—ç”¨ CAP
                if global_apply and valve_triggered:
                    from SSS_EnsembleTab import risk_valve_backtest
                    bench = df_raw  # å·²å« open/high/low/close/volume

                    logger.info(f"[{strat}] ğŸ”´ é–‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")

                    rv = risk_valve_backtest(
                        open_px=backtest_result.price_series,
                        w=backtest_result.weight_curve,
                        cost=cost_params,
                        benchmark_df=bench,
                        mode="cap",
                        cap_level=float(risk_cap),
                    )
                    # è¦†å¯«çµæœï¼Œç¢ºä¿ UI èˆ‡è¼¸å‡ºä¸€è‡´
                    backtest_result.daily_state = rv["daily_state_valve"]
                    backtest_result.ledger = rv["trade_ledger_valve"]
                    backtest_result.weight_curve = rv["weights_valve"]
                    backtest_result.equity_curve = rv["daily_state_valve"]["equity"]
                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")

                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
                    if "metrics" in rv:
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv['metr
ics'].get('pf_valve', 'N/A'):.2f}")
                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€={rv['
metrics'].get('mdd_valve', 'N/A'):.2f}%")

                    # çµ¦ UI çš„æ¨™è¨˜ï¼ˆèˆ‡ SSMA åˆ†æ”¯å°é½Šï¼‰
                    result['valve'] = {
                        "applied": True,
                        "cap": float(risk_cap),
                        "atr_ratio": ratio
                    }
                else:
                    if global_apply:
                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªè§¸ç™¼ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": float(risk_cap),
                            "atr_ratio": ratio if ratio is not None else "N/A"
                        }
                    else:
                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
                        # çµ¦ UI çš„æ¨™è¨˜ï¼ˆæœªå•Ÿç”¨ï¼‰
                        result['valve'] = {
                            "applied": False,
                            "cap": "N/A",
                            "atr_ratio": "N/A"
                        }

                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
                    'trade_ledger': pack_df(backtest_result.ledger),
                    'daily_state_std': pack_df(backtest_result.daily_state),
                    'trade_ledger_std': pack_df(backtest_result.ledger)
                }

                logger.info(f"[Ensemble] åŸ·è¡ŒæˆåŠŸ: ç›Šæ›²ç·šé•·åº¦={len(backtest_result.equity_curve)}, äº¤æ˜“æ•¸={len(backtest_result.l
edger) if backtest_result.ledger is not None and not backtest_result.ledger.empty else 0}")

            except Exception as e:
                logger.error(f"Ensemble ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
                # å‰µå»ºç©ºçš„çµæœ
                result = {
                    'trades': [],
                    'trade_df': pd.DataFrame(),
                    'trades_df': pd.DataFrame(),
                    'signals_df': pd.DataFrame(),
                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calmar_rat
io': 0.0, 'num_trades': 0},
                    'equity_curve': pd.Series(1.0, index=df_raw.index)
                }

            # === ä¿®å¾© 3ï¼šæ·»åŠ èª¿è©¦æ—¥èªŒï¼Œæ ¸å°å­ç­–ç•¥é›†åˆæ˜¯å¦ä¸€è‡´ ===
            logger.info(f"[Ensemble] åŸ·è¡Œå®Œæˆï¼Œticker={ticker}, method={flat_params.get('method')}")
            if 'equity_curve' in result and hasattr(result['equity_curve'], 'shape'):
                logger.info(f"[Ensemble] æ¬Šç›Šæ›²ç·šé•·åº¦: {len(result['equity_curve'])}")
            if 'trade_df' in result and hasattr(result['trade_df'], 'shape'):
                logger.info(f"[Ensemble] äº¤æ˜“è¨˜éŒ„æ•¸é‡: {len(result['trade_df'])}")
        else:
            if strat_type == 'single':
                df_ind = compute_single(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen
"], strat_params["devwin"], smaa_source=smaa_src)
            elif strat_type == 'dual':
                df_ind = compute_dual(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen"]
, strat_params["short_win"], strat_params["long_win"], smaa_source=smaa_src)
            elif strat_type == 'RMA':
                df_ind = compute_RMA(df_raw, df_factor, strat_params["linlen"], strat_params["factor"], strat_params["smaalen"],
 strat_params["rma_len"], strat_params["dev_len"], smaa_source=smaa_src)
            if df_ind.empty:
                continue
            result = backtest_unified(df_ind, strat_type, strat_params, discount=discount, trade_cooldown_bars=cooldown, bad_hol
ding=bad_holding)
I'm checking how result gets transformed after run_ensemble and the impact of risk valve.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1188,1324p' app_dash.py
            if hasattr(result.get('trade_df'), 'to_json'):
                result['trade_df'] = result['trade_df'].to_json(date_format='iso', orient='split')
            if 'signals_df' in result and hasattr(result['signals_df'], 'to_json'):
                result['signals_df'] = result['signals_df'].to_json(date_format='iso', orient='split')
            if 'trades_df' in result and hasattr(result['trades_df'], 'to_json'):
                result['trades_df'] = result['trades_df'].to_json(date_format='iso', orient='split')
            if 'equity_curve' in result and hasattr(result['equity_curve'], 'to_json'):
                result['equity_curve'] = result['equity_curve'].to_json(date_format='iso', orient='split')
        if 'trades' in result and isinstance(result['trades'], list):
            result['trades'] = [
                (str(t[0]), t[1], str(t[2])) if isinstance(t, tuple) and len(t) == 3 else t
                for t in result['trades']
            ]

        # << æ–°å¢ï¼šä¸€å¾‹åšæœ€å¾Œä¿éšªæ‰“åŒ…ï¼Œè£œä¸Š daily_state / weight_curve ç­‰ >>
        result = _pack_result_for_store(result)

        # === å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨ï¼ˆèˆ‡å¢å¼·åˆ†æä¸€è‡´ï¼‰ ===
        if global_apply:
            # 1) å– dsï¼ˆdaily_stateï¼‰ï¼Œä¸¦è§£åŒ…
            ds_raw = result.get("daily_state_std") or result.get("daily_state")
            ds = df_from_pack(ds_raw)
            if ds is None or ds.empty or "w" not in ds.columns:
                logger.warning(f"[{strat}] daily_state ä¸å« 'w'ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
            else:
                # 2) ç®—é€æ—¥ ATR æ¯”å€¼èˆ‡é€æ—¥ mask
                atr20 = calculate_atr(df_raw, 20)
                atr60 = calculate_atr(df_raw, 60)
                if atr20 is None or atr60 is None:
                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— ATR20/60ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
                else:
                    ratio = (atr20 / atr60).replace([np.inf, -np.inf], np.nan)
                    mask = (ratio >= float(atr_ratio))
                    if force_trigger:
                        mask[:] = True  # å¼·åˆ¶å…¨éƒ¨æ—¥å­å¥— CAP

                    # 3) å°é½Šåˆ° ds.indexï¼Œé€æ—¥å£“ w è‡³ CAP
                    mask_aligned = mask.reindex(ds.index).fillna(False).to_numpy()
                    w = ds["w"].astype(float).to_numpy()
                    w_new = w.copy()
                    w_new[mask_aligned] = np.minimum(w_new[mask_aligned], float(risk_cap))
                    ds["w"] = w_new

                    # 4) å›å¯« dsï¼Œä¸¦é‡ç®—äº¤æ˜“/æ¬Šç›Š
                    result["daily_state_std"] = pack_df(ds)

                    # open åƒ¹ï¼ˆæ²’æœ‰ open å°±é€€è€Œæ±‚å…¶æ¬¡ç”¨æ”¶ç›¤åƒ¹ï¼‰
                    open_px = (df_raw["open"] if "open" in df_raw.columns else df_raw.get("æ”¶ç›¤åƒ¹")).astype(float)
                    open_px = open_px.reindex(ds.index).dropna()

                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
                    try:
                        from SSS_EnsembleTab import risk_valve_backtest, CostParams

                        # æˆæœ¬åƒæ•¸
                        trade_cost = (strat_params.get("trade_cost", {})
                                      if isinstance(strat_params, dict) else {})
                        cost = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0)),
                        )

                        # åŸºæº–ï¼ˆæœ‰é«˜ä½åƒ¹å°±å¸¶ä¸Šï¼‰
                        bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
                        if 'close' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
                        elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
                        if 'high' in df_raw.columns and 'low' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
                        elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")

                        result_cap = risk_valve_backtest(
                            open_px=open_px,
                            w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
The metrics dictionary is cleared after applying the global risk valve, but it isn't recomputed. I'll adjust the code to recompute metrics, using either calculate_pareto_metrics or perf_stats, after updating the equity_curve.

I'll search for perf_stats and calculate_pareto_metrics to compute metrics after risk valve.

root@9a3d3c80743e:/workspace/SSS096# rg "def perf_stats" -n
SSS_EnsembleTab.py
691:def perf_stats(equity: pd.Series, w: pd.Series) -> Dict[str, float]:
I'll check if perf_stats is already imported in app_dash and confirm its usage.

root@9a3d3c80743e:/workspace/SSS096# rg "perf_stats" -n app_dash.py
Iâ€™ll take a look at the top of app_dash to review existing imports and style.

root@9a3d3c80743e:/workspace/SSS096# head -n 80 app_dash.py
import dash
from dash import dcc, html, dash_table, Input, Output, State, ctx, no_update
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.graph_objects as go
import json
import io
from dash.dependencies import ALL
import shutil
import os
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from analysis import config as cfg
import yfinance as yf
import logging
import numpy as np
from urllib.parse import quote as urlparse

# é…ç½® logger - ä½¿ç”¨æ–°çš„é¡¯å¼åˆå§‹åŒ–
from analysis.logging_config import init_logging
import os

# å¼·åˆ¶å•Ÿç”¨æª”æ¡ˆæ—¥èªŒï¼Œä¸ä¾è³´ç’°å¢ƒè®Šæ•¸
os.environ["SSS_CREATE_LOGS"] = "1"

# ç›´æ¥å‘¼å« init_loggingï¼Œå®ƒæœƒä½¿ç”¨æ­£ç¢ºçš„æ—¥èªŒç›®éŒ„
init_logging(enable_file=True)
logger = logging.getLogger("SSS.App")

# è¨­å®šç‚º DEBUG ç´šåˆ¥ï¼Œè©³ç´°èª¿è©¦è³‡è¨Šæœƒå¯«å…¥æ—¥èªŒæª”æ¡ˆ
logger.setLevel(logging.DEBUG)
logger.info("å·²å•Ÿç”¨è©³ç´°èª¿è©¦æ¨¡å¼ - èª¿è©¦è³‡è¨Šå°‡å¯«å…¥æ—¥èªŒæª”æ¡ˆ")
logger.info(f"æ—¥èªŒç›®éŒ„: {os.path.abspath('analysis/log')}")

# ATR è¨ˆç®—å‡½æ•¸
def calculate_atr(df, window):
    """è¨ˆç®— ATR (Average True Range)"""
    try:
        # æ”¯æ´å¤šç¨®æ¬„ä½åç¨±æ ¼å¼
        high_col = None
        low_col = None
        close_col = None

        # æª¢æŸ¥è‹±æ–‡æ¬„ä½åç¨±
        if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:
            high_col = 'high'
            low_col = 'low'
            close_col = 'close'
        # æª¢æŸ¥ä¸­æ–‡æ¬„ä½åç¨±
        elif 'æœ€é«˜åƒ¹' in df.columns and 'æœ€ä½åƒ¹' in df.columns and 'æ”¶ç›¤åƒ¹' in df.columns:
            high_col = 'æœ€é«˜åƒ¹'
            low_col = 'æœ€ä½åƒ¹'
            close_col = 'æ”¶ç›¤åƒ¹'
        # æª¢æŸ¥å…¶ä»–å¯èƒ½çš„æ¬„ä½åç¨±
        elif 'open' in df.columns and 'close' in df.columns:
            # å¦‚æœæ²’æœ‰é«˜ä½åƒ¹ï¼Œç”¨é–‹ç›¤åƒ¹å’Œæ”¶ç›¤åƒ¹è¿‘ä¼¼
            high_col = 'open'
            low_col = 'close'
            close_col = 'close'

        if high_col and low_col and close_col:
            # æœ‰é«˜ä½åƒ¹æ™‚ï¼Œè¨ˆç®— True Range
            high = df[high_col]
            low = df[low_col]
            close = df[close_col]

            # ç¢ºä¿æ•¸æ“šç‚ºæ•¸å€¼å‹
            high = pd.to_numeric(high, errors='coerce')
            low = pd.to_numeric(low, errors='coerce')
            close = pd.to_numeric(close, errors='coerce')

            tr1 = high - low
            tr2 = abs(high - close.shift(1))
            tr3 = abs(low - close.shift(1))

            true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
            atr = true_range.rolling(window=window).mean()
        else:
            # åªæœ‰æ”¶ç›¤åƒ¹æ™‚ï¼Œç”¨åƒ¹æ ¼è®ŠåŒ–è¿‘ä¼¼
I'll search for "from SSS_EnsembleTab import" to check the existing import style.


root@9a3d3c80743e:/workspace/SSS096# rg "SSS_EnsembleTab" -n app_dash.py
893:                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
956:                from SSS_EnsembleTab import EnsembleParams, CostParams, RunConfig
1075:                    from SSS_EnsembleTab import risk_valve_backtest
1240:                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
2413:    # æˆæœ¬åƒæ•¸ï¼ˆä½¿ç”¨ SSS_EnsembleTab é è¨­ï¼‰
2424:    # éœ€è¦ç”¨åˆ° SSS_EnsembleTab å…§æ–°åŠ çš„å‡½å¼
2426:        from SSS_EnsembleTab import risk_valve_backtest
2530:        from SSS_EnsembleTab import trade_contribution_by_phase
2858:        from SSS_EnsembleTab import trade_contribution_by_phase
3426:        from SSS_EnsembleTab import risk_valve_backtest
3839:            from SSS_EnsembleTab import risk_valve_backtest
I'll check if the perf_stats function is available and contains necessary imports.

root@9a3d3c80743e:/workspace/SSS096# sed -n '681,722p' SSS_EnsembleTab.py
    # æ§‹å»ºæ¯æ—¥ç‹€æ…‹ DataFrame
    daily_state = pd.DataFrame({
        'equity': E,
        'cash': cash,
        'w': w.reindex(open_px.index).fillna(0),
        'position_value': E - cash
    })

    return E, trades_df, daily_state, trade_ledger_df

def perf_stats(equity: pd.Series, w: pd.Series) -> Dict[str, float]:
    """è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
    if len(equity) < 2:
        return {}

    # è¨ˆç®—æ—¥å ±é…¬ç‡
    r = equity.pct_change().dropna()

    # åŸºæœ¬æŒ‡æ¨™
    total_return = (equity.iloc[-1] / equity.iloc[0]) - 1
    annual_return = total_return * (252 / len(equity))

    # æœ€å¤§å›æ’¤
    cummax = equity.cummax()
    drawdown = (equity - cummax) / cummax
    max_drawdown = drawdown.min()

    # é¢¨éšªèª¿æ•´æŒ‡æ¨™
    sharpe_ratio = r.mean() / r.std() * np.sqrt(252) if r.std() > 0 else 0
    calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown != 0 else 0
    sortino_ratio = r.mean() / r[r < 0].std() * np.sqrt(252) if len(r[r < 0]) > 0 and r[r < 0].std() > 0 else 0

    # å…¶ä»–æŒ‡æ¨™
    time_in_market = (w > 0.5).mean()
    turnover_py = w.diff().abs().sum() / len(w) * 252

    return {
        'total_return': total_return,
        'annual_return': annual_return,
        'max_drawdown': max_drawdown,
        'sharpe_ratio': sharpe_ratio,
        'calmar_ratio': calmar_ratio,
root@9a3d3c80743e:/workspace/SSS096# sed -n '722,780p' SSS_EnsembleTab.py
        'calmar_ratio': calmar_ratio,
        'sortino_ratio': sortino_ratio,
        'time_in_market': time_in_market,
        'turnover_py': turnover_py,
        'num_trades': len(w.diff()[w.diff() != 0])
    }

# ---------------------------------------------------------------------
# ä¸»æµç¨‹ï¼šè®€åƒ¹ã€è®€å­ç­–ç•¥éƒ¨ä½ -> ç”Ÿæˆæ¬Šé‡ -> æ¬Šç›Šèˆ‡äº¤æ˜“ -> è¼¸å‡º
# ---------------------------------------------------------------------

@dataclass
class RunConfig:
    ticker: str
    method: str                  # "majority" | "proportional"
    strategies: List[str] | None = None  # None => è‡ªå‹•å¾ trades_* æ¨æ–·
    params: EnsembleParams = None
    cost: CostParams = None      # é è¨­ä¸åŠ æˆæœ¬ï¼›è¦è²¼è¿‘å¯¦ç›¤å¯èª¿æ•´
    file_map: Dict[str, Path] = None  # ç­–ç•¥å -> æ–‡ä»¶è·¯å¾‘çš„æ˜ å°„
    majority_k_pct: float = None  # æ¯”ä¾‹é–€æª»ï¼ˆ0.0~1.0ï¼‰ï¼Œå„ªå…ˆæ–¼å›ºå®š majority_k

    def __post_init__(self):
        if self.params is None:
            self.params = EnsembleParams()
        if self.cost is None:
            self.cost = CostParams()

def run_ensemble(cfg: RunConfig) -> Tuple[pd.Series, pd.Series, pd.DataFrame, Dict[str, float], str, pd.Series, pd.DataFrame, pd
.DataFrame]:
    """å›å‚³ï¼š(open åƒ¹), (æ¯æ—¥æ¬Šé‡ w), (äº¤æ˜“ç´€éŒ„ trades), (ç¸¾æ•ˆæŒ‡æ¨™ dict), (æ–¹æ³•åç¨±), (æ¬Šç›Šæ›²ç·š), (æ¯æ—¥ç‹€æ…‹ daily_state), (äº¤æ˜“æµ
æ°´å¸³ trade_ledger)"""
    # è®€åƒ¹ï¼ˆOpenï¼‰
    px_path = DATA_DIR / f"{cfg.ticker.replace(':','_')}_data_raw.csv"
    px = _read_market_csv_auto(px_path)

    # èª¿è©¦ä¿¡æ¯ï¼šåƒ¹æ ¼æ•¸æ“š
    logger.info(f"[Ensemble] åƒ¹è³‡æ–™å¤©æ•¸={len(px)}, é¦–æœ«={px.index.min()}~{px.index.max()}")

    # æ¨æ–·ç­–ç•¥åˆ—è¡¨
    if cfg.strategies is None:
        # å„ªå…ˆä½¿ç”¨ trades_from_results_*.csvï¼ˆ120æª”ç­–ç•¥ï¼‰
        strat_names = []
        file_map = {}  # ç­–ç•¥å -> æ–‡ä»¶è·¯å¾‘çš„æ˜ å°„

        # å…ˆæ‰¾ trades_from_results_*.csv
        trades_files = list(OUT_DIR.glob("trades_from_results_*.csv"))
        if trades_files:
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_from_results_*.csv æ–‡ä»¶ï¼ˆ120æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_from_results_", "")
                strat_names.append(name)
                file_map[name] = f
        else:
            # æ‰¾ä¸åˆ°å†ä½¿ç”¨èˆŠçš„ trades_*.csvï¼ˆ11æª”ç­–ç•¥ï¼‰
            trades_files = list(OUT_DIR.glob("trades_*.csv"))
            logger.info(f"[Ensemble] æ‰¾åˆ° {len(trades_files)} å€‹ trades_*.csv æ–‡ä»¶ï¼ˆ11æª”ç­–ç•¥ï¼‰")
            for f in sorted(trades_files):
                # å¾æ–‡ä»¶åæ¨æ–·ç­–ç•¥åç¨±
                name = f.stem.replace("trades_", "")
                strat_names.append(name)
After applying global risk valve, I'll recompute metrics before removing the old ones.

I'll dynamically import perf_stats, ensure comments are in Traditional Chinese, and handle test checks.

I'll ensure metrics are recomputed after the global risk valve without removing it prematurely.

I'll run date to confirm the current date and include it in the comment.

root@9a3d3c80743e:/workspace/SSS096# date
Wed Aug 20 01:22:56 UTC 2025
I'll add a timestamp to the comment in Traditional Chinese. Root AGENTS instructions apply here.

I'll confirm the storage and refreshing of metrics after recomputing, then check tests folder.

root@9a3d3c80743e:/workspace/SSS096# ls test
__init___.py  __pycache__  pytest.ini  test_payload_smoke.py
I'll list files in the 'tools' directory to see if quick_check.ps1 is there.

root@9a3d3c80743e:/workspace/SSS096# ls tools
quick_check.ps1
I'll try running pwsh first, then file the commit with changes and relevant message.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1258,1288p' app_dash.py
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
                        elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")

                        result_cap = risk_valve_backtest(
                            open_px=open_px,
                            w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])
root@9a3d3c80743e:/workspace/SSS096# sed -n '1288,1328p' app_dash.py
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)

    # çµ±ä¸€ä½¿ç”¨ orient="split" åºåˆ—åŒ–ï¼Œç¢ºä¿ä¸€è‡´æ€§
    payload = {
I'll modify the code to add calculations after clearing caches but before _risk_valve_applied.

root@9a3d3c80743e:/workspace/SSS096# rg "return {" -n SSS_EnsembleTab.py | head
rg: regex parse error:
    (?:return {)
               ^
error: repetition quantifier expects a valid decimal
I realized I misused regex. I'll escape or use rg "return \{" instead.

root@9a3d3c80743e:/workspace/SSS096# rg "return \{" -n SSS_EnsembleTab.py
694:        return {}
717:    return {
1397:    return {
I'll inspect the code around line 1390 to evaluate the risk_valve_backtest return.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1390,1405p' SSS_EnsembleTab.py
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)

    return {
        "signals": sig,
        "weights_orig": w,
        "weights_valve": w2,
        "daily_state_orig": daily_state1,
        "daily_state_valve": daily_state2,
        "trade_ledger_orig": ledger1,
        "trade_ledger_valve": ledger2,
        "metrics": {
root@9a3d3c80743e:/workspace/SSS096# sed -n '1405,1428p' SSS_EnsembleTab.py
        "metrics": {
            "pf_orig": float(pf1), "pf_valve": float(pf2),
            "mdd_orig": float(mdd1), "mdd_valve": float(mdd2),
            "right_tail_sum_orig": float(rt1), "right_tail_sum_valve": float(rt2),
            "right_tail_reduction": float(right_tail_reduction)
        }
    }


# === PATCH: äº¤æ˜“è²¢ç»æ‹†è§£ï¼ˆæŒ‰åŠ ç¢¼/æ¸›ç¢¼éšæ®µï¼‰===
def trade_contribution_by_phase(trade_ledger: pd.DataFrame,
                                daily_equity: pd.Series,
                                min_spacing_days: int = 0,
                                cooldown_days: int = 0) -> pd.DataFrame:
    """å°‡äº¤æ˜“ä¾ã€æ¬Šé‡è®ŠåŒ–æ­£/è² ã€åˆ†æ®µï¼Œçµ±è¨ˆå„éšæ®µçš„è³£å‡ºå¯¦ç¾å ±é…¬èˆ‡è©²æ®µå…§çš„ MDDã€‚"""
    tl = trade_ledger.copy()
    # æ¬„å°é½Š
    for a,b in [("date","äº¤æ˜“æ—¥æœŸ"),("type","äº¤æ˜“é¡å‹"),("w_before","äº¤æ˜“å‰æ¬Šé‡"),("w_after","äº¤æ˜“å¾Œæ¬Šé‡")]:
        if a in tl.columns: tl.rename(columns={a:b}, inplace=True)
    tl["äº¤æ˜“æ—¥æœŸ"] = pd.to_datetime(tl["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
    tl = tl.dropna(subset=["äº¤æ˜“æ—¥æœŸ"]).sort_values("äº¤æ˜“æ—¥æœŸ").reset_index(drop=True)
    tl["dw"] = pd.to_numeric(tl["äº¤æ˜“å¾Œæ¬Šé‡"], errors="coerce") - pd.to_numeric(tl["äº¤æ˜“å‰æ¬Šé‡"], errors="coerce")

    # å¯é¸ï¼šæ–½åŠ æœ€å°é–“è·/å†·å»ï¼ˆåœ¨åˆ†æ®µå‰å°éå¯†è²·å–®é™å™ªï¼‰
Iâ€™m adding the necessary import, ensuring all metrics are computed, and fixing indentation.

# è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result
It seems like I'll need to use container.feed_chars with patch-style editing, but apply_patch isnâ€™t available. I'll work with tools like sed, python, or ed for modifications instead.

I'll try using python to modify files. Patch is more straightforward, if available.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1296,1326p' app_dash.py
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)
I'll add code after the loop to clear caches, then follow it with a patch.

root@9a3d3c80743e:/workspace/SSS096# perl -0 -i -pe 's/(for k in \[\'trades_ui\', \"trade_df\", \"trade_ledger_std\", \"metrics\
"\]:\n\s+if k in result:\n\s+result.pop\(k, None\)\n\n)/$1
bash: syntax error near unexpected token `)'
root@9a3d3c80743e:/workspace/SSS096#                         # 2025-08-20 ä¿®å¾©ï¼šé‡ç®—ç¸¾æ•ˆæŒ‡æ¨™é¿å…æ¸…ç©ºå¾Œåƒ…å‰©å¹³å‡æŒå€‰å¤©æ•¸\n
                try:\n                            from SSS_EnsembleTab import perf_stats\n                            eq_series
= result_cap.get(\'daily_state_valve\', pd.DataFrame())\n                            w_series = result_cap.get(\'weights_valve\'
, pd.Series(dtype=float))\n                            if isinstance(eq_series, pd.DataFrame) and \'equity\' in eq_series.column
s:\n                                result[\'metrics\'] = perf_stats(eq_series[\'equity\'], w_series)\n
       logger.info(f"[{strat}] é‡æ–°è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™å®Œæˆ")\n                        except Exception as e:\n
   logger.warning(f"[{strat}] é‡ç®—ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}")\n\n/' app_dash.py
Looks like I need to reopen the file to make sure nothing got corrupted.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1296,1326p' app_dash.py
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)
The patch command failed, so I'll implement the fix using Python, but simplifying it with ed or sed instead.

root@9a3d3c80743e:/workspace/SSS096# python - <<'PY'
> from pathlib import Path
> import pandas as pd
> file = Path('app_dash.py')
> text = file.read_text(encoding='utf-8').splitlines()
> for i,line in enumerate(text):
>     if line.strip() == "for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:":
>         insert_index = i + 2  # after the 'result.pop' line (next line is the 'for' block's second line, not including blank?)
>         # but we will search for next blank line after the for-loop block
>         j = i + 1
>         while j < len(text) and text[j].startswith(' ' * 28):
>             j += 1
>         insert_index = j
>         snippet = [
>             "                        # 2025-08-20 ä¿®å¾©ï¼šé‡ç®—ç¸¾æ•ˆæŒ‡æ¨™é¿å…æ¸…ç©ºå¾Œåƒ…å‰©å¹³å‡æŒå€‰å¤©æ•¸",
>             "                        try:",
>             "                            from SSS_EnsembleTab import perf_stats",
>             "                            eq_series = result_cap.get('daily_state_valve', pd.DataFrame())",
>             "                            w_series = result_cap.get('weights_valve', pd.Series(dtype=float))",
>             "                            if isinstance(eq_series, pd.DataFrame) and 'equity' in eq_series.columns:",
>             "                                result['metrics'] = perf_stats(eq_series['equity'], w_series)",
>             "                                logger.info(f'[{strat}] é‡æ–°è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™å®Œæˆ')",
>             "                        except Exception as e:",
>             "                            logger.warning(f'[{strat}] é‡ç®—ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}')",
>             ""
>         ]
>         text[insert_index:insert_index] = snippet
>         break
> file.write_text('\n'.join(text), encoding='utf-8')
> PY
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
ModuleNotFoundError: No module named 'pandas'
The script failed due to the missing pandas import. Iâ€™ll remove it and retry.

root@9a3d3c80743e:/workspace/SSS096# python - <<'PY'
> from pathlib import Path
> file = Path('app_dash.py')
> text = file.read_text(encoding='utf-8').splitlines()
> for i,line in enumerate(text):
>     if line.strip() == "for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:":
>         j = i + 1
>         while j < len(text) and text[j].startswith(' ' * 28):
>             j += 1
>         insert_index = j
>         snippet = [
>             "                        # 2025-08-20 ä¿®å¾©ï¼šé‡ç®—ç¸¾æ•ˆæŒ‡æ¨™é¿å…æ¸…ç©ºå¾Œåƒ…å‰©å¹³å‡æŒå€‰å¤©æ•¸",
>             "                        try:",
>             "                            from SSS_EnsembleTab import perf_stats",
>             "                            eq_series = result_cap.get('daily_state_valve', pd.DataFrame())",
>             "                            w_series = result_cap.get('weights_valve', pd.Series(dtype=float))",
>             "                            if isinstance(eq_series, pd.DataFrame) and 'equity' in eq_series.columns:",
>             "                                result['metrics'] = perf_stats(eq_series['equity'], w_series)",
>             "                                logger.info(f'[{strat}] é‡æ–°è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™å®Œæˆ')",
>             "                        except Exception as e:",
>             "                            logger.warning(f'[{strat}] é‡ç®—ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}')",
>             ""
>         ]
>         text[insert_index:insert_index] = snippet
>         break
> file.write_text('\n'.join(text), encoding='utf-8')
> PY
The script ran without errors. I'll check the file content now.

root@9a3d3c80743e:/workspace/SSS096# sed -n '1296,1340p' app_dash.py
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)
                        # 2025-08-20 ä¿®å¾©ï¼šé‡ç®—ç¸¾æ•ˆæŒ‡æ¨™é¿å…æ¸…ç©ºå¾Œåƒ…å‰©å¹³å‡æŒå€‰å¤©æ•¸
                        try:
                            from SSS_EnsembleTab import perf_stats
                            eq_series = result_cap.get('daily_state_valve', pd.DataFrame())
                            w_series = result_cap.get('weights_valve', pd.Series(dtype=float))
                            if isinstance(eq_series, pd.DataFrame) and 'equity' in eq_series.columns:
                                result['metrics'] = perf_stats(eq_series['equity'], w_series)
                                logger.info(f'[{strat}] é‡æ–°è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™å®Œæˆ')
                        except Exception as e:
                            logger.warning(f'[{strat}] é‡ç®—ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}')


                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)

    # çµ±ä¸€ä½¿ç”¨ orient="split" åºåˆ—åŒ–ï¼Œç¢ºä¿ä¸€è‡´æ€§
    payload = {
        'results': results,
I confirmed pd is imported globally, and the snippet is correct. Let's check for syntax.

root@9a3d3c80743e:/workspace/SSS096# python -m py_compile app_dash.py
I'll check the git status before committing to make sure everything is ready.

root@9a3d3c80743e:/workspace/SSS096# git status --short
 M app_dash.py
?? __pycache__/
Iâ€™ll remove the __pycache__ directory to keep things tidy.

root@9a3d3c80743e:/workspace/SSS096# rm -r __pycache__
Let me quickly check the git status once more to see if anything's changed.

root@9a3d3c80743e:/workspace/SSS096# git status --short
 M app_dash.py
I'll run tests and try tools/quick_check.ps1, using pwsh or powershell if needed.

root@9a3d3c80743e:/workspace/SSS096# pwsh -ExecutionPolicy Bypass -File tools/quick_check.ps1
bash: command not found: pwsh
root@9a3d3c80743e:/workspace/SSS096# powershell -ExecutionPolicy Bypass -File tools/quick_check.ps1
bash: command not found: powershell
PowerShell isn't installed, so I'll note this in my final message and run pytest.

root@9a3d3c80743e:/workspace/SSS096# pytest -q
============================================================ ERRORS ============================================================
________________________________ ERROR collecting analysis/past/comprehensive_distance_test.py _________________________________
ImportError while importing test module '/workspace/SSS096/analysis/past/comprehensive_distance_test.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/root/.pyenv/versions/3.12.10/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
analysis/past/comprehensive_distance_test.py:14: in <module>
    import pandas as pd
E   ModuleNotFoundError: No module named 'pandas'
____________________________________ ERROR collecting analysis/past/quick_distance_test.py _____________________________________
ImportError while importing test module '/workspace/SSS096/analysis/past/quick_distance_test.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/root/.pyenv/versions/3.12.10/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
analysis/past/quick_distance_test.py:14: in <module>
    import pandas as pd
E   ModuleNotFoundError: No module named 'pandas'
_________________________________________ ERROR collecting test/test_payload_smoke.py __________________________________________
ImportError while importing test module '/workspace/SSS096/test/test_payload_smoke.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/root/.pyenv/versions/3.12.10/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
test/test_payload_smoke.py:3: in <module>
    import pandas as pd
E   ModuleNotFoundError: No module named 'pandas'
=================================================== short test summary info ====================================================
ERROR analysis/past/comprehensive_distance_test.py
ERROR analysis/past/quick_distance_test.py
ERROR test/test_payload_smoke.py
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
3 errors in 0.23s
I'm installing pandas to resolve test failures, assuming numpy is already present in the container.

root@9a3d3c80743e:/workspace/SSS096# pip install pandas numpy
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('
Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /simple/pandas/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('
Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /simple/pandas/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('
Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /simple/pandas/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('
Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /simple/pandas/

WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('
Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden'))': /simple/pandas/
ERROR: Could not find a version that satisfies the requirement pandas (from versions: none)
ERROR: No matching distribution found for pandas
Installation failed due to proxy issues. Can't run tests due to missing dependencies (pandas).

root@9a3d3c80743e:/workspace/SSS096# git status --short
 M app_dash.py
?? analysis/past/__pycache__/comprehensive_distance_test.cpython-312-pytest-8.4.1.pyc
?? analysis/past/__pycache__/quick_distance_test.cpython-312-pytest-8.4.1.pyc
?? test/__pycache__/test_payload_smoke.cpython-312-pytest-8.4.1.pyc
I'll remove the generated pycache files from running pytest.

root@9a3d3c80743e:/workspace/SSS096# rm -r analysis/past/__pycache__ test/__pycache__
Let's check the git status once more to confirm everything's up to date.

root@9a3d3c80743e:/workspace/SSS096# git status --short
 D analysis/past/__pycache__/comprehensive_distance_test.cpython-312-pytest-7.4.4.pyc
 D analysis/past/__pycache__/quick_distance_test.cpython-312-pytest-7.4.4.pyc
 M app_dash.py
 D test/__pycache__/test_payload_smoke.cpython-312-pytest-7.4.4.pyc
 D test/__pycache__/test_payload_smoke.cpython-312.pyc
I'm wondering if these .pyc files are tracked. Let me investigate the repository's baseline.

I need to check if the pycache directory is tracked in the repo. Let's list files.

root@9a3d3c80743e:/workspace/SSS096# ls analysis/past
adaptive_clustering_analysis_v7.py      compare_clustering_methods.py               op_hsa_analysis_v2.py
adaptive_fine_clustering_v16.py         comprehensive_distance_test.py              param_cluster_distribution_analysis.py
analyze_clustering_results.py           fast_adaptive_clustering_v8.py              practical_cluster_analysis_v12.py
bailey_pbo_implementation.py            fine_grained_clustering_v13.py              quick_distance_test.py
balanced_clustering_analysis_v10.py     hierarchical_cluster_stability_analysis.py  risk_enhanced_clustering_analysis_v2.py
balanced_pbo_approach.py                hierarchical_score_analysis_v1.py           simple_fine_clustering_v17.py
check_results_v15.py                    improved_layered_clustering_v6.py           single_analysis.py
cluster_trials_app.py                   improved_supervised_clustering_v5.py        small_cluster_analysis_v11.py
clustering_comparison_analysis_v9.py    multi_stage_clustering_v14.py               supervised_clustering_analysis_v3.py
clustering_interpretation_report_v4.py  op_hsa_analysis.py
I'll check if __pycache__ is listed in .gitignore or previously tracked with git ls-files.

root@9a3d3c80743e:/workspace/SSS096# git ls-files analysis/past | grep __pycache__
analysis/past/__pycache__/comprehensive_distance_test.cpython-312-pytest-7.4.4.pyc
analysis/past/__pycache__/quick_distance_test.cpython-312-pytest-7.4.4.pyc
Hmm, looks like the pycache files were tracked but not listed. I'll restore them from Git.

root@9a3d3c80743e:/workspace/SSS096# git checkout -- analysis/past/__pycache__ test/__pycache__
root@9a3d3c80743e:/workspace/SSS096# git status --short
 M app_dash.py
Ready to commit! Adding the message: "ä¿®å¾©å¥—ç”¨é¢¨éšªé–¥é–€å¾Œç¸¾æ•ˆæŒ‡æ¨™æ¶ˆå¤±". Let's go.

root@9a3d3c80743e:/workspace/SSS096# git commit -am "ä¿®å¾©é¢¨éšªé–¥é–€æ¸…ç©ºç¸¾æ•ˆæŒ‡æ¨™" && git status --short
[work e95a6f3] ä¿®å¾©é¢¨éšªé–¥é–€æ¸…ç©ºç¸¾æ•ˆæŒ‡æ¨™
 1 file changed, 11 insertions(+)
It seems the commit ran successfully, but no git status output appearsâ€”perhaps because there were no changes. I'll run git status again for clarity.

root@9a3d3c80743e:/workspace/SSS096# git status --short
I'll mark relevant lines from app_dash.py and reference the commit where metrics are recomputed.

root@9a3d3c80743e:/workspace/SSS096# nl -ba app_dash.py | sed -n '1296,1350p'
  1296                                  result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
  1297                              except Exception:
  1298                                  # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
  1299                                  result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equ
ity'))
  1300
  1301                          # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
  1302                          for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
  1303                              if k in result:
  1304                                  result.pop(k, None)
  1305                          # 2025-08-20 ä¿®å¾©ï¼šé‡ç®—ç¸¾æ•ˆæŒ‡æ¨™é¿å…æ¸…ç©ºå¾Œåƒ…å‰©å¹³å‡æŒå€‰å¤©æ•¸
  1306                          try:
  1307                              from SSS_EnsembleTab import perf_stats
  1308                              eq_series = result_cap.get('daily_state_valve', pd.DataFrame())
  1309                              w_series = result_cap.get('weights_valve', pd.Series(dtype=float))
  1310                              if isinstance(eq_series, pd.DataFrame) and 'equity' in eq_series.columns:
  1311                                  result['metrics'] = perf_stats(eq_series['equity'], w_series)
  1312                                  logger.info(f'[{strat}] é‡æ–°è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™å®Œæˆ')
  1313                          except Exception as e:
  1314                              logger.warning(f'[{strat}] é‡ç®—ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}')
  1315
  1316
  1317                          # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
  1318                          result['_risk_valve_applied'] = True
  1319                          result['_risk_valve_params'] = {
  1320                              'cap': float(risk_cap),
  1321                              'atr_ratio': float(atr_ratio),
  1322                              'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
  1323                              'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
  1324                          }
  1325
  1326                          true_days = int(mask_aligned.sum())
  1327                          logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
  1328                      else:
  1329                          logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")
  1330
  1331          results[strat] = result
  1332
  1333      # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
  1334      first_strat = list(results.keys())[0] if results else strategy_names[0]
  1335      first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
  1336      df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)
  1337
  1338      # çµ±ä¸€ä½¿ç”¨ orient="split" åºåˆ—åŒ–ï¼Œç¢ºä¿ä¸€è‡´æ€§
  1339      payload = {
  1340          'results': results,
  1341          'df_raw': df_raw_main.to_json(date_format='iso', orient='split'),
  1342          'ticker': ticker
  1343      }
  1344
  1345      # é˜²å®ˆæ€§æª¢æŸ¥ï¼šå¦‚é‚„æœ‰æ¼ç¶²çš„éåºåˆ—åŒ–ç‰©ä»¶å°±èƒ½ææ—©çœ‹å‡º
  1346      try:
  1347          json.dumps(payload)
  1348      except Exception as e:
  1349          logger.exception("[BUG] backtest-store payload ä»å«ä¸å¯åºåˆ—åŒ–ç‰©ä»¶ï¼š%s", e)
  1350          # å¦‚æœè¦å¼·åˆ¶ä¸å™´ï¼Œå¯åš fallbackï¼šjson.dumps(..., default=str) ä½†é€šå¸¸ä¸å»ºè­°åæ‰
I'll summarize the fix, reference relevant lines, and mention the failed tests due to missing dependencies.

The testing summary needs commands with emojis. I'll list them in the right order.

Iâ€™ll include the fix and reference the relevant lines for the performance metrics.

4.
Configuring language runtimes...
# Python: 3.12
# Node.js: v20 (default: v22)
default -> 20 (-> v20.19.4)
Now using node v20.19.4 (npm v10.9.3)
# Ruby: 3.4.4 (default: 3.2.3)
mise ruby@3.4.4                                                            â   0s
mise ~/.config/mise/config.toml tools: ruby@3.4.4
# Rust: 1.88.0 (default: 1.89.0)
info: using existing install for '1.88.0-x86_64-unknown-linux-gnu'
info: default toolchain set to '1.88.0-x86_64-unknown-linux-gnu'

  1.88.0-x86_64-unknown-linux-gnu unchanged - rustc 1.88.0 (6b00bc388 2025-06-23)

# Go: go1.24.3 (default: go1.24.3)
# Swift: 6.1 (default: 6.1)
# PHP: 8.4 (default: 8.4)
Running auto setup...
Scanning repository (up to depth 3) for dependency configurations...
No known dependency configuration files found up to depth 3.
å¾Œé¢ç‰ˆæœ¬ä¸æ˜¯æ–°å¢äº†é¢¨éšªé–¥é–€å¥—ç”¨åœ¨å„ç­–ç•¥å—
ä½†ä¸çŸ¥é“ç‚ºä»€éº¼å¥—ç”¨å¾Œ å„æŒ‡æ¨™æœƒåªå‰©ä¸‹å¹³å‡æŒå€‰å¤©æ•¸,å…¶ä»–æŒ‡æ¨™éƒ½æ¶ˆå¤±äº† è«‹ä¿®å¾©


^C
root@a3bd66c837b3:/workspace# cd SSS096
root@a3bd66c837b3:/workspace/SSS096# ls
AGENTS.md                           data                      run_workflow_example.py
DEFENSIVE_CHECKS_UPDATE_SUMMARY.md  debug_enhanced_data.py    runners
ENSEMBLE_WORKFLOW.md                ensemble_wrapper.py       setup.sh
LOGGING_SYSTEM_UPDATE_SUMMARY.md    extract_params.py         sss_backtest_outputs
PORTFOLIO_LEDGER_README.md          leverage.py               sss_core
SSS_EnsembleTab.py                  list.txt                  test
SSSv096.py                          list_folder_structure.py  tools
analysis                            presets                   utils_payload.py
app_dash.py                         ruff.toml                 version_history.py
assets                              run_enhanced_debug.py     workflow_summary_20250812_023903.txt
config.yaml                         run_enhanced_ensemble.py  workflow_summary_20250818_040656.txt
convert_results_to_trades.py        run_workflow.py
I'll look for AGENTS.md at the repository root to check instructions.

root@a3bd66c837b3:/workspace/SSS096# cat AGENTS.md
# SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

SSS096 æ˜¯ä¸€å€‹è‚¡ç¥¨ç­–ç•¥å›æ¸¬èˆ‡åˆ†æç³»çµ±ï¼Œä¸»è¦åŒ…å«ï¼š
- ç­–ç•¥å›æ¸¬å¼•æ“ï¼ˆSSSv096.pyï¼‰
- Web UI ç•Œé¢ï¼ˆapp_dash.pyï¼‰
- å¢å¼·åˆ†ææ¨¡çµ„ï¼ˆanalysis/ï¼‰
- Ensemble ç­–ç•¥åŸ·è¡Œï¼ˆrunners/ï¼‰
- æ•¸æ“šè™•ç†èˆ‡è½‰æ›å·¥å…·

## ğŸ¯ å·¥ä½œé‡é»å€åŸŸ

### æ ¸å¿ƒæª”æ¡ˆ
- `SSSv096.py` - ä¸»è¦ç­–ç•¥å›æ¸¬å¼•æ“
- `app_dash.py` - Web UI ä¸»æ‡‰ç”¨
- `ensemble_wrapper.py` - Ensemble ç­–ç•¥åŒ…è£å™¨
- `analysis/` - åˆ†ææ¨¡çµ„ç›®éŒ„

### é¿å…ä¿®æ”¹çš„æª”æ¡ˆ
- `tools/quick_check.ps1` - è‡ªå‹•åŒ–æª¢æŸ¥è…³æœ¬ï¼ˆé™¤éå¿…è¦ï¼‰
- å·²æ¨™è¨˜ç‚º "past/" çš„èˆŠç‰ˆæœ¬æª”æ¡ˆ
- ç·¨è­¯å¾Œçš„ `.pyc` æª”æ¡ˆ

## ğŸ”§ é–‹ç™¼ç’°å¢ƒè¨­å®š

### Codex ç’°å¢ƒè¨­ç½®ï¼ˆæ¨è–¦ï¼‰
```bash
# åŸ·è¡Œè‡ªå‹•è¨­ç½®è…³æœ¬
chmod +x setup.sh
./setup.sh

# æ¸¬è©¦è¨­ç½®æ˜¯å¦æˆåŠŸ
python test_setup.py
```

### Python ç’°å¢ƒï¼ˆæ‰‹å‹•è¨­ç½®ï¼‰
```bash
# å®‰è£ä¾è³´å¥—ä»¶
pip install pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components

# æª¢æŸ¥ Python ç‰ˆæœ¬ï¼ˆå»ºè­° 3.8+ï¼‰
python --version
```

### ä»£ç†è¨­ç½®ï¼ˆå¦‚æœé‡åˆ° 403 éŒ¯èª¤ï¼‰
```bash
# æª¢æŸ¥ä»£ç†è­‰æ›¸
echo $CODEX_PROXY_CERT

# é…ç½® pip ä½¿ç”¨ä»£ç†
pip config set global.cert "$CODEX_PROXY_CERT"
pip config set global.trusted-host "proxy:8080"
```

### å°ˆæ¡ˆçµæ§‹å°èˆª
```bash
# å¿«é€ŸæŸ¥çœ‹ç›®éŒ„çµæ§‹
python list_folder_structure.py

# æŸ¥çœ‹ç‰¹å®šç›®éŒ„å…§å®¹
ls analysis/
ls runners/
```

## ğŸ§ª æ¸¬è©¦èˆ‡é©—è­‰

### å¿«é€Ÿæª¢æŸ¥
```bash
# åŸ·è¡Œè‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆé‡è¦ï¼ï¼‰
powershell -ExecutionPolicy Bypass -File tools\quick_check.ps1
```

### å›æ¸¬æ¸¬è©¦
```bash
# åŸ·è¡Œå–®ä¸€ç­–ç•¥å›æ¸¬
python SSSv096.py --strategy RMA_Factor --param_preset op.json

# åŸ·è¡Œ Ensemble ç­–ç•¥
python run_enhanced_ensemble.py --method majority --top_k 5
```

### UI æ¸¬è©¦
```bash
# å•Ÿå‹• Web UI
python app_dash.py
```

## ğŸ“ ç¨‹å¼ç¢¼è¦ç¯„

### è¨»è§£èˆ‡è¼¸å‡º
- **ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡**é€²è¡Œè¨»è§£å’Œè¼¸å‡º
- ä¿®æ”¹ç´€éŒ„éœ€åŠ å…¥æ—¥æœŸæ™‚é–“æˆ³è¨˜
- è·¯å¾‘èªªæ˜æ ¼å¼ï¼š`#å­è³‡æ–™å¤¾/æ¡ˆå`

### æ—¥èªŒè¨˜éŒ„
- ä½¿ç”¨ `analysis/logging_config.py` ä¸­çš„æ—¥èªŒå™¨
- é‡è¦æ“ä½œéœ€è¨˜éŒ„åˆ°æ—¥èªŒæª”æ¡ˆ
- éŒ¯èª¤è™•ç†éœ€åŒ…å«è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯

### è³‡æ–™æ ¼å¼
- æ—¥æœŸæ¬„ä½çµ±ä¸€ä½¿ç”¨ ISO æ ¼å¼ï¼š`YYYY-MM-DD`
- æ•¸å€¼æ¬„ä½ä½¿ç”¨ float é¡å‹
- é¿å…ä½¿ç”¨ä¸­æ–‡æ¬„ä½åç¨±ï¼ˆé™¤éå¿…è¦ï¼‰

## ğŸ” é™¤éŒ¯æŒ‡å—

### å¸¸è¦‹å•é¡Œ
1. **æ¨¡çµ„å°å…¥å¤±æ•—**ï¼šæª¢æŸ¥ `sys.path` å’Œç›¸å°å°å…¥
2. **æ•¸æ“šæ ¼å¼éŒ¯èª¤**ï¼šé©—è­‰ CSV æª”æ¡ˆçµæ§‹å’Œæ¬„ä½åç¨±
3. **è¨˜æ†¶é«”ä¸è¶³**ï¼šæª¢æŸ¥å¤§æ•¸æ“šé›†çš„è™•ç†æ–¹å¼

### é™¤éŒ¯å·¥å…·
```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(level=logging.DEBUG)

# æª¢æŸ¥æ•¸æ“šçµæ§‹
print(df.info())
print(df.head())
```

## ğŸ“Š æ•¸æ“šè™•ç†è¦ç¯„

### è¼¸å…¥æ•¸æ“š
- æ”¯æ´ CSVã€Excelã€JSON æ ¼å¼
- å¿…è¦æ¬„ä½ï¼šäº¤æ˜“æ—¥æœŸã€æ¬Šé‡è®ŠåŒ–ã€ç›ˆè™§%
- å¯é¸æ¬„ä½ï¼šäº¤æ˜“é¡å‹ã€åƒ¹æ ¼ã€æˆäº¤é‡

### è¼¸å‡ºæ•¸æ“š
- çµ±ä¸€ Schemaï¼šequityã€tradesã€daily_stateã€trade_ledger
- é¿å… KeyError å’Œæ¬„ä½ç¼ºå¤±
- æ”¯æ´å¤šç¨®è¼¸å‡ºæ ¼å¼

## ğŸš€ éƒ¨ç½²èˆ‡ç¶­è­·

### æª”æ¡ˆç®¡ç†
- å®šæœŸæ¸…ç†èˆŠçš„æ—¥èªŒå’Œå¿«å–æª”æ¡ˆ
- å‚™ä»½é‡è¦çš„é…ç½®å’Œçµæœæª”æ¡ˆ
- ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶è¿½è¹¤è®Šæ›´

### æ€§èƒ½å„ªåŒ–
- å¤§æ•¸æ“šé›†ä½¿ç”¨å¿«å–æ©Ÿåˆ¶
- é¿å…é‡è¤‡è¨ˆç®—
- ä½¿ç”¨é©ç•¶çš„æ•¸æ“šçµæ§‹

## âš ï¸ æ³¨æ„äº‹é …

### å®‰å…¨è€ƒé‡
- ä¸è¦ç¡¬ç·¨ç¢¼ API é‡‘é‘°
- é©—è­‰æ‰€æœ‰ç”¨æˆ¶è¼¸å…¥
- ä¿è­·æ•æ„Ÿæ•¸æ“š

### ç›¸å®¹æ€§
- ç¶­æŒèˆ‡ç¾æœ‰å·¥ä½œæµç¨‹çš„ç›¸å®¹æ€§
- æ¸¬è©¦æ‰€æœ‰æ•´åˆé»
- é¿å…ç ´å£ç¾æœ‰åŠŸèƒ½

## ğŸš€ Codex ç’°å¢ƒè¨­ç½®

### è‡ªå‹•è¨­ç½®
```bash
# åŸ·è¡Œè¨­ç½®è…³æœ¬
./setup.sh

# è…³æœ¬æœƒè‡ªå‹•ï¼š
# 1. é…ç½®ä»£ç†è¨­ç½®
# 2. å®‰è£æ‰€æœ‰å¿…è¦ä¾è³´
# 3. å‰µå»ºå›é€€æ—¥èªŒç³»çµ±
# 4. è¨­ç½®ç’°å¢ƒè®Šæ•¸
```

### æ‰‹å‹•è¨­ç½®ï¼ˆå¦‚æœè‡ªå‹•è¨­ç½®å¤±æ•—ï¼‰
```bash
# 1. é…ç½®ä»£ç†
export PIP_CERT="$CODEX_PROXY_CERT"
export NODE_EXTRA_CA_CERTS="$CODEX_PROXY_CERT"

# 2. å®‰è£æ ¸å¿ƒä¾è³´
pip install --no-cache-dir pandas numpy matplotlib seaborn openpyxl dash dash-bootstrap-components yfinance pyyaml joblib

# 3. å®‰è£åˆ†æå¥—ä»¶
pip install --no-cache-dir scikit-learn scipy statsmodels plotly kaleido

# 4. å‰µå»ºå¿…è¦ç›®éŒ„
mkdir -p analysis/log analysis/cache cache log results sss_backtest_outputs
```

### å¸¸è¦‹å•é¡Œè§£æ±º
- **pip install 403 éŒ¯èª¤**ï¼šæª¢æŸ¥ `$CODEX_PROXY_CERT` ç’°å¢ƒè®Šæ•¸
- **joblib å°å…¥å¤±æ•—**ï¼šä½¿ç”¨ `logging_config_fallback.py` å›é€€ç‰ˆæœ¬
- **æ¨¡çµ„è·¯å¾‘å•é¡Œ**ï¼šè¨­ç½® `PYTHONPATH` ç’°å¢ƒè®Šæ•¸

## ğŸ“ æŠ€è¡“æ”¯æ´

### å•é¡Œå›å ±
- æä¾›å®Œæ•´çš„éŒ¯èª¤è¨Šæ¯å’Œå †ç–Šè¿½è¹¤
- åŒ…å«é‡ç¾æ­¥é©Ÿå’Œç’°å¢ƒä¿¡æ¯
- æª¢æŸ¥ç›¸é—œçš„æ—¥èªŒæª”æ¡ˆ

### æ–‡æª”æ›´æ–°
- ä¿®æ”¹åŠŸèƒ½æ™‚åŒæ­¥æ›´æ–°ç›¸é—œæ–‡æª”
- ä½¿ç”¨æ¸…æ™°çš„ç¯„ä¾‹å’Œèªªæ˜
- ä¿æŒæ–‡æª”çš„æ™‚æ•ˆæ€§

---

**ç‰ˆæœ¬ï¼š** v1.0
**æœ€å¾Œæ›´æ–°ï¼š** 2025-08-18
**é©ç”¨æ–¼ï¼š** SSS096 å°ˆæ¡ˆ AI ä»£ç†å·¥ä½œæŒ‡å—
I'll search for AGENTS.md in the directories I might modify to ensure compliance.

root@a3bd66c837b3:/workspace/SSS096# find . -name AGENTS.md -print
./AGENTS.md
I'll start by using ripgrep to search for "risk valve" or "é¢¨éšªé–¥é–€" in the code.

root@a3bd66c837b3:/workspace/SSS096# rg -i "é¢¨éšª" -n
SSS_EnsembleTab.py
708:    # é¢¨éšªèª¿æ•´æŒ‡æ¨™
1280:    # é¢¨éšªè§¸ç™¼ï¼šå…©æ–œç‡çš† < é–€æª»ï¼Œä¸” ATR æŠ¬é ­è¶…éå€ç‡
1287:# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
1292:    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š
1293:       - cap: é¢¨éšªæ—¥å°‡ w é™åœ¨ cap_level ä»¥ä¸‹
1294:       - ban_add: é¢¨éšªæ—¥ç¦æ­¢ã€Œæé«˜ã€å€‰ä½ï¼ˆå…è¨±æ¸›å€‰ï¼‰ï¼Œç”¨è¿­ä»£æ³•ç¢ºä¿å–®èª¿"""

DEFENSIVE_CHECKS_UPDATE_SUMMARY.md
36:- åœ¨ç¹ªè£½é¢¨éšªé–¥é–€æ™‚åºåœ–å‰æª¢æŸ¥å¿…è¦æ¬„ä½

SSSv096.py
2689:        # è¨ˆç®—ä¸‹è¡Œé¢¨éšª

analysis/OSv3_6270833.py
396:    è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œçš„å ±é…¬ç‡æŒ‡æ¨™
414:    # ç´¢æè«¾æ¯”ç‡ï¼ˆåªè€ƒæ…®ä¸‹è¡Œé¢¨éšªï¼‰
426:    # ç¶œåˆè©•åˆ†ï¼ˆçµåˆå ±é…¬ç‡å’Œé¢¨éšªï¼‰
872:                # è¨ˆç®—é¢¨éšªèª¿æ•´æŒ‡æ¨™
927:                    **risk_adjusted_metrics  # æ·»åŠ é¢¨éšªèª¿æ•´æŒ‡æ¨™

analysis/OSv3.py
52:        'sharpe_ratio': 'å¤æ™®å€¼ï¼šè¶…é¡å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬',
59:        'overfitting_score': 'éæ“¬åˆåˆ†æ•¸ï¼š0-100ï¼Œåˆ†æ•¸è¶Šé«˜è¡¨ç¤ºéæ“¬åˆé¢¨éšªè¶Šå¤§',
63:        'overfitting_risk': 'éæ“¬åˆé¢¨éšªï¼šç¶œåˆéæ“¬åˆé¢¨éšªè©•åˆ†ï¼Œ0-100',
88:        # é¢¨éšªåˆ†å¸ƒ
89:        'low_risk': 'ä½é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšªâ‰¤30çš„ç­–ç•¥æ¯”ä¾‹',
90:        'medium_risk': 'ä¸­é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª31-60çš„ç­–ç•¥æ¯”ä¾‹',
91:        'high_risk': 'é«˜é¢¨éšªæ¯”ä¾‹ï¼šéæ“¬åˆé¢¨éšª>60çš„ç­–ç•¥æ¯”ä¾‹',
92:        'risk_return_correlation': 'é¢¨éšªå ±é…¬ç›¸é—œæ€§ï¼šéæ“¬åˆé¢¨éšªèˆ‡ç¸½å ±é…¬ç‡çš„ç›¸é—œä¿‚æ•¸'
564:            # è¨ˆç®—é¢¨éšªç­‰ç´š
673:    è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œçš„å ±é…¬ç‡æŒ‡æ¨™
691:    # ç´¢æè«¾æ¯”ç‡ï¼ˆåªè€ƒæ…®ä¸‹è¡Œé¢¨éšªï¼‰
703:    # ç¶œåˆè©•åˆ†ï¼ˆçµåˆå ±é…¬ç‡å’Œé¢¨éšªï¼‰
1622:                # è¨ˆç®—é¢¨éšªèª¿æ•´æŒ‡æ¨™
1677:                    **risk_adjusted_metrics  # æ·»åŠ é¢¨éšªèª¿æ•´æŒ‡æ¨™
1711:                    - ä½ç›¸é—œæ€§ï¼ˆæ¥è¿‘0ï¼‰: é©åˆçµ„åˆæŠ•è³‡ï¼Œåˆ†æ•£é¢¨éšª
1712:                    - é«˜ç›¸é—œæ€§ï¼ˆæ¥è¿‘Â±1ï¼‰: çµ„åˆæ•ˆæœæœ‰é™ï¼Œé¢¨éšªé›†ä¸­
1972:                    - é¢¨éšªèª¿æ•´å ±é…¬: å¤æ™®æ¯”*ç¸½å ±é…¬ç‡
1973:                    - ç¶œåˆè©•åˆ†: sqrt(ç¸½å ±é…¬ç‡)*10*0.3+å¤æ™®å€¼*0.25+æœ€å¤§å›æ’¤*0.2+å‹ç‡*0.15+é¢¨éšªèª¿æ•´å ±é…¬*0.1-éæ“¬åˆåˆ†æ•¸*0.5
1984:                        # è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œå ±é…¬ç‡
1996:                            "é¢¨éšªèª¿æ•´å ±é…¬": risk_adjusted_return
2039:                                overfitting_scores[name] = 50  # é è¨­ä¸­ç­‰é¢¨éšª
2041:                            overfitting_scores[name] = 50  # é è¨­ä¸­ç­‰é¢¨éšª
2049:                        np.sqrt(summary_df['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05
2051:                    print(f"ç¸½å ±é…¬ç‡åˆ†æ•¸: {np.sqrt(summary_df['ç¸½å ±é…¬ç‡']) * 10 * 0.3},å¤æ™®å€¼åˆ†æ•¸: {summary_df['å¤æ™®å€¼'] *
0.25},æœ€å¤§å›æ’¤åˆ†æ•¸: {(1 + summary_df['æœ€å¤§å›æ’¤']) * 0.2},å‹ç‡åˆ†æ•¸: {summary_df['å‹ç‡'] * 0.1},é¢¨éšªèª¿æ•´å ±é…¬åˆ†æ•¸: {np.sqrt(summary
_df['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05}")
2071:                        'é¢¨éšªèª¿æ•´å ±é…¬': '{:.3f}',
2091:                        # å ±é…¬ç‡ vs é¢¨éšªæ•£é»åœ–
2099:                            title="å ±é…¬ç‡ vs é¢¨éšªæ•£é»åœ–",
2180:                        **éæ“¬åˆåˆ†æ•¸**: 0-100ï¼Œåˆ†æ•¸è¶Šé«˜è¡¨ç¤ºéæ“¬åˆé¢¨éšªè¶Šå¤§
2181:                        - 0-30: ä½é¢¨éšªï¼Œç­–ç•¥è¼ƒç©©å®š
2182:                        - 31-60: ä¸­é¢¨éšªï¼Œéœ€è¦é—œæ³¨
2183:                        - 61-100: é«˜é¢¨éšªï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆå•é¡Œ
2389:                    **æœ€å¤§å›æ’¤**: æ¬Šç›Šæ›²ç·šå¾å³°å€¼åˆ°è°·å€¼çš„æœ€å¤§è·Œå¹…ï¼Œè¡¡é‡ä¸‹è¡Œé¢¨éšª
2391:                    **å¤æ™®å€¼**: è¶…é¡å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬
2393:                    **å¡ç‘ªå€¼**: å¹´åŒ–å ±é…¬ç‡èˆ‡æœ€å¤§å›æ’¤çš„æ¯”å€¼ï¼Œè¡¡é‡é¢¨éšªèª¿æ•´å¾Œå ±é…¬
2449:                        # è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œå ±é…¬ç‡
2450:                        optuna_results['é¢¨éšªèª¿æ•´å ±é…¬'] = optuna_results['sharpe_ratio'] * optuna_results['total_return']
2458:                            np.sqrt(optuna_results['é¢¨éšªèª¿æ•´å ±é…¬']) * 0.05

analysis/enhanced_trade_analysis.py
4:æ•´åˆé¢¨éšªé–¥é–€å›æ¸¬ã€äº¤æ˜“è²¢ç»æ‹†è§£ã€åŠ ç¢¼æ¢¯åº¦å„ªåŒ–
97:        é¢¨éšªé–¥é–€å›æ¸¬
100:            risk_rules: é¢¨éšªè¦å‰‡å­—å…¸ï¼Œé è¨­åŒ…å«TWIIæ–œç‡å’ŒATRè¦å‰‡
109:        print("=== é¢¨éšªé–¥é–€å›æ¸¬åˆ†æ ===")
112:        # æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼
115:        # è¨ˆç®—é¢¨éšªé–¥é–€æ•ˆæœ
121:        """æ¨¡æ“¬é¢¨éšªé–¥é–€è§¸ç™¼"""
123:            print("è­¦å‘Šï¼šç¼ºå°‘åŸºæº–æ•¸æ“šï¼Œç„¡æ³•é€²è¡Œé¢¨éšªé–¥é–€å›æ¸¬")
179:        # é¢¨éšªé–¥é–€è§¸ç™¼æ¢ä»¶
184:        # ç¶œåˆé¢¨éšªé–¥é–€
194:        """è¨ˆç®—é¢¨éšªé–¥é–€å°ç¸¾æ•ˆçš„å½±éŸ¿"""
200:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'æ—¥æœŸ' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
204:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
207:        # æ‰¾å‡ºé¢¨éšªé–¥é–€è§¸ç™¼çš„æœŸé–“
212:        # åˆ†æé¢¨éšªæœŸé–“çš„äº¤æ˜“è¡¨ç¾
243:        print(f"é¢¨éšªé–¥é–€è§¸ç™¼æœŸé–“æ•¸ï¼š{len(risk_periods)}")
244:        print(f"é¢¨éšªæœŸé–“äº¤æ˜“æ•¸ï¼š{len(risk_sells)}")
246:        print("\né¢¨éšªæœŸé–“è¡¨ç¾ï¼š")
486:        # é¢¨éšªæŒ‡æ¨™ï¼ˆæ”¹ç”¨æ¬Šç›Šæ›²ç·šè¨ˆ MDDï¼›æŠŠç™¾åˆ†æ¯”è½‰æˆå°æ•¸ï¼‰
555:        # 1. é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº
559:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘å¿…è¦æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
561:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
575:                               color='red', s=50, alpha=0.8, label='é¢¨éšªé–¥é–€è§¸ç™¼')
578:                ax1.set_title('é¢¨éšªé–¥é–€è§¸ç™¼æ™‚åº')
637:    print("2. åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬ï¼šanalyzer.risk_valve_backtest()")

analysis/past/risk_enhanced_clustering_analysis_v2.py
1:# v2: å¢å¼·å‹é¢¨éšªåˆ†ç¾¤åˆ†æï¼ŒåŠ å…¥å¤šç¨®é¢¨éšªæŒ‡æ¨™èˆ‡æ¬Šç›Šæ›²ç·šç‰¹å¾µ
2:# ä¸»è¦åŠŸèƒ½ï¼šè‡ªå‹•å±•é–‹åƒæ•¸ã€è¨ˆç®—å¤šç¨®é¢¨éšªæŒ‡æ¨™ã€åˆ†ç¾¤èˆ‡å¯è¦–åŒ–
52:    """è¨ˆç®—é¢¨éšªæŒ‡æ¨™"""
67:    # å¤æ™®æ¯”ç‡ (å‡è¨­ç„¡é¢¨éšªåˆ©ç‡ç‚º0)
75:    # ä¸‹è¡Œé¢¨éšª (Downside Risk)
90:    # å°¾éƒ¨é¢¨éšªæŒ‡æ¨™
158:            # è¼‰å…¥æ¬Šç›Šæ›²ç·šä¸¦è¨ˆç®—é¢¨éšªæŒ‡æ¨™
190:            # å°‡é¢¨éšªæŒ‡æ¨™æ·»åŠ åˆ°DataFrame
209:    """åŸ·è¡Œé¢¨éšªå¢å¼·çš„åˆ†ç¾¤åˆ†æ"""
284:    """å‰µå»ºé¢¨éšªåˆ†ç¾¤å¯è¦–åŒ–"""
302:    # 2. é¢¨éšªæŒ‡æ¨™ç®±ç·šåœ–
362:    # 5. é¢¨éšª-æ”¶ç›Šæ•£é»åœ–
390:    """å‰µå»ºé¢¨éšªåˆ†ç¾¤æ¯”è¼ƒåˆ†æ"""
446:    """ç”Ÿæˆé¢¨éšªåˆ†ç¾¤åˆ†æå ±å‘Š"""
448:    report.append("=== é¢¨éšªå¢å¼·åˆ†ç¾¤åˆ†æå ±å‘Š ===\n")
488:    print("é¢¨éšªå¢å¼·åˆ†ç¾¤åˆ†æå ±å‘Šå·²ç”Ÿæˆå®Œæˆï¼")
493:    print("é–‹å§‹é¢¨éšªå¢å¼·åˆ†ç¾¤åˆ†æ...")
501:    # å°æ¯å€‹ç­–ç•¥-æ•¸æ“šæºçµ„åˆé€²è¡Œé¢¨éšªå¢å¼·åˆ†ç¾¤
505:        # åŸ·è¡Œé¢¨éšªå¢å¼·åˆ†ç¾¤
521:            # ä¿å­˜å¸¶æœ‰é¢¨éšªåˆ†ç¾¤æ¨™ç±¤çš„æ•¸æ“š

analysis/past/bailey_pbo_implementation.py
242:        return "å„ªç§€ï¼šéæ“¬åˆé¢¨éšªå¾ˆä½ï¼Œç­–ç•¥ç©©å¥æ€§å¾ˆé«˜"
244:        return "è‰¯å¥½ï¼šéæ“¬åˆé¢¨éšªè¼ƒä½ï¼Œç­–ç•¥è¼ƒç‚ºç©©å¥"
246:        return "ä¸€èˆ¬ï¼šæœ‰ä¸€å®šéæ“¬åˆé¢¨éšªï¼Œéœ€è¦è¬¹æ…"
248:        return "è¼ƒå·®ï¼šéæ“¬åˆé¢¨éšªè¼ƒé«˜ï¼Œå»ºè­°é‡æ–°è¨­è¨ˆç­–ç•¥"
250:        return "å¾ˆå·®ï¼šéæ“¬åˆé¢¨éšªå¾ˆé«˜ï¼Œç­–ç•¥å¯èƒ½ç„¡æ•ˆ"

analysis/past/comprehensive_distance_test.py
223:        # 3. éæ“¬åˆé¢¨éšªåˆ†æ•¸æ¯”è¼ƒ
226:        ax3.set_title('éæ“¬åˆé¢¨éšªåˆ†æ•¸')
227:        ax3.set_ylabel('é¢¨éšªåˆ†æ•¸')

...
306:        return "ä¸€èˆ¬ï¼šæœ‰ä¸€å®šéæ“¬åˆé¢¨éšªï¼Œéœ€è¦è¬¹æ…"
308:        return "è¼ƒå·®ï¼šéæ“¬åˆé¢¨éšªè¼ƒé«˜ï¼Œå»ºè­°é‡æ–°è¨­è¨ˆç­–ç•¥"
310:        return "å¾ˆå·®ï¼šéæ“¬åˆé¢¨éšªå¾ˆé«˜ï¼Œç­–ç•¥å¯èƒ½ç„¡æ•ˆ"

analysis/past/improved_layered_clustering_v6.py
30:    """è¼‰å…¥é¢¨éšªåˆ†ç¾¤å¾Œçš„æ•¸æ“š"""
126:    # æ–¹æ³•1: åªç”¨åƒæ•¸+é¢¨éšªåˆ†ç¾¤ï¼ˆå»é™¤ç¸¾æ•ˆï¼‰
127:    print("æ–¹æ³•1: åªç”¨åƒæ•¸+é¢¨éšªåˆ†ç¾¤")
141:    # åªç”¨åƒæ•¸+é¢¨éšªç‰¹å¾µ
163:    print(f"  åƒæ•¸+é¢¨éšªæœ€ä½³åˆ†ç¾¤æ•¸: {best_k_param_risk} (Silhouette Score: {best_score_param_risk:.3f})")
176:            # åœ¨ç¸¾æ•ˆç¾¤çµ„å…§é€²è¡Œåƒæ•¸+é¢¨éšªåˆ†ç¾¤
210:    # æ–¹æ³•3: éšå±¤å¼åˆ†ç¾¤ï¼ˆåªç”¨åƒæ•¸+é¢¨éšªï¼‰
352:        recommendations.append("âœ“ ç™¼ç¾é‡è¤‡ç¸¾æ•ˆç¾¤çµ„ï¼Œå»ºè­°ä½¿ç”¨åƒæ•¸+é¢¨éšªåˆ†ç¾¤")
371:        print("1. å°æ–¼é‡è¤‡ç¸¾æ•ˆç¾¤çµ„ï¼Œä½¿ç”¨åƒæ•¸+é¢¨éšªåˆ†ç¾¤æ–¹æ³•")
376:        print("2. åƒæ•¸+é¢¨éšªåˆ†ç¾¤ä½œç‚ºè¼”åŠ©é©—è­‰")
388:        print("æœªæ‰¾åˆ°é¢¨éšªåˆ†ç¾¤æ•¸æ“š")

analysis/past/quick_distance_test.py
110:                # æª¢æ¸¬éæ“¬åˆé¢¨éšª
136:                logger.info(f"  éæ“¬åˆé¢¨éšªåˆ†æ•¸: {results[config['name']]['overfitting_risk_score']:.3f}")
174:        # éæ“¬åˆé¢¨éšªåˆ†æ•¸ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
205:            logger.info(f"âœ“ æ”¹é€²æ–¹æ³•ç›¸æ¯”æ¨™æº–æ­æ°è·é›¢é™ä½äº† {improvement:.1f}% çš„éæ“¬åˆé¢¨éšª")

analysis/UI_INTEGRATION_GUIDE.md
11:1. é¢¨éšªé–¥é–€å›æ¸¬
260:- **è‡ªå®šç¾©åŸºæº–æ•¸æ“š**ï¼šä¸Šå‚³åŸºæº–æ•¸æ“šé€²è¡Œé¢¨éšªé–¥é–€åˆ†æ

app_dash.py
445:                        html.Label("é¢¨éšªé–¥é–€ CAP", style={"fontSize":"12px","color":"#888"}),
455:                        dbc.Checkbox(id='force-valve-trigger', value=False, label="å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼ˆæ¸¬è©¦ç”¨ï¼‰", style={"font
Size":"11px","color":"#dc3545"}),
456:                        html.Small("ğŸ’¡ å‹¾é¸å¾Œå°‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€ï¼Œç”¨æ–¼æ¸¬è©¦åŠŸèƒ½", style={"color":"#dc3545","fontSize":"10px"})
460:                    # === é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤ºå€åŸŸ ===
464:                        html.Small("ğŸ”’ é¢¨éšªé–¥é–€èªªæ˜:", style={"color":"#28a745","fontWeight":"bold","fontSize":"11px"}),
465:                        html.Small("â€¢ CAP: æ§åˆ¶æœ€å¤§é¢¨éšªæš´éœ² (0.1=10%, 0.3=30%)", style={"color":"#666","fontSize":"10px"}),
466:                        html.Small("â€¢ ATRæ¯”å€¼: ç•¶çŸ­æœŸæ³¢å‹•>é•·æœŸæ³¢å‹•æ™‚ï¼Œè‡ªå‹•é™ä½é¢¨éšª", style={"color":"#666","fontSize":"10px"
}),
573:# --------- é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ---------
587:    """å‹•æ…‹æ›´æ–°é¢¨éšªé–¥é–€ç‹€æ…‹é¡¯ç¤º"""
588:    logger.info(f"=== é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–° ===")
598:        logger.info("é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
600:            html.Small("ğŸ”´ é¢¨éšªé–¥é–€æœªå•Ÿç”¨", style={"color":"#dc3545","fontWeight":"bold"}),
602:            html.Small("é»æ“Šä¸Šæ–¹è¤‡é¸æ¡†å•Ÿç”¨å…¨å±€é¢¨éšªæ§åˆ¶", style={"color":"#666","fontSize":"10px"})
655:                            # åˆ¤æ–·æ˜¯å¦éœ€è¦è§¸ç™¼é¢¨éšªé–¥é–€
658:                            # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
661:                                logger.info(f"å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
663:                            # è¨˜éŒ„é¢¨éšªé–¥é–€ç‹€æ…‹åˆ°æ—¥èªŒ
665:                            logger.info(f"é¢¨éšªé–¥é–€é–€æª»: {atr_ratio}, ç•¶å‰æ¯”å€¼: {atr_ratio_current:.4f}")
666:                            logger.info(f"é¢¨éšªé–¥é–€è§¸ç™¼: {'æ˜¯' if valve_triggered else 'å¦'}")
667:                            logger.info(f"é¢¨éšªé–¥é–€ç‹€æ…‹: {'ğŸ”´ è§¸ç™¼' if valve_triggered else 'ğŸŸ¢ æ­£å¸¸'}")
680:                                    html.Small(f"{status_icon} é¢¨éšªé–¥é–€ç‹€æ…‹: {status_text}",
688:                                    html.Small(f"é¢¨éšªCAP: {risk_cap*100:.0f}%", style={"color":"#666","fontSize":"11px"}),
742:        logger.error(f"é¢¨éšªé–¥é–€ç‹€æ…‹æ›´æ–°å¤±æ•—: {e}")
797:    # === æ–°å¢ï¼šå…¨å±€é¢¨éšªé–¥é–€è§¸ç™¼ç‹€æ…‹è¿½è¹¤ ===
810:        # æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€è¨­å®šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
811:        logger.info(f"[{strat}] é¢¨éšªé–¥é–€é–‹é—œç‹€æ…‹: global_apply={global_apply}, é¡å‹={type(global_apply)}")
813:            logger.info(f"[{strat}] æ‡‰ç”¨å…¨å±€é¢¨éšªé–¥é–€: CAP={risk_cap}, ATRæ¯”å€¼é–€æª»={atr_ratio}")
836:                        logger.warning(f"[{strat}] ATR(60) å€¼ç•°å¸¸: {atr_60_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
840:                        logger.warning(f"[{strat}] ATR(20) å€¼ç•°å¸¸: {atr_20_latest}ï¼Œè·³éé¢¨éšªé–¥é–€")
851:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
856:            logger.info(f"[{strat}] æœªå•Ÿç”¨å…¨å±€é¢¨éšªé–¥é–€")
864:            # é‡æ–°è¨ˆç®—ç­–ç•¥ä¿¡è™Ÿï¼ˆå› ç‚ºåƒæ•¸å¯èƒ½å·²ç¶“è¢«é¢¨éšªé–¥é–€èª¿æ•´ï¼‰
870:            # === åœ¨ ssma_turn ä¹Ÿå¥—ç”¨é¢¨éšªé–¥é–€ï¼ˆå’Œ Ensemble ä¸€è‡´çš„å¾Œç½®è¦†å¯«ï¼‰ ===
941:                        logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio_local:.4f}ï¼‰")
943:                        logger.warning(f"[{strat}] SSMA ç„¡æ³•å–å¾—æ¬Šé‡åºåˆ—ï¼Œè·³éé¢¨éšªé–¥é–€å¥—ç”¨")
945:                    logger.info(f"[{strat}] SSMA é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹çµæœ")
981:                # è¨»è§£æ‰åŸæœ¬çš„ç„¡æ¢ä»¶é¢¨éšªé–¥é–€èª¿æ•´ï¼ˆæœƒé€ æˆ floor æ–¹å‘éŒ¯èª¤ï¼‰
983:                #     logger.info(f"[{strat}] Ensemble ç­–ç•¥æ‡‰ç”¨é¢¨éšªé–¥é–€: åŸå§‹ delta_cap={ensemble_params.delta_cap}, floor={
ensemble_params.floor}")
985:                #     # èª¿æ•´ delta_capï¼ˆæœ€å¤§é¢¨éšªæš´éœ²ï¼‰
1020:                # --- æ–°å¢ï¼šåªåœ¨ ATR è§¸ç™¼æ™‚å•Ÿç”¨é¢¨éšªé–¥é–€ ---
1047:                                # å¢åŠ é¢¨éšªé–¥é–€è§¸ç™¼çš„è©³ç´°è³‡è¨Š
1049:                                    logger.info(f"[{strat}] ğŸ”´ é¢¨éšªé–¥é–€è§¸ç™¼ï¼ATRæ¯”å€¼({ratio:.4f}) > é–€æª»({atr_ratio})")
1051:                                    logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼ŒATRæ¯”å€¼({ratio:.4f}) <= é–€æª»({atr_ratio})")
1063:                # å¦‚æœå•Ÿç”¨å¼·åˆ¶è§¸ç™¼ï¼Œå‰‡å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€
1066:                    logger.info(f"[{strat}] ğŸ”´ å¼·åˆ¶è§¸ç™¼é¢¨éšªé–¥é–€å•Ÿç”¨")
1078:                    logger.info(f"[{strat}] ğŸ”´ é–‹å§‹å¥—ç”¨é¢¨éšªé–¥é–€: cap={risk_cap}, ratio={ratio:.4f}")
1093:                    logger.info(f"[{strat}] é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆcap={risk_cap}, ratio={ratio:.4f}ï¼‰")
1095:                    # å¢åŠ é¢¨éšªé–¥é–€æ•ˆæœçš„è©³ç´°è³‡è¨Š
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1108:                        logger.info(f"[{strat}] ğŸŸ¢ é¢¨éšªé–¥é–€æœªè§¸ç™¼ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
1116:                        logger.info(f"[{strat}] âšª å…¨å±€é¢¨éšªé–¥é–€æœªå•Ÿç”¨")
1175:                    "applied": False,  # å…¶ä»–ç­–ç•¥é¡å‹æš«æ™‚ä¸æ”¯æ´é¢¨éšªé–¥é–€
1205:        # === å…¨å±€é¢¨éšªé–¥é–€ï¼šé€æ—¥å‹•æ…‹å¥—ç”¨ï¼ˆèˆ‡å¢å¼·åˆ†æä¸€è‡´ï¼‰ ===
1211:                logger.warning(f"[{strat}] daily_state ä¸å« 'w'ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1217:                    logger.warning(f"[{strat}] ç„¡æ³•è¨ˆç®— ATR20/60ï¼Œè·³éå…¨å±€é¢¨éšªé–¥é–€")
1316:                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
1318:                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")
1701:            # === è¨ˆç®—é¢¨éšªé–¥é–€å¾½ç« å…§å®¹ ===
1713:                "ğŸ›¡ï¸ é¢¨éšªé–¥é–€ï¼š" + valve_badge_text + ((" | " + " | ".join(valve_badge_extra)) if valve_badge_extra else ""),
1968:                html.Summary("é¢¨éšªé–¥é–€å›æ¸¬"),
1981:                    html.Button("åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬", id="run-rv", n_clicks=0, style={"marginTop":"8px"})
1991:            # === æ–°å¢ï¼šé¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰å€å¡Š ===
1993:                html.Summary("ğŸ“Š é¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰"),
1995:                    html.Div("ç”Ÿæˆç­–ç•¥çš„é¢¨éšª-å ±é…¬åˆ†æåœ–è¡¨",
2010:                            html.Li("é»å¤§å°ï¼šé¢¨éšªè§¸ç™¼å¤©æ•¸ï¼ˆè¶Šå¤§ï¼ç®¡å¾—è¶Šå‹¤ï¼‰"),
2369:# --------- å¢å¼·åˆ†æ Callbackï¼šé¢¨éšªé–¥é–€å›æ¸¬ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---------
2434:        return f"é¢¨éšªé–¥é–€å›æ¸¬åŸ·è¡Œå¤±æ•—: {e}", no_update, no_update
2438:    # è¨ˆç®—é¢¨éšªè§¸ç™¼å¤©æ•¸
2446:        html.Code(f"é¢¨éšªè§¸ç™¼å¤©æ•¸ï¼š{trigger_days} å¤©")
2624:        kpi("é¢¨éšªæ•ˆç‡", f"{risk_eff:.3f}" if pd.notna(risk_eff) else "â€”"),
2652:            kpi("é¢¨éšªæ•ˆç‡",      _fmt(m['eff'],  dec=3) if m else "â€”"),
2940:                                "é¢¨éšªæ•ˆç‡": round(risk_eff, 3),
2971:        # æŒ‰é¢¨éšªæ•ˆç‡æ’åºï¼ˆé™åºï¼‰
2972:        results_df = results_df.sort_values("é¢¨éšªæ•ˆç‡", ascending=False)
2996:            html.Div(f"æœ€ä½³é¢¨éšªæ•ˆç‡ï¼š{results_df['é¢¨éšªæ•ˆç‡'].max():.3f}", style={"fontSize": "12px"}),
3009:            html.H6("å‰10åæœ€ä½³åƒæ•¸çµ„åˆï¼ˆæŒ‰é¢¨éšªæ•ˆç‡æ’åºï¼‰", style={"marginTop": "16px", "marginBottom": "8px"}),
3316:# --------- æ–°å¢ï¼šé¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰Callback ---------
3327:    """ç”Ÿæˆé¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰ï¼šæƒæ cap èˆ‡ ATR(20)/ATR(60) æ¯”å€¼å…¨çµ„åˆ"""
3488:        return go.Figure(), "âŒ ç„¡æ³•å¾é¢¨éšªé–¥é–€å›æ¸¬çš„åƒæ•¸çµ„åˆä¸­å–å¾—è³‡æ–™"
3521:            "é¢¨éšªè§¸ç™¼å¤©æ•¸: %{marker.size:.0f} å¤©<br>" +
3530:            'text': f'é¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰- {succeeded}/{tried} çµ„',
3544:    status_msg = f"âœ… æˆåŠŸç”Ÿæˆï¼šæƒæ capÃ—ATR æ¯”å€¼ {succeeded}/{tried} çµ„ã€‚é¡è‰²=å³å°¾èª¿æ•´å¹…åº¦ï¼ˆç´…=å‰Šæ¸›ï¼Œè—=æ”¾å¤§ï¼‰ï¼Œå¤§å°=é¢¨éšªè§¸
ç™¼å¤©æ•¸ã€‚è³‡æ–™ä¾†æºï¼š{data_source}"
3587:                # è¨ˆç®—å³å°¾é¢¨éšªï¼šä½¿ç”¨ååº¦å’Œå³°åº¦ä¾†è¡¡é‡
3615:                # 4. é¢¨éšªè§¸ç™¼å¤©æ•¸ï¼ˆé»å¤§å°ï¼Œè¶Šå¤§ï¼ç®¡å¾—è¶Šå‹¤ï¼‰
3633:                # ä¼°ç®—é¢¨éšªè§¸ç™¼å¤©æ•¸
3656:    """å‰µå»ºé¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰"""
3668:            size=df['risk_trigger_days'] / 10,  # é»å¤§å°ï¼šé¢¨éšªè§¸ç™¼å¤©æ•¸
3690:            "é¢¨éšªè§¸ç™¼å¤©æ•¸: %{marker.size:.0f}<br>" +
3727:            'text': 'é¢¨éšª-å ±é…¬åœ°åœ–ï¼ˆPareto Mapï¼‰',
The bug suggests only average holding days remain, possibly due to improper aggregation. I'll search for relevant code sections.

root@a3bd66c837b3:/workspace/SSS096# rg "å¹³å‡æŒå€‰å¤©æ•¸" -n
SSSv096.py
927:    è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸.
933:        float: å¹³å‡æŒå€‰å¤©æ•¸.
1828:            "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1864:    # è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸
1867:    # å°‡å¹³å‡æŒå€‰å¤©æ•¸åŠ å…¥ metrics
2293:            # é¡¯ç¤ºç­–ç•¥æ‘˜è¦,å‚³å…¥ trade_df ä»¥è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸

analysis/OSv3.py
82:        'avg_hold_days': 'å¹³å‡æŒå€‰å¤©æ•¸ï¼šå¹³å‡æ¯æ¬¡äº¤æ˜“çš„æŒå€‰æ™‚é–“',

analysis/optuna_16.py
10:v10-12  æ”¯æ´å–®ä¸€/éš¨æ©Ÿ/ä¾åºæ•¸æ“šæºæ¨¡å¼,æ–°å¢å¹³å‡æŒå€‰å¤©æ•¸,ç›¸é—œä¿‚æ•¸åˆ†æ,ä¸¦è‡ªå‹•è¼¸å‡ºç†±åœ–,è©¦é©—çµæœèˆ‡æœ€ä½³åƒæ•¸è‡ªå‹•è¼¸å‡º CSV/JSON

app_dash.py
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
It seems the risk valve might be overriding the metrics dictionary with only avg_holding_period.

root@a3bd66c837b3:/workspace/SSS096# sed -n '1270,1320p' SSS_EnsembleTab.py
            (high - low),
            (high - prev_close).abs(),
            (low  - prev_close).abs()
        ], axis=1).max(axis=1)
    else:
        tr = close.diff().abs()

    b["atr"] = tr.rolling(atr_win, min_periods=atr_win).mean()
    b["atr_ratio"] = b["atr"] / b["atr"].rolling(atr_ref_win, min_periods=atr_ref_win).mean()

    # é¢¨éšªè§¸ç™¼ï¼šå…©æ–œç‡çš† < é–€æª»ï¼Œä¸” ATR æŠ¬é ­è¶…éå€ç‡
    b["risk_trigger"] = (b["slope_20d"] < slope20_thresh) & \
                        (b["slope_60d"] < slope60_thresh) & \
                        (b["atr_ratio"] > atr_ratio_mult)
    return b[["slope_20d", "slope_60d", "atr", "atr_ratio", "risk_trigger"]]


# === PATCH: é¢¨éšªé–¥é–€æ–½ä½œåœ¨æ¬Šé‡ ===
def apply_valve_to_weights(w: pd.Series,
                           risk_trigger: pd.Series,
                           mode: str = "cap",   # "cap" æˆ– "ban_add"
                           cap_level: float = 0.5) -> pd.Series:
    """å°æ¯æ—¥ç›®æ¨™æ¬Šé‡ w æ–½ä½œé¢¨éšªé–¥é–€ï¼š
       - cap: é¢¨éšªæ—¥å°‡ w é™åœ¨ cap_level ä»¥ä¸‹
       - ban_add: é¢¨éšªæ—¥ç¦æ­¢ã€Œæé«˜ã€å€‰ä½ï¼ˆå…è¨±æ¸›å€‰ï¼‰ï¼Œç”¨è¿­ä»£æ³•ç¢ºä¿å–®èª¿"""
    w = w.copy().reindex(risk_trigger.index).ffill().fillna(0.0)
    out = w.copy()

    if mode == "cap":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        out[mask] = np.minimum(out[mask], cap_level)

    elif mode == "ban_add":
        mask = risk_trigger.reindex(w.index).fillna(False).astype(bool)
        for i in range(1, len(out)):
            if mask.iloc[i] and (out.iloc[i] > out.iloc[i-1]):
                out.iloc[i] = out.iloc[i-1]  # ç¦æ­¢å¢åŠ 
    else:
        raise ValueError("mode éœ€ç‚º 'cap' æˆ– 'ban_add'")
    return out.clip(0.0, 1.0)


# === PATCH: ç”± trade_ledger å»ºç«‹ã€Œæ¯ç­†ç›ˆè™§%ã€èˆ‡å¿«ç…§ï¼ˆå¼•ç”¨ re.txt åšæ³•ï¼‰===
def build_trade_snapshots_from_ledger(trade_ledger: pd.DataFrame) -> pd.DataFrame:
    """ä»¥äº¤æ˜“å¾Œæ¬Šç›Š/ç¾é‡‘ç‚ºè³‡ç”¢å¿«ç…§ï¼Œè¨ˆç®—æ¯ç­†ç›ˆè™§%ï¼ˆåªåœ¨æœ‰è®Šå‹•çš„äº¤æ˜“æ—¥è¨˜ä¸€ç­†ï¼‰"""
    df = trade_ledger.copy()
    # æ¬„åå°é½Š
    if "date" in df.columns: df.rename(columns={"date": "äº¤æ˜“æ—¥æœŸ"}, inplace=True)
    if "type" in df.columns: df.rename(columns={"type": "äº¤æ˜“é¡å‹"}, inplace=True)
    if "equity_after" in df.columns: df.rename(columns={"equity_after": "äº¤æ˜“å¾Œæ¬Šç›Š"}, inplace=True)
    if "cash_after" in df.columns: df.rename(columns={"cash_after": "äº¤æ˜“å¾Œç¾é‡‘"}, inplace=True)
I'll scroll more to find where the risk valve is applied to the metrics.

root@a3bd66c837b3:/workspace/SSS096# rg "risk_valve" -n SSS_EnsembleTab.py
1241:def compute_risk_valve_signals(benchmark_df: pd.DataFrame,
1359:def risk_valve_backtest(open_px: pd.Series,
1371:    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
I'm opening the risk_valve_backtest function around line 1359 to review it.

root@a3bd66c837b3:/workspace/SSS096# sed -n '1359,1405p' SSS_EnsembleTab.py
def risk_valve_backtest(open_px: pd.Series,
                        w: pd.Series,
                        cost,
                        benchmark_df: pd.DataFrame,
                        mode: str = "cap",
                        cap_level: float = 0.5,
                        slope20_thresh: float = 0.0,
                        slope60_thresh: float = 0.0,
                        atr_win: int = 20,
                        atr_ref_win: int = 60,
                        atr_ratio_mult: float = 1.3) -> dict:
    """å›å‚³ï¼šåŸå§‹èˆ‡é–¥é–€ç‰ˆæœ¬çš„ç¸¾æ•ˆã€å³å°¾å‰Šæ¸›åº¦ã€ä»¥åŠå…©ç‰ˆ equity/ledger"""
    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,
                                     atr_win, atr_ref_win, atr_ratio_mult)
    w2 = apply_valve_to_weights(w, sig["risk_trigger"], mode, cap_level)

    # åŸå§‹ç‰ˆæœ¬
    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
    # é–¥é–€ç‰ˆæœ¬
    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)

    # äº¤æ˜“çµ±è¨ˆï¼ˆåªç”¨è³£å‡ºåˆ—ï¼›ç¬¦åˆ re.txt å°å¯¦ç¾æç›Šçš„å£å¾‘ï¼‰
    r1 = _sell_returns_pct_from_ledger(ledger1)
    r2 = _sell_returns_pct_from_ledger(ledger2)
    pf1 = (r1[r1>0].sum() / abs(r1[r1<0].sum())) if (r1[r1<0].sum()!=0) else np.inf
    pf2 = (r2[r2>0].sum() / abs(r2[r2<0].sum())) if (r2[r2<0].sum()!=0) else np.inf
    mdd1 = _mdd_from_daily_equity(daily_state1["equity"])
    mdd2 = _mdd_from_daily_equity(daily_state2["equity"])

    # å³å°¾å‰Šæ¸›ï¼šæ¯”è¼ƒå…©ç‰ˆçš„ã€Œ>P90 çš„æ­£å ±é…¬ç¸½å’Œã€
    def right_tail_sum(x: pd.Series):
        if x.empty: return 0.0
        thr = x.quantile(0.90)
        return float(x[x > max(thr, 0)].sum())
    rt1 = right_tail_sum(r1)
    rt2 = right_tail_sum(r2)
    right_tail_reduction = (rt1 - rt2)

    return {
        "signals": sig,
        "weights_orig": w,
        "weights_valve": w2,
        "daily_state_orig": daily_state1,
        "daily_state_valve": daily_state2,
        "trade_ledger_orig": ledger1,
        "trade_ledger_valve": ledger2,
        "metrics": {
root@a3bd66c837b3:/workspace/SSS096# sed -n '1405,1435p' SSS_EnsembleTab.py
        "metrics": {
            "pf_orig": float(pf1), "pf_valve": float(pf2),
            "mdd_orig": float(mdd1), "mdd_valve": float(mdd2),
            "right_tail_sum_orig": float(rt1), "right_tail_sum_valve": float(rt2),
            "right_tail_reduction": float(right_tail_reduction)
        }
    }


# === PATCH: äº¤æ˜“è²¢ç»æ‹†è§£ï¼ˆæŒ‰åŠ ç¢¼/æ¸›ç¢¼éšæ®µï¼‰===
def trade_contribution_by_phase(trade_ledger: pd.DataFrame,
                                daily_equity: pd.Series,
                                min_spacing_days: int = 0,
                                cooldown_days: int = 0) -> pd.DataFrame:
    """å°‡äº¤æ˜“ä¾ã€æ¬Šé‡è®ŠåŒ–æ­£/è² ã€åˆ†æ®µï¼Œçµ±è¨ˆå„éšæ®µçš„è³£å‡ºå¯¦ç¾å ±é…¬èˆ‡è©²æ®µå…§çš„ MDDã€‚"""
    tl = trade_ledger.copy()
    # æ¬„å°é½Š
    for a,b in [("date","äº¤æ˜“æ—¥æœŸ"),("type","äº¤æ˜“é¡å‹"),("w_before","äº¤æ˜“å‰æ¬Šé‡"),("w_after","äº¤æ˜“å¾Œæ¬Šé‡")]:
        if a in tl.columns: tl.rename(columns={a:b}, inplace=True)
    tl["äº¤æ˜“æ—¥æœŸ"] = pd.to_datetime(tl["äº¤æ˜“æ—¥æœŸ"], errors="coerce")
    tl = tl.dropna(subset=["äº¤æ˜“æ—¥æœŸ"]).sort_values("äº¤æ˜“æ—¥æœŸ").reset_index(drop=True)
    tl["dw"] = pd.to_numeric(tl["äº¤æ˜“å¾Œæ¬Šé‡"], errors="coerce") - pd.to_numeric(tl["äº¤æ˜“å‰æ¬Šé‡"], errors="coerce")

    # å¯é¸ï¼šæ–½åŠ æœ€å°é–“è·/å†·å»ï¼ˆåœ¨åˆ†æ®µå‰å°éå¯†è²·å–®é™å™ªï¼‰
    if min_spacing_days > 0:
        last_buy_dt = None
        for i, row in tl.iterrows():
            if row["dw"] > 0:
                if last_buy_dt is not None and (row["äº¤æ˜“æ—¥æœŸ"] - last_buy_dt).days < min_spacing_days:
                    tl.at[i, "dw"] = 0.0  # å¿½ç•¥é€™ç­†å¾®å°åŠ ç¢¼
                else:
I think the risk valve might overwrite metrics. I'll search for related terms in the repo.

root@a3bd66c837b3:/workspace/SSS096# rg "risk_valve" -n
SSS_EnsembleTab.py
1241:def compute_risk_valve_signals(benchmark_df: pd.DataFrame,
1359:def risk_valve_backtest(open_px: pd.Series,
1371:    sig = compute_risk_valve_signals(benchmark_df, slope20_thresh, slope60_thresh,

DEFENSIVE_CHECKS_UPDATE_SUMMARY.md
25:**1.1 _simulate_risk_valves å‡½å¼**
30:**1.2 _calculate_risk_valve_impact å‡½å¼**
32:- æª¢æŸ¥ `benchmark_enhanced` æ˜¯å¦æœ‰ `risk_valve_triggered` æ¬„ä½

analysis/enhanced_trade_analysis.py
95:    def risk_valve_backtest(self, risk_rules=None):
113:        self._simulate_risk_valves(risk_rules)
116:        self._calculate_risk_valve_impact()
118:        return self.analysis_results.get('risk_valve', {})
120:    def _simulate_risk_valves(self, risk_rules):
180:        benchmark['risk_valve_20d'] = benchmark['slope_20d'] < t20
181:        benchmark['risk_valve_60d'] = benchmark['slope_60d'] < t60
182:        benchmark['risk_valve_atr'] = benchmark['atr_ratio'] > atr_mult
185:        benchmark['risk_valve_triggered'] = (
186:            benchmark['risk_valve_20d'] &
187:            benchmark['risk_valve_60d'] &
188:            benchmark['risk_valve_atr']
193:    def _calculate_risk_valve_impact(self):
203:        if 'risk_valve_triggered' not in self.benchmark_enhanced.columns:
204:            print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œç„¡æ³•è¨ˆç®—é¢¨éšªé–¥é–€å½±éŸ¿")
209:            self.benchmark_enhanced['risk_valve_triggered']
229:        self.analysis_results['risk_valve'] = {
530:        self.risk_valve_backtest()
556:        if 'risk_valve' in self.analysis_results and hasattr(self, 'benchmark_enhanced'):
560:            elif 'risk_valve_triggered' not in self.benchmark_enhanced.columns:
561:                print("è­¦å‘Šï¼šbenchmark_enhanced ç¼ºå°‘ 'risk_valve_triggered' æ¬„ä½ï¼Œè·³éé¢¨éšªé–¥é–€æ™‚åºåœ–")
567:                    self.benchmark_enhanced['risk_valve_triggered']
573:                                   self.benchmark_enhanced['risk_valve_triggered'], 'æ”¶ç›¤åƒ¹'
637:    print("2. åŸ·è¡Œé¢¨éšªé–¥é–€å›æ¸¬ï¼šanalyzer.risk_valve_backtest()")

analysis/ENHANCED_ANALYSIS_README.md
17:### 1. é¢¨éšªé–¥é–€å›æ¸¬ (`risk_valve_backtest`)
34:risk_results = analyzer.risk_valve_backtest()
41:custom_results = analyzer.risk_valve_backtest(custom_rules)
108:risk_results = analyzer.risk_valve_backtest()

app_dash.py
586:def update_risk_valve_status(global_apply, risk_cap, atr_ratio, force_trigger, ticker, start_date, end_date):
893:                    from SSS_EnsembleTab import risk_valve_backtest, CostParams
918:                        rv = risk_valve_backtest(
1075:                    from SSS_EnsembleTab import risk_valve_backtest
1080:                    rv = risk_valve_backtest(
1238:                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
1240:                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
1264:                        result_cap = risk_valve_backtest(
1273:                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
1307:                        result['_risk_valve_applied'] = True
1308:                        result['_risk_valve_params'] = {
2426:        from SSS_EnsembleTab import risk_valve_backtest
2427:        out = risk_valve_backtest(
3424:    # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3426:        from SSS_EnsembleTab import risk_valve_backtest
3427:        logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3429:        logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3430:        return go.Figure(), f"âŒ ç„¡æ³•åŒ¯å…¥ risk_valve_backtest: {e}"
3440:                out = risk_valve_backtest(
3837:        # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŒ¯å…¥ risk_valve_backtest
3839:            from SSS_EnsembleTab import risk_valve_backtest
3840:            logger.info("æˆåŠŸåŒ¯å…¥ risk_valve_backtest")
3842:            logger.error(f"åŒ¯å…¥ risk_valve_backtest å¤±æ•—: {e}")
3853:                    out = risk_valve_backtest(
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
I'm looking into why other metrics disappear when applying the risk valveâ€”let's search for "avg_holding_period".

root@a3bd66c837b3:/workspace/SSS096# sed -n '1230,1315p' app_dash.py

                    # 4) å›å¯« dsï¼Œä¸¦é‡ç®—äº¤æ˜“/æ¬Šç›Š
                    result["daily_state_std"] = pack_df(ds)

                    # open åƒ¹ï¼ˆæ²’æœ‰ open å°±é€€è€Œæ±‚å…¶æ¬¡ç”¨æ”¶ç›¤åƒ¹ï¼‰
                    open_px = (df_raw["open"] if "open" in df_raw.columns else df_raw.get("æ”¶ç›¤åƒ¹")).astype(float)
                    open_px = open_px.reindex(ds.index).dropna()

                    # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
                    try:
                        from SSS_EnsembleTab import risk_valve_backtest, CostParams

                        # æˆæœ¬åƒæ•¸
                        trade_cost = (strat_params.get("trade_cost", {})
                                      if isinstance(strat_params, dict) else {})
                        cost = CostParams(
                            buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
                            sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
                            sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0)),
                        )

                        # åŸºæº–ï¼ˆæœ‰é«˜ä½åƒ¹å°±å¸¶ä¸Šï¼‰
                        bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
                        if 'close' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
                        elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
                        if 'high' in df_raw.columns and 'low' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
                        elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")

                        result_cap = risk_valve_backtest(
                            open_px=open_px,
                            w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
root@a3bd66c837b3:/workspace/SSS096# sed -n '1315,1345p' app_dash.py
                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)

    # çµ±ä¸€ä½¿ç”¨ orient="split" åºåˆ—åŒ–ï¼Œç¢ºä¿ä¸€è‡´æ€§
    payload = {
        'results': results,
        'df_raw': df_raw_main.to_json(date_format='iso', orient='split'),
        'ticker': ticker
    }

    # é˜²å®ˆæ€§æª¢æŸ¥ï¼šå¦‚é‚„æœ‰æ¼ç¶²çš„éåºåˆ—åŒ–ç‰©ä»¶å°±èƒ½ææ—©çœ‹å‡º
    try:
        json.dumps(payload)
    except Exception as e:
        logger.exception("[BUG] backtest-store payload ä»å«ä¸å¯åºåˆ—åŒ–ç‰©ä»¶ï¼š%s", e)
        # å¦‚æœè¦å¼·åˆ¶ä¸å™´ï¼Œå¯åš fallbackï¼šjson.dumps(..., default=str) ä½†é€šå¸¸ä¸å»ºè­°åæ‰

    # === å›æ¸¬å®Œæˆæ—¥èªŒ ===
    logger.info(f"å›æ¸¬å®Œæˆ - ç­–ç•¥æ•¸: {len(results)}, ticker: {ticker}, æ•¸æ“šè¡Œæ•¸: {len(df_raw_main)}")
    logger.debug(f"ç­–ç•¥åˆ—è¡¨: {list(results.keys())}")

    return payload
After applying the risk valve, metrics were popped and not recalculated, causing them to vanish.

root@a3bd66c837b3:/workspace/SSS096# rg -n "avg_holding_period" app_dash.py
1545:            metrics['avg_holding_period'] = avg_holding
1559:                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
1573:                elif k in ["max_drawdown_duration", "avg_holding_period"]:
root@a3bd66c837b3:/workspace/SSS096# sed -n '1525,1575p' app_dash.py
            # === äº¤æ˜“æµæ°´å¸³(ledger)è¡¨æ ¼ï¼šå…ˆæº–å‚™é¡¯ç¤ºç‰ˆ ===
            ledger_src = trade_ledger_std if (trade_ledger_std is not None and not trade_ledger_std.empty) else \
                         (ledger_ui if (ledger_ui is not None and not ledger_ui.empty) else pd.DataFrame())

            if ledger_src is not None and not ledger_src.empty:
                # ç‚ºäº† 100% ä¿è­‰ weight_change å‡ºç¾ï¼Œå…ˆç¢ºä¿æ¬Šé‡æ¬„ä½
                ledger_src = _ensure_weight_columns(ledger_src)
                # ä½¿ç”¨æ–°çš„çµ±ä¸€æ ¼å¼åŒ–å‡½å¼
                ledger_display = format_trade_like_df_for_display(ledger_src)
                ledger_columns = [{"name": i, "id": i} for i in ledger_display.columns]
                ledger_data = ledger_display.to_dict('records')
            else:
                ledger_columns = []
                ledger_data = []

            metrics = result.get('metrics', {})
            tooltip = f"{strategy} ç­–ç•¥èª¬æ˜"
            param_display = {k: v for k, v in param_presets[strategy].items() if k != "strategy_type"}
            param_str = ", ".join(f"{k}: {v}" for k, v in param_display.items())
            avg_holding = calculate_holding_periods(trade_df)
            metrics['avg_holding_period'] = avg_holding

            label_map = {
                "total_return": "ç¸½å›å ±ç‡",
                "annual_return": "å¹´åŒ–å›å ±ç‡",
                "win_rate": "å‹ç‡",
                "max_drawdown": "æœ€å¤§å›æ’¤",
                "max_drawdown_duration": "å›æ’¤æŒçºŒ",
                "calmar_ratio": "å¡ç‘ªæ¯”ç‡",
                "sharpe_ratio": "å¤æ™®æ¯”ç‡",
                "sortino_ratio": "ç´¢æè«¾æ¯”ç‡",
                "payoff_ratio": "ç›ˆè™§æ¯”",
                "profit_factor": "ç›ˆè™§å› å­",
                "num_trades": "äº¤æ˜“æ¬¡æ•¸",
                "avg_holding_period": "å¹³å‡æŒå€‰å¤©æ•¸",
                "annualized_volatility": "å¹´åŒ–æ³¢å‹•ç‡",
                "max_consecutive_wins": "æœ€å¤§é€£çºŒç›ˆåˆ©",
                "max_consecutive_losses": "æœ€å¤§é€£çºŒè™§æ",
                "avg_win": "å¹³å‡ç›ˆåˆ©",
                "avg_loss": "å¹³å‡è™§æ",
            }

            metric_cards = []
            for k, v in metrics.items():
                if k in ["total_return", "annual_return", "win_rate", "max_drawdown", "annualized_volatility", "avg_win", "avg_l
oss"]:
                    txt = f"{v:.2%}" if pd.notna(v) else ""
                elif k in ["calmar_ratio", "sharpe_ratio", "sortino_ratio", "payoff_ratio", "profit_factor"]:
                    txt = f"{v:.2f}" if pd.notna(v) else ""
                elif k in ["max_drawdown_duration", "avg_holding_period"]:
                    txt = f"{v:.2f} å¤©" if pd.notna(v) else ""
                elif k in ["num_trades", "max_consecutive_wins", "max_consecutive_losses"]:
It seems metrics are cleared by the risk valve. I'll search for calculate_metrics.

root@a3bd66c837b3:/workspace/SSS096# rg "calculate_metrics" -n
SSSv096.py
40:    "compute_backtest_for_periods", "calculate_metrics",
824:def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series
= None) -> Dict:
1295:    metrics = calculate_metrics(trades, df_ind, equity_curve)

analysis/ROEAv4.py
25:from SSSv095a1 import calculate_metrics, load_data_wrapper
407:    all_metric_cols = list(calculate_metrics([], pd.DataFrame()).keys())
Iâ€™ll check the imports in app_dash to see if calculate_metrics is already imported.

root@a3bd66c837b3:/workspace/SSS096# rg -n "calculate_metrics" app_dash.py
I'll check if app_dash.py has a metrics function. If not, I'll use SSSv096.

root@a3bd66c837b3:/workspace/SSS096# sed -n '800,860p' SSSv096.py
        logger.warning(f"æœ€çµ‚ df_ind ç‚ºç©º,å¯èƒ½æ˜¯ SMAA æ•¸æ“šä¸è¶³,ç­–ç•¥: ssma_turn, linlen={linlen}, smaalen={smaalen}, valid_smaa={
len(smaa.dropna())}")
    return df_ind.dropna(), buy_dates, sell_dates

def calculate_trade_mmds(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], equity_curve: pd.Series) -> List[float]:
    """
    è¨ˆç®—æ¯ç­†æŒæœ‰æœŸé–“çš„æœ€å¤§å›æ’¤ï¼ˆMMDï¼‰ã€‚
    Args:
        trades: äº¤æ˜“è¨˜éŒ„,åŒ…å« entry_date, return, exit_dateã€‚
        equity_curve: å…¨éƒ¨å›æ¸¬æœŸé–“çš„æ¯æ—¥è³‡ç”¢æ·¨å€¼ã€‚
    Returns:
        List[float]: æ¯ç­†æŒæœ‰æœŸé–“çš„æœ€å¤§å›æ’¤ã€‚
    """
    mmds = []
    for entry_date, _, exit_date in trades:
        # å–å‡ºæŒæœ‰æœŸé–“çš„ equity
        period_equity = equity_curve.loc[entry_date:exit_date]
        if len(period_equity) < 2:
            mmds.append(0.0)
            continue
        roll_max = period_equity.cummax()
        drawdown = period_equity / roll_max - 1
        mmds.append(drawdown.min())
    return mmds

def calculate_metrics(trades: List[Tuple[pd.Timestamp, float, pd.Timestamp]], df_ind: pd.DataFrame, equity_curve: pd.Series = No
ne) -> Dict:
    """
    è¨ˆç®—å›æ¸¬ç¸¾æ•ˆæŒ‡æ¨™.
    Args:
        trades: äº¤æ˜“è¨˜éŒ„,åŒ…å«æ—¥æœŸå’Œå ±é…¬ç‡.
        df_ind: æŒ‡æ¨™æ•¸æ“š DataFrame,åŒ…å«äº¤æ˜“æ—¥ç´¢å¼•.
        equity_curve: å…¨éƒ¨å›æ¸¬æœŸé–“çš„æ¯æ—¥è³‡ç”¢æ·¨å€¼ï¼ˆå¯é¸, è‹¥è¦è¨ˆç®—æŒæœ‰æœŸé–“MMDéœ€æä¾›ï¼‰
    Returns:
        Dict: åŒ…å«ç¸½å›å ±ç‡ã€å¹´åŒ–å›å ±ç‡ã€æœ€å¤§å›æ’¤ç­‰æŒ‡æ¨™.
    """
    metrics = {
        'total_return': 0.0,
        'annual_return': 0.0,
        'max_drawdown': 0.0,  # é€™è£¡ä¹‹å¾Œæœƒç›´æ¥ç”¨ max_mmd è¦†è“‹
        'max_drawdown_duration': 0,
        'calmar_ratio': np.nan,
        'num_trades': 0,
        'win_rate': 0.0,
        'avg_win': np.nan,
        'avg_loss': np.nan,
        'payoff_ratio': np.nan,
        'sharpe_ratio': np.nan,
        'sortino_ratio': np.nan,
        'max_consecutive_wins': 0,# æ–°å¢
        'max_consecutive_losses': 0,# æ–°å¢
        'avg_holding_period': np.nan,# æ–°å¢
        'annualized_volatility': np.nan,# æ–°å¢
        'profit_factor': np.nan,# æ–°å¢
        # 'avg_mmd': np.nan, # ä¸å†éœ€è¦
        # 'max_mmd': np.nan, # ä¸å†éœ€è¦å–®ç¨æ¬„ä½
    }
    if not trades:
        return metrics

    trade_metrics = pd.DataFrame(trades, columns=['entry_date', 'return', 'exit_date']).set_index('exit_date')
    trade_metrics['equity'] = (1 + trade_metrics['return']).cumprod()
    roll_max = trade_metrics['equity'].cummax()
Iâ€™ll double-check how trades and the df_ind (indicator DataFrame) are structured for metrics.

root@a3bd66c837b3:/workspace/SSS096# rg -n "metrics'" app_dash.py
1097:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: PFåŸå§‹={rv['metrics'].get('pf_orig', 'N/A'):.2f}, PFé–¥é–€={rv[
'metrics'].get('pf_valve', 'N/A'):.2f}")
1098:                        logger.info(f"[{strat}] é¢¨éšªé–¥é–€æ•ˆæœ: MDDåŸå§‹={rv['metrics'].get('mdd_orig', 'N/A'):.2f}%, MDDé–¥é–€=
{rv['metrics'].get('mdd_valve', 'N/A'):.2f}%")
1130:                    'metrics': backtest_result.stats,
1151:                    'metrics': {'total_return': 0.0, 'annual_return': 0.0, 'max_drawdown': 0.0, 'sharpe_ratio': 0.0, 'calma
r_ratio': 0.0, 'num_trades': 0},
1302:                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
1540:            metrics = result.get('metrics', {})
1878:            metrics = result['metrics']
I see that metrics is assigned at line 1130 as 'metrics': backtest_result.stats.

root@a3bd66c837b3:/workspace/SSS096# sed -n '1120,1140p' app_dash.py
                            "cap": "N/A",
                            "atr_ratio": "N/A"
                        }

                # è½‰æ›ç‚ºèˆŠæ ¼å¼ä»¥ä¿æŒç›¸å®¹æ€§
                result = {
                    'trades': [],
                    'trade_df': pack_df(backtest_result.trades),
                    'trades_df': pack_df(backtest_result.trades),
                    'signals_df': pack_df(backtest_result.trades[['trade_date', 'type', 'price']].rename(columns={'type': 'actio
n'}) if not backtest_result.trades.empty else pd.DataFrame(columns=['trade_date', 'action', 'price'])),
                    'metrics': backtest_result.stats,
                    'equity_curve': pack_series(backtest_result.equity_curve),
                    'cash_curve': pack_series(backtest_result.cash_curve) if backtest_result.cash_curve is not None else "",
                    'weight_curve': pack_series(backtest_result.weight_curve) if backtest_result.weight_curve is not None else p
ack_series(pd.Series(0.0, index=backtest_result.equity_curve.index)),
                    'price_series': pack_series(backtest_result.price_series) if backtest_result.price_series is not None else p
ack_series(pd.Series(1.0, index=backtest_result.equity_curve.index)),
                    'daily_state': pack_df(backtest_result.daily_state),
                    'trade_ledger': pack_df(backtest_result.ledger),
                    'daily_state_std': pack_df(backtest_result.daily_state),
                    'trade_ledger_std': pack_df(backtest_result.ledger)
                }
The backtest metrics weren't recalculated after replacing components; I'll recompute using calculate_metrics.

root@a3bd66c837b3:/workspace/SSS096# sed -n '860,940p' SSSv096.py
    roll_max = trade_metrics['equity'].cummax()
    daily_drawdown = trade_metrics['equity'] / roll_max - 1

    # åŸºæœ¬æŒ‡æ¨™
    metrics['total_return'] = trade_metrics['equity'].iloc[-1] - 1
    years = max((trade_metrics.index[-1] - trade_metrics.index[0]).days / 365.25, 1)
    metrics['annual_return'] = (1 + metrics['total_return']) ** (1 / years) - 1

    # æ—¢æœ‰æ—¥å…§å›æ’¤ï¼ˆnumpy é™£åˆ—ï¼‰
    dd_np = (daily_drawdown < 0).astype(bool)

    # è½‰æˆ pandas Seriesï¼ˆç´¢å¼•ç”¨å› trade_metrics.indexï¼‰ï¼Œæ‰èƒ½å®‰å…¨ groupby
    dd_series = pd.Series(dd_np, index=trade_metrics.index)

    # é€£çºŒå›æ’¤å€æ®µçš„æœ€é•·å¤©æ•¸ï¼ˆTrue é€£çºŒæ®µçš„é•·åº¦æœ€å¤§å€¼ï¼‰
    blocks = (~dd_series).cumsum()
    dd_dur = int((dd_series.groupby(blocks).cumcount() + 1).where(dd_series).max() or 0)
    metrics['max_drawdown_duration'] = dd_dur

    # å›å¡« max_drawdownï¼ˆå®‰å…¨åœ°ç”¨ float è½‰æ›ï¼‰
    metrics['max_drawdown'] = float(daily_drawdown.min())
    # metrics['calmar_ratio'] = metrics['annual_return'] / abs(metrics['max_drawdown']) if metrics['max_drawdown'] < 0 else np.n
an
    metrics['num_trades'] = len(trade_metrics)
    metrics['win_rate'] = (trade_metrics['return'] > 0).sum() / metrics['num_trades'] if metrics['num_trades'] > 0 else 0
    metrics['avg_win'] = trade_metrics[trade_metrics['return'] > 0]['return'].mean() if metrics['win_rate'] > 0 else np.nan
    metrics['avg_loss'] = trade_metrics[trade_metrics['return'] < 0]['return'].mean() if metrics['win_rate'] < 1 else np.nan
    metrics['payoff_ratio'] = abs(metrics['avg_win'] / metrics['avg_loss']) if metrics['avg_loss'] != 0 and not np.isnan(metrics
['avg_win']) else np.nan

    # æ–°å¢:è¨ˆç®—æ¯æ—¥å ±é…¬ç‡èˆ‡æ³¢å‹•ç‡
    daily_dates = df_ind.index.intersection(pd.date_range(start=trade_metrics.index.min(), end=trade_metrics.index.max(), freq='
B'))
    daily_equity = pd.Series(index=daily_dates, dtype=float)
    for date, row in trade_metrics.iterrows():
        daily_equity.loc[date] = row['equity']
    daily_equity = daily_equity.ffill()
    daily_returns = daily_equity.pct_change().dropna()

    metrics['sharpe_ratio'] = (daily_returns.mean() * np.sqrt(252)) / daily_returns.std() if daily_returns.std() != 0 else np.na
n
    downside_returns = daily_returns[daily_returns < 0]
    downside_std = downside_returns.std() if len(downside_returns) > 0 else np.nan
    metrics['sortino_ratio'] = (daily_returns.mean() * np.sqrt(252)) / downside_std if downside_std != 0 else np.nan

    # æ–°å¢:è¨ˆç®—æœ€å¤§é€£çºŒç›ˆåˆ©å’Œæœ€å¤§é€£çºŒè™§æ
    trade_metrics['win_flag'] = trade_metrics['return'] > 0
    trade_metrics['grp'] = (trade_metrics['win_flag'] != trade_metrics['win_flag'].shift(1)).cumsum()
    consec = trade_metrics.groupby(['grp', 'win_flag']).size()
    metrics['max_consecutive_wins'] = consec[consec.index.get_level_values('win_flag') == True].max() if True in consec.index.ge
t_level_values('win_flag') else 0
    metrics['max_consecutive_losses'] = consec[consec.index.get_level_values('win_flag') == False].max() if False in consec.inde
x.get_level_values('win_flag') else 0

    # æ–°å¢:è¨ˆç®—å¹´åŒ–æ³¢å‹•ç‡
    metrics['annualized_volatility'] = daily_returns.std() * np.sqrt(252) if len(daily_returns) > 0 else np.nan

    # æ–°å¢:è¨ˆç®—ç›ˆè™§å› å­ (Profit Factor)
    total_profits = trade_metrics[trade_metrics['return'] > 0]['return'].sum()
    total_losses = abs(trade_metrics[trade_metrics['return'] < 0]['return'].sum())
    metrics['profit_factor'] = total_profits / total_losses if total_losses != 0 else np.nan

    # æ–°å¢:è¨ˆç®—æ¯ç­†æŒæœ‰æœŸé–“MMDï¼Œä¸¦ç›´æ¥ç”¨ max_mmd è¦†è“‹ max_drawdown
    if equity_curve is not None:
        mmds = calculate_trade_mmds(trades, equity_curve)
        if mmds:
            metrics['max_drawdown'] = float(np.min(mmds)) # drawdownç‚ºè² å€¼,æœ€å°å€¼ç‚ºæœ€å¤§è·Œå¹…
            metrics['calmar_ratio'] = metrics['annual_return'] / abs(metrics['max_drawdown']) if metrics['max_drawdown'] < 0 els
e np.nan

    return metrics

def calculate_holding_periods(trade_df: pd.DataFrame) -> float:
    """
    è¨ˆç®—å¹³å‡æŒå€‰å¤©æ•¸.

    Args:
        trade_df: äº¤æ˜“è¨˜éŒ„ DataFrame,åŒ…å« signal_date, trade_date, type.

    Returns:
        float: å¹³å‡æŒå€‰å¤©æ•¸.
    """
    if trade_df.empty or 'trade_date' not in trade_df.columns or 'type' not in trade_df.columns:
        return np.nan

    holding_periods = []
    entry_date = None
root@a3bd66c837b3:/workspace/SSS096# rg -n "trade_df" app_dash.py
247:        'trade_df', 'trades_df', 'signals_df',
1127:                    'trade_df': pack_df(backtest_result.trades),
1148:                    'trade_df': pd.DataFrame(),
1159:            if 'trade_df' in result and hasattr(result['trade_df'], 'shape'):
1160:                logger.info(f"[Ensemble] äº¤æ˜“è¨˜éŒ„æ•¸é‡: {len(result['trade_df'])}")
1188:            if hasattr(result.get('trade_df'), 'to_json'):
1189:                result['trade_df'] = result['trade_df'].to_json(date_format='iso', orient='split')
1302:                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
1403:            trade_df = None
1407:                result.get('trade_df'),    # æŸäº›ç­–ç•¥è‡ªå¸¶
1416:                    trade_df = df.copy()
1419:            if trade_df is None:
1421:                trade_df = pd.DataFrame(columns=['trade_date','type','price','shares','return'])
1437:            logger.info(f"[UI] {strategy} trades ä¾†æºå„ªå…ˆåºï¼štrades -> trades_ui -> trade_dfï¼›å¯¦éš›ä½¿ç”¨={'trades' if 'trades
' in result else ('trades_ui' if 'trades_ui' in result else 'trade_df')}")
1443:                trade_df = norm(trade_df)
1444:                logger.info(f"æ¨™æº–åŒ–å¾Œ trades_ui æ¬„ä½: {list(trade_df.columns)}")
1448:                if trade_df is not None and len(trade_df) > 0:
1449:                    trade_df = trade_df.copy()
1450:                    trade_df.columns = [str(c).lower() for c in trade_df.columns]
1453:                    if "trade_date" not in trade_df.columns:
1454:                        if "date" in trade_df.columns:
1455:                            trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
1456:                        elif isinstance(trade_df.index, pd.DatetimeIndex):
1457:                            trade_df = trade_df.reset_index().rename(columns={"index": "trade_date"})
1459:                            trade_df["trade_date"] = pd.NaT
1461:                        trade_df["trade_date"] = pd.to_datetime(trade_df["trade_date"], errors="coerce")
1464:                    if "type" not in trade_df.columns:
1465:                        if "action" in trade_df.columns:
1466:                            trade_df["type"] = trade_df["action"].astype(str).str.lower()
1467:                        elif "side" in trade_df.columns:
1468:                            trade_df["type"] = trade_df["side"].astype(str).str.lower()
1470:                            trade_df["type"] = "hold"
1473:                    if "price" not in trade_df.columns:
1475:                            if c in trade_df.columns:
1476:                                trade_df["price"] = trade_df[c]
1478:                        if "price" not in trade_df.columns:
1479:                            trade_df["price"] = 0.0
1482:            if 'trade_date' in trade_df.columns:
1483:                trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
1484:            if 'signal_date' in trade_df.columns:
1485:                trade_df['signal_date'] = pd.to_datetime(trade_df['signal_date'])
1486:            if 'price' in trade_df.columns:
1487:                trade_df['price'] = pd.to_numeric(trade_df['price'], errors='coerce')
1488:            if 'shares' in trade_df.columns:
1489:                trade_df['shares'] = pd.to_numeric(trade_df['shares'], errors='coerce')
1514:                base_df = trade_df
1517:            logger.info("[UI] trade_df åŸå§‹æ¬„ä½ï¼š%s", list(base_df.columns) if base_df is not None else None)
1544:            avg_holding = calculate_holding_periods(trade_df)
1590:            fig1 = plot_stock_price(df_raw, trade_df, ticker)
1611:            logger.info(f"[UI] trade_df cols={list(trade_df.columns)} head=\n{trade_df.head(3)}")
1619:                fig_w = plot_weight_series(daily_state_display, trade_df)
1678:                fig2 = plot_equity_cash(trade_df, df_raw)  # ä½¿ç”¨ SSSv096 çš„ fallback é‚è¼¯
1799:            trade_df = df_from_pack(result.get('trade_df'))
1804:                trade_df = norm(trade_df)
1807:                if trade_df is not None and len(trade_df) > 0:
1808:                    trade_df = trade_df.copy()
1809:                    trade_df.columns = [str(c).lower() for c in trade_df.columns]
1810:                    if "trade_date" not in trade_df.columns and "date" in trade_df.columns:
1811:                        trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
1812:                    if "type" not in trade_df.columns and "action" in trade_df.columns:
1813:                        trade_df["type"] = trade_df["action"].astype(str).str.lower()
1814:                    if "price" not in trade_df.columns:
1816:                            if c in trade_df.columns:
1817:                                trade_df["price"] = trade_df[c]
1820:            if 'trade_date' in trade_df.columns:
1821:                trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
1822:            if trade_df.empty:
1824:            buys = trade_df[trade_df['type'] == 'buy']
1825:            sells = trade_df[trade_df['type'] == 'sell']
1851:            trade_df = df_from_pack(result.get('trade_df'))
1856:                trade_df = norm(trade_df)
1859:                if trade_df is not None and len(trade_df) > 0:
1860:                    trade_df = trade_df.copy()
1861:                    trade_df.columns = [str(c).lower() for c in trade_df.columns]
1862:                    if "trade_date" not in trade_df.columns and "date" in trade_df.columns:
1863:                        trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
1864:                    if "type" not in trade_df.columns and "action" in trade_df.columns:
1865:                        trade_df["type"] = trade_df["action"].astype(str).str.lower()
1866:                    if "price" not in trade_df.columns:
1868:                            if c in trade_df.columns:
1869:                                trade_df["price"] = trade_df[c]
1872:            if 'trade_date' in trade_df.columns:
1873:                trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
1876:            detailed_stats = calculate_holding_periods(trade_df)
1949:                    html.Div("é¸æ“‡ç­–ç•¥ï¼ˆè‡ªå‹•è©•åˆ†ï¼šledger_std > ledger > trade_dfï¼‰",
2114:    trade_df = df_from_pack(result.get('trade_df'))
2119:        trade_df = norm(trade_df)
2122:        if trade_df is not None and len(trade_df) > 0:
2123:            trade_df = trade_df.copy()
2124:            trade_df.columns = [str(c).lower() for c in trade_df.columns]
2125:            if "trade_date" not in trade_df.columns and "date" in trade_df.columns:
2126:                trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
2127:            if "type" not in trade_df.columns and "action" in trade_df.columns:
2128:                trade_df["type"] = trade_df["action"].astype(str).str.lower()
2129:            if "price" not in trade_df.columns:
2131:                    if c in trade_df.columns:
2132:                        trade_df["price"] = trade_df[c]
2138:            trade_df.to_excel(writer, sheet_name='äº¤æ˜“ç´€éŒ„', index=False)
2167:    trade_df = df_from_pack(result.get('trade_df'))
2172:        trade_df = norm(trade_df)
2175:        if trade_df is not None and len(trade_df) > 0:
2176:            trade_df = trade_df.copy()
2177:            trade_df.columns = [str(c).lower() for c in trade_df.columns]
2178:            if "trade_date" not in trade_df.columns and "date" in trade_df.columns:
2179:                trade_df["trade_date"] = pd.to_datetime(trade_df["date"], errors="coerce")
2180:            if "type" not in trade_df.columns and "action" in trade_df.columns:
2181:                trade_df["type"] = trade_df["action"].astype(str).str.lower()
2182:            if "price" not in trade_df.columns:
2184:                    if c in trade_df.columns:
2185:                        trade_df["price"] = trade_df[c]
2192:        trade_df.to_csv(bytes_io, index=False, encoding='utf-8-sig')
2196:def calculate_strategy_detailed_stats(trade_df, df_raw):
2198:    if trade_df.empty:
2207:    if 'trade_date' in trade_df.columns:
2208:        trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
2211:    trade_df = trade_df.sort_values('trade_date').reset_index(drop=True)
2215:    for i in range(len(trade_df) - 1):
2216:        current_type = trade_df.iloc[i]['type']
2217:        next_type = trade_df.iloc[i+1]['type']
2219:            buy_date = trade_df.iloc[i]['trade_date']
2220:            sell_date = trade_df.iloc[i+1]['trade_date']
2227:    for i in range(len(trade_df) - 1):
2228:        current_type = trade_df.iloc[i]['type']
2229:        next_type = trade_df.iloc[i+1]['type']
2231:            sell_date = trade_df.iloc[i]['trade_date']
2232:            buy_date = trade_df.iloc[i+1]['trade_date']
2238:    last_trade = trade_df.iloc[-1] if not trade_df.empty else None
2259:            last_buy = trade_df[trade_df['type'] == 'buy']
2514:    trade_df = df_from_pack(cache.get("trade_data"))
2517:    if trade_df is None or trade_df.empty:
2531:        table = trade_contribution_by_phase(trade_df, equity, min_gap, cooldown).copy()
2795:    trade_df = df_from_pack(cache.get("trade_data"))
2798:    if trade_df is None or trade_df.empty:
2811:    debug_info.append(f"åŸå§‹äº¤æ˜“è³‡æ–™æ¬„ä½: {list(trade_df.columns)}")
2812:    debug_info.append(f"äº¤æ˜“è³‡æ–™è¡Œæ•¸: {len(trade_df)}")
2827:            if name in trade_df.columns:
2838:        if "weight_change" in trade_df.columns and "w_before" not in found_columns:
2840:            trade_df = trade_df.copy()
2841:            trade_df["w_before"] = 0.0
2842:            trade_df["w_after"] = trade_df["weight_change"]
2847:        if "price" in trade_df.columns and "type" not in found_columns:
2849:            trade_df["type"] = "buy"
2878:            if "weight_change" in trade_df.columns:
2879:                debug_info.append(f"æ‰¾åˆ° weight_change æ¬„ä½ï¼Œç¯„åœ: {trade_df['weight_change'].min():.4f} ~ {trade_df['weigh
t_change'].max():.4f}")
2885:            table = trade_contribution_by_phase(trade_df, equity, test_min_gap, test_cooldown)
2919:                        table = trade_contribution_by_phase(trade_df, equity, min_gap, cooldown)
3026:        (cache.get("trade_data") or cache.get("trade_df") or cache.get("trade_ledger") or cache.get("trade_ledger_std"))
3048:    # ç­–ç•¥è©•åˆ†ï¼šledger_std > ledger > trade_df
3056:        elif result.get("trade_df"):
3097:    # å„ªå…ˆé †åºï¼šledger_std > ledger > trade_df
3107:    elif result.get("trade_df"):
3108:        trade_data = df_from_pack(result["trade_df"])
3109:        data_source = "trade_df (äº¤æ˜“æ˜ç´°)"
3204:    # ç­–ç•¥è©•åˆ†ï¼šledger_std > ledger > trade_df
3212:        elif result.get("trade_df"):
3229:    # æº–å‚™äº¤æ˜“è³‡æ–™ï¼ˆå„ªå…ˆé †åºï¼šledger_std > ledger > trade_dfï¼‰
3239:    elif best_result.get("trade_df"):
3240:        trade_data = df_from_pack(best_result["trade_df"])
3241:        data_source = "trade_df (äº¤æ˜“æ˜ç´°)"
3547:def calculate_pareto_metrics(equity_curve, trade_df):
3576:                if trade_df is not None and not trade_df.empty and 'return' in trade_df.columns:
3577:                    profits = trade_df[trade_df['return'] > 0]['return'].sum()
3578:                    losses = abs(trade_df[trade_df['return'] < 0]['return'].sum())
3616:                if trade_df is not None and not trade_df.empty and 'trade_date' in trade_df.columns:
3617:                    trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
3619:                    risk_trigger_days = len(trade_df['trade_date'].dt.date.unique())
3626:            if trade_df is not None and not trade_df.empty:
3628:                if 'return' in trade_df.columns:
3629:                    profits = trade_df[trade_df['return'] > 0]['return'].sum()
3630:                    losses = abs(trade_df[trade_df['return'] < 0]['return'].sum())
3634:                if 'trade_date' in trade_df.columns:
3635:                    trade_df['trade_date'] = pd.to_datetime(trade_df['trade_date'])
3636:                    risk_trigger_days = len(trade_df['trade_date'].dt.date.unique())
3638:                    risk_trigger_days = len(trade_df)
3865:                        trade_df = None  # risk_valve_backtest ä¸ç›´æ¥æä¾›äº¤æ˜“è¨˜éŒ„
3867:                        metrics = calculate_pareto_metrics(equity_curve, trade_df)
The risk valve popped trade_df and metrics. I plan to recompute and adjust them.

root@a3bd66c837b3:/workspace/SSS096# rg "build_portfolio_ledger" -n
SSS_EnsembleTab.py
363:def build_portfolio_ledger(open_px: pd.Series, w: pd.Series, cost: CostParams,
850:    daily_state, trade_ledger = build_portfolio_ledger(
1376:    daily_state1, ledger1 = build_portfolio_ledger(open_px, w, cost)
1378:    daily_state2, ledger2 = build_portfolio_ledger(open_px, w2, cost)

PORTFOLIO_LEDGER_README.md
9:### 1. `build_portfolio_ledger()` å‡½æ•°
12:def build_portfolio_ledger(open_px, w, cost: CostParams, initial_capital=1_000_000.0, lot_size=None):
82:daily_state, trade_ledger = build_portfolio_ledger(
102:from SSS_EnsembleTab import build_portfolio_ledger, CostParams
112:daily_state, trade_ledger = build_portfolio_ledger(
root@a3bd66c837b3:/workspace/SSS096# sed -n '363,470p' SSS_EnsembleTab.py
def build_portfolio_ledger(open_px: pd.Series, w: pd.Series, cost: CostParams,
                          initial_capital: float = 1_000_000.0, lot_size: int | None = None) -> Tuple[pd.DataFrame, pd.DataFrame
]:
    """
    ä¾ç…§æ¯æ—¥ open åƒ¹èˆ‡ç›®æ¨™æ¬Šé‡ w_tï¼ˆå« floorã€delta_cap ç­‰é™åˆ¶å¾Œçš„æœ€çµ‚ w_tï¼‰ï¼Œ
    ç”¢å‡ºå…©å€‹ DataFrameï¼š
      1) daily_state: æ¯æ—¥ç¾é‡‘/æŒå€‰/ç¸½è³‡ç”¢/æ¬Šé‡
      2) trade_ledger: åªæœ‰æ¬Šé‡è®Šå‹•æ—¥çš„äº¤æ˜“æ˜ç´°ï¼ˆè²·è³£é‡‘é¡ã€è²»ç”¨ã€ç¨…ã€äº¤æ˜“å¾Œè³‡ç”¢ï¼‰
    """
    if cost is None:
        cost = CostParams()

    # è¨ˆç®—æ—¥å ±é…¬ç‡
    r = open_px.shift(-1) / open_px - 1
    r = r.dropna()

    # æ¬Šç›Šæ›²ç·š
    E = pd.Series(index=r.index, dtype=float)
    E.iloc[0] = initial_capital

    # ç¾é‡‘æ›²ç·š
    cash = pd.Series(index=r.index, dtype=float)
    cash.iloc[0] = initial_capital

    # æŒå€‰åƒ¹å€¼æ›²ç·š
    position_value = pd.Series(index=r.index, dtype=float)
    position_value.iloc[0] = 0.0

    # äº¤æ˜“è¨˜éŒ„
    trades = []

    # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°è¨˜éŒ„ï¼‰
    trade_ledger = []

    for i in range(1, len(r)):
        prev_w = w.iloc[i-1] if i-1 < len(w) else 0
        curr_w = w.iloc[i] if i < len(w) else 0

        # æ¬Šé‡è®ŠåŒ–
        dw = curr_w - prev_w

        if abs(dw) > 0.001:  # æœ‰é¡¯è‘—è®ŠåŒ–
            # 1) ä»¥ã€Œç¸½æ¬Šç›Šã€è¨ˆåŸ·è¡Œåç›®é‡‘é¡
            exec_notional = abs(dw) * E.iloc[i-1]

            if dw > 0:  # è²·å…¥
                c = exec_notional * cost.buy_rate
                tax = 0.0
                total_cost = c

                # è¨ˆç®—é¡¯ç¤ºç”¨è‚¡æ•¸ï¼ˆåƒ…ä¾› UI/å ±è¡¨ï¼Œä¸å½±éŸ¿é‚è¼¯ï¼‰
                px = float(open_px.iloc[i])
                shares_before = (prev_w * E.iloc[i-1]) / px if px > 0 else 0.0
                delta_units   =  exec_notional / px if px > 0 else 0.0
                shares_after  = shares_before + delta_units

                # äº¤æ˜“æ˜ç´°ï¼ˆçµ¦åœ–è¡¨/è¡¨æ ¼ï¼‰â€”è£œä¸Šæ¬Šé‡æ¬„ä½ï¼ˆè²·å…¥ä¹Ÿæœ‰ï¼‰
                trades.append({
                    'trade_date': r.index[i],
                    'type': 'buy',
                    'price_open': float(open_px.iloc[i]),
                    'price': float(open_px.iloc[i]),
                    'weight_change': float(dw),          # signed dw
                    'w_before': float(prev_w),
                    'w_after': float(curr_w),
                    'shares_before': float(shares_before),
                    'shares_after': float(shares_after),
                    'exec_notional': float(exec_notional),
                    'equity_after': float(E.iloc[i-1]),  # äº¤æ˜“ç™¼ç”Ÿåœ¨ i é–‹ç›¤ï¼Œäº¤æ˜“å¾Œçš„ç¸½è³‡ç”¢(åç›®)â‰ˆå‰ä¸€æ—¥ E æ‰£è²»ç”¨
                    'cash_after': np.nan                 # çœŸæ­£çš„æ—¥çµ‚ç¾é‡‘åœ¨ daily_state å…§ï¼Œé€™è£¡åƒ…ä¾› UI ç™¾åˆ†æ¯”æ¨å°ï¼ˆå¯é¸ï¼‰
                })

                # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°ï¼‰
                trade_ledger.append({
                    'date': r.index[i],
                    'type': 'buy',
                    'open': open_px.iloc[i],
                    'delta_units': delta_units,
                    'exec_notional': exec_notional,
                    'fee_buy': c,
                    'fee_sell': 0.0,
                    'tax': tax,  # è³£å‡ºæ‰æœƒæœ‰è­‰äº¤ç¨…ï¼Œé€™è£¡ç‚º 0
                    'shares_before': shares_before,
                    'shares_after': shares_after,
                    'w_before': prev_w,
                    'w_after': curr_w,
                    'cash_after': cash.iloc[i-1] - exec_notional - c,
                    'equity_open_after_trade': E.iloc[i-1] - total_cost,
                    'equity_after': E.iloc[i-1] * (1 + r.iloc[i] * curr_w) - total_cost
                })
            else:  # è³£å‡º
                c = exec_notional * cost.sell_rate
                tax = exec_notional * cost.sell_tax_rate
                total_cost = c + tax

                # è¨ˆç®—é¡¯ç¤ºç”¨æ•¸ï¼ˆåƒ…ä¾› UI/å ±è¡¨ï¼Œä¸å½±éŸ¿é‚è¼¯ï¼‰
                px = float(open_px.iloc[i])
                shares_before = (prev_w * E.iloc[i-1]) / px if px > 0 else 0.0
                delta_units   = -exec_notional / px if px > 0 else 0.0
                shares_after  = shares_before + delta_units

                # äº¤æ˜“æ˜ç´°ï¼ˆçµ¦åœ–è¡¨/è¡¨æ ¼ï¼‰â€”è£œä¸Šæ¬Šé‡æ¬„ä½
                trades.append({
                    'trade_date': r.index[i],
                    'type': 'sell',
                    'price_open': float(open_px.iloc[i]),
                    'price': float(open_px.iloc[i]),
                    'weight_change': float(dw),          # è² æ•¸
                    'w_before': float(prev_w),
'w_before': float(prev_w),
                    'w_after': float(curr_w),
                    'shares_before': float(shares_before),
                    'shares_after': float(shares_after),
                    'exec_notional': float(exec_notional),
                    'equity_after': float(E.iloc[i-1]),
                    'cash_after': np.nan
                })

                # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°ï¼‰
                trade_ledger.append({
                    'date': r.index[i],
                    'type': 'sell',
                    'open': open_px.iloc[i],
                    'delta_units': delta_units,
                    'exec_notional': exec_notional,
                    'fee_buy': 0.0,
                    'fee_sell': c,
                    'tax': tax,
                    'shares_before': shares_before,
                    'shares_after': shares_after,
                    'w_before': prev_w,
                    'w_after': curr_w,
                    'cash_after': cash.iloc[i-1] + exec_notional - total_cost,
                    'equity_open_after_trade': E.iloc[i-1] - total_cost,
                    'equity_after': E.iloc[i-1] * (1 + r.iloc[i] * curr_w) - total_cost
                })

            # æ‰£é™¤äº¤æ˜“æˆæœ¬
            E.iloc[i] = E.iloc[i-1] * (1 + r.iloc[i] * curr_w) - total_cost
            cash.iloc[i] = cash.iloc[i-1] - (dw * E.iloc[i-1] + total_cost) if dw > 0 else cash.iloc[i-1] + (abs(dw) * E.iloc[i-
1] - total_cost)
        else:
            E.iloc[i] = E.iloc[i-1] * (1 + r.iloc[i] * curr_w)
            cash.iloc[i] = cash.iloc[i-1]

        # è¨ˆç®—æŒå€‰åƒ¹å€¼
        position_value.iloc[i] = E.iloc[i] - cash.iloc[i]

    # è£œé½Šæ¬Šç›Šæ›²ç·šå’Œç¾é‡‘æ›²ç·šï¼ˆåŒ…æ‹¬æ²’æœ‰äº¤æ˜“çš„å¤©æ•¸ï¼‰
    E = E.reindex(open_px.index).ffill().fillna(initial_capital)
    cash = cash.reindex(open_px.index).ffill().fillna(initial_capital)
    position_value = position_value.reindex(open_px.index).ffill().fillna(0.0)

    # è½‰æ›äº¤æ˜“è¨˜éŒ„ç‚º DataFrame
    if trades:
        trades_df = pd.DataFrame(trades)
    else:
        trades_df = pd.DataFrame(columns=['trade_date', 'type', 'price_open', 'weight_change', 'cost'])

    # è½‰æ›äº¤æ˜“æµæ°´å¸³ç‚º DataFrame
    if trade_ledger:
        trade_ledger_df = pd.DataFrame(trade_ledger)
    else:
        trade_ledger_df = pd.DataFrame(columns=['date', 'type', 'open', 'delta_units', 'exec_notional', 'fee_buy', 'fee_sell', '
tax', 'cash_after', 'equity_after'])

    # æ§‹å»ºæ¯æ—¥ç‹€æ…‹ DataFrame
    daily_state = pd.DataFrame({
        'equity': E,
        'cash': cash,
        'position_value': position_value,
        'w': w.reindex(open_px.index).fillna(0),
        'invested_pct': position_value / E,
        'cash_pct': cash / E
    })

    return daily_state, trade_ledger_df


def equity_open_to_open(open_px: pd.Series, w: pd.Series, cost: CostParams | None = None,
                        start_equity: float = 1.0) -> Tuple[pd.Series, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """è¨ˆç®— Open-to-Open æ¬Šç›Šæ›²ç·šã€äº¤æ˜“è¨˜éŒ„ã€æ¯æ—¥ç‹€æ…‹å’Œäº¤æ˜“æµæ°´å¸³"""
    if cost is None:
        cost = CostParams()

    # è¨ˆç®—æ—¥å ±é…¬ç‡
    r = open_px.shift(-1) / open_px - 1
    r = r.dropna()

    # æ¬Šç›Šæ›²ç·š
    E = pd.Series(index=r.index, dtype=float)
    E.iloc[0] = start_equity

    # ç¾é‡‘æ›²ç·š
    cash = pd.Series(index=r.index, dtype=float)
    cash.iloc[0] = start_equity

    # äº¤æ˜“è¨˜éŒ„
    trades = []

    # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°è¨˜éŒ„ï¼‰
    trade_ledger = []

    for i in range(1, len(r)):
        prev_w = w.iloc[i-1] if i-1 < len(w) else 0
        curr_w = w.iloc[i] if i < len(w) else 0

        # æ¬Šé‡è®ŠåŒ–
        dw = curr_w - prev_w

        if abs(dw) > 0.001:  # æœ‰é¡¯è‘—è®ŠåŒ–
            # 1) ä»¥ã€Œç¸½æ¬Šç›Šã€è¨ˆåŸ·è¡Œåç›®é‡‘é¡
            exec_notional = abs(dw) * E.iloc[i-1]

            if dw > 0:  # è²·å…¥
                c = exec_notional * cost.buy_rate
                tax = 0.0
                total_cost = c

                # è¨ˆç®—é¡¯ç¤ºç”¨è‚¡æ•¸ï¼ˆåƒ…ä¾› UI/å ±è¡¨ï¼Œä¸å½±éŸ¿é‚è¼¯ï¼‰
                px = float(open_px.iloc[i])
                shares_before = (prev_w * E.iloc[i-1]) / px if px > 0 else 0.0
                delta_units   =  exec_notional / px if px > 0 else 0.0
                shares_after  = shares_before + delta_units

                # äº¤æ˜“æ˜ç´°ï¼ˆçµ¦åœ–è¡¨/è¡¨æ ¼ï¼‰â€”è£œä¸Šæ¬Šé‡æ¬„ä½ï¼ˆè²·å…¥ä¹Ÿæœ‰ï¼‰
                trades.append({
                    'trade_date': r.index[i],
                    'type': 'buy',
                    'price_open': open_px.iloc[i],
                    'weight_change': dw,          # ç”¨ signed dwï¼Œè²·ç‚ºæ­£ã€è³£ç‚ºè² 
                    'w_before': prev_w,
                    'w_after': curr_w,
                    'shares_before': shares_before,
                    'shares_after': shares_after,
                    'cost': total_cost
                })

                # äº¤æ˜“æµæ°´å¸³ï¼ˆè©³ç´°ï¼‰
                trade_ledger.append({
                    'date': r.index[i],
                    'type': 'buy',
                    'open': open_px.iloc[i],
                    'delta_units': delta_units,
                    'exec_notional': exec_notional,
                    'fee_buy': c,
                    'fee_sell': 0.0,
                    'tax': tax,  # è³£å‡ºæ‰æœƒæœ‰è­‰äº¤ç¨…ï¼Œé€™è£¡ç‚º 0
                    'shares_before': shares_before,
                    'shares_after': shares_after,
                    'w_before': prev_w,
                    'w_after': curr_w,
Iâ€™ll keep the existing metrics and just add risk_valve metrics separately, avoiding removals.

After applying the risk valve, I'll recompute metrics using equity curve and ledger data.

Iâ€™ll compute basic metrics from equity and returns, avoiding full calculate_metrics complexity.

Iâ€™ll extend the imports in app_dash.py to include necessary functions from SSS_EnsembleTab.

Iâ€™ll recompute performance metrics and include a timestamped comment in the code.

root@a3bd66c837b3:/workspace/SSS096# date
Wed Aug 20 01:23:36 UTC 2025
I'll edit app_dash.py to restore trade_df and update metrics after key loops.

I should compute metrics before setting _risk_valve_applied. I'll edit with sed to implement this.

root@a3bd66c837b3:/workspace/SSS096# sed -n '1270,1310p' app_dash.py
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
I'll add setting trade_df after trade_ledger and daily_state. Also, I'll import the necessary functions and compute metrics accordingly.

root@a3bd66c837b3:/workspace/SSS096# sed -n '1250,1330p' app_dash.py

                        # åŸºæº–ï¼ˆæœ‰é«˜ä½åƒ¹å°±å¸¶ä¸Šï¼‰
                        bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
                        if 'close' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
                        elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
                            bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
                        if 'high' in df_raw.columns and 'low' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
                        elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
                            bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
                            bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")

                        result_cap = risk_valve_backtest(
                            open_px=open_px,
                            w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
                            cost=cost,
                            benchmark_df=bench,
                            mode="cap",
                            cap_level=1.0,
                        )
                    except Exception as e:
                        logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
                        result_cap = None

                    if result_cap:
                        # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
                        logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataFrame()))
} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")

                        # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
                        if 'trade_ledger_valve' in result_cap:
                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])

                        if 'daily_state_valve' in result_cap:
                            result['daily_state'] = pack_df(result_cap['daily_state_valve'])
                            result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])

                        if 'weights_valve' in result_cap:
                            result['weight_curve'] = pack_series(result_cap['weights_valve'])

                        # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
                        if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
                            try:
                                result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
                            except Exception:
                                # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
                                result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equity'))

                        # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                            if k in result:
                                result.pop(k, None)

                        # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                        result['_risk_valve_applied'] = True
                        result['_risk_valve_params'] = {
                            'cap': float(risk_cap),
                            'atr_ratio': float(atr_ratio),
                            'atr20_last': float(atr_20_valid.iloc[-1]) if len(atr_20_valid) > 0 else None,
                            'atr60_last': float(atr_60_valid.iloc[-1]) if len(atr_60_valid) > 0 else None,
                        }

                        true_days = int(mask_aligned.sum())
                        logger.info(f"[{strat}] å…¨å±€é¢¨éšªé–¥é–€å·²å¥—ç”¨ï¼ˆé€æ—¥ï¼‰ï¼Œé¢¨éšªå¤©æ•¸={true_days}, CAP={risk_cap:.2f}")
                    else:
                        logger.warning(f"[{strat}] é¢¨éšªé–¥é–€é‡ç®—æ²’æœ‰è¿”å›çµæœ")

        results[strat] = result

    # ä½¿ç”¨ç¬¬ä¸€å€‹ç­–ç•¥çš„æ•¸æ“šä½œç‚ºä¸»è¦é¡¯ç¤ºæ•¸æ“š
    first_strat = list(results.keys())[0] if results else strategy_names[0]
    first_smaa_src = param_presets[first_strat].get("smaa_source", "Self")
    df_raw_main, _ = load_data(ticker, start_date, end_date if end_date else None, smaa_source=first_smaa_src)

    # çµ±ä¸€ä½¿ç”¨ orient="split" åºåˆ—åŒ–ï¼Œç¢ºä¿ä¸€è‡´æ€§
    payload = {
        'results': results,
        'df_raw': df_raw_main.to_json(date_format='iso', orient='split'),
I need to import risk_valve_backtest, CostParams, and do some metric assignments.

root@a3bd66c837b3:/workspace/SSS096# apply_patch <<'PATCH'
_return': float(total> *** Begin Patch
_ret),
+        > *** Update File: app_dash.py
> @@
al_> -                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
> +                        from SSS_EnsembleTab import (
tio> +                            risk_valve_backtest,
> +                            CostParams,
> +                            _mdd_from_daily_equity,
> +                            _sell_returns_pct_from_ledger,
 > +                        )
> @@
> -                        if 'trade_ledger_valve' in result_cap:
> -                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
> -                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
> -                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
> +                        if 'trade_ledger_valve' in result_cap:
f not np.isnan(sharpe) else np.nan,
+             > +                            result['trades'] = pack_df(result_cap['trade_ledger_valve'])
> +                            result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
           > +                            result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
> +                            result['trade_df'] = pack_df(result_cap['trade_ledger_valve'])
> @@
> -                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
> +                        for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
>                              if k in result:
>                                  result.pop(k, None)
> +
> +                        # 2025-08-20 é‡ç®—æŒ‡æ¨™ä»¥ä¿ç•™ç¸¾æ•ˆè³‡è¨Š #app_dash.py
> +                        ledger_valve = result_cap.get('trade_ledger_valve', pd.DataFrame())
> +                        ds_valve = result_cap.get('daily_state_valve', pd.DataFrame())
> +                        if not ledger_valve.empty and not ds_valve.empty and 'equity' in ds_valve:
> +                            r = _sell_returns_pct_from_ledger(ledger_valve)
> +                            eq = ds_valve['equity']
> +                            total_ret = eq.iloc[-1] / eq.iloc[0] - 1
> +                            years = max((eq.index[-1] - eq.index[0]).days / 365.25, 1)
> +                            ann_ret = (1 + total_ret) ** (1 / years) - 1
> +                            mdd = _mdd_from_daily_equity(eq)
> +                            dd = eq / eq.cummax() - 1
> +                            blocks = (~(dd < 0)).cumsum()
> +                            dd_dur = int((dd.groupby(blocks).cumcount() + 1).where(dd < 0).max() or 0)
> +                            num_trades = len(r)
> +                            win_rate = (r > 0).sum() / num_trades if num_trades > 0 else 0
> +                            avg_win = r[r > 0].mean() if win_rate > 0 else np.nan
> +                            avg_loss = r[r < 0].mean() if win_rate < 1 else np.nan
> +                            payoff = abs(avg_win / avg_loss) if avg_loss != 0 and not np.isnan(avg_win) else np.nan
> +                            daily_r = eq.pct_change().dropna()
> +                            sharpe = (daily_r.mean() * np.sqrt(252)) / daily_r.std() if daily_r.std() != 0 else np.nan
> +                            downside = daily_r[daily_r < 0]
> +                            sortino = (daily_r.mean() * np.sqrt(252)) / downside.std() if downside.std() != 0 else np.nan
> +                            ann_vol = daily_r.std() * np.sqrt(252) if len(daily_r) > 0 else np.nan
> +                            prof = r[r > 0].sum()
> +                            loss = abs(r[r < 0].sum())
> +                            pf = prof / loss if loss != 0 else np.nan
> +                            win_flag = r > 0
> +                            grp = (win_flag != win_flag.shift()).cumsum()
> +                            consec = win_flag.groupby(grp).cumcount() + 1
> +                            max_wins = int(consec[win_flag].max() if True in win_flag.values else 0)
> +                            max_losses = int(consec[~win_flag].max() if False in win_flag.values else 0)
> +                            result['metrics'] = {
> +                                'total_return': float(total_ret),
> +                                'annual_return': float(ann_ret),
> +                                'max_drawdown': float(mdd),
> +                                'max_drawdown_duration': dd_dur,
> +                                'calmar_ratio': float(ann_ret / abs(mdd)) if mdd < 0 else np.nan,
> +                                'num_trades': int(num_trades),
> +                                'win_rate': float(win_rate),
> +                                'avg_win': float(avg_win) if not np.isnan(avg_win) else np.nan,
> +                                'avg_loss': float(avg_loss) if not np.isnan(avg_loss) else np.nan,
> +                                'payoff_ratio': float(payoff) if not np.isnan(payoff) else np.nan,
> +                                'sharpe_ratio': float(sharpe) if not np.isnan(sharpe) else np.nan,
> +                                'sortino_ratio': float(sortino) if not np.isnan(sortino) else np.nan,
> +                                'max_consecutive_wins': max_wins,
> +                                'max_consecutive_losses': max_losses,
> +                                'annualized_volatility': float(ann_vol) if not np.isnan(ann_vol) else np.nan,
> +                                'profit_factor': float(pf) if not np.isnan(pf) else np.nan,
> +                            }
> *** End Patch
> PATCH
Success. Updated the following files:
M app_dash.py
Patch succeeded, but I should verify pandas and numpy imports. Running git status next.

root@a3bd66c837b3:/workspace/SSS096# git status --short
 M app_dash.py
root@a3bd66c837b3:/workspace/SSS096# powershell -ExecutionPolicy Bypass -File tools\quick_check.ps1
bash: command not found: powershell
I'll give pwsh a shot since the PowerShell command isn't found.

root@a3bd66c837b3:/workspace/SSS096# pwsh -Command ./tools/quick_check.ps1
bash: command not found: pwsh
The container lacks PowerShell. I'll attempt to install it with apt-get, but it might not work.

root@a3bd66c837b3:/workspace/SSS096# apt-get update >/tmp/apt.log && tail -n 20 /tmp/apt.log


^C
root@a3bd66c837b3:/workspace/SSS096# tail -n 20 /tmp/apt.log
Err:1 http://security.ubuntu.com/ubuntu noble-security InRelease
  403  Forbidden [IP: 172.30.2.131 8080]
Err:2 http://apt.llvm.org/noble llvm-toolchain-noble-20 InRelease
  403  Forbidden [IP: 172.30.2.131 8080]
Ign:3 https://mise.jdx.dev/deb stable InRelease
Err:4 http://archive.ubuntu.com/ubuntu noble InRelease
  403  Forbidden [IP: 172.30.2.131 8080]
Err:5 http://archive.ubuntu.com/ubuntu noble-updates InRelease
  403  Forbidden [IP: 172.30.2.131 8080]
Err:6 http://archive.ubuntu.com/ubuntu noble-backports InRelease
  403  Forbidden [IP: 172.30.2.131 8080]
Ign:3 https://mise.jdx.dev/deb stable InRelease
Ign:3 https://mise.jdx.dev/deb stable InRelease
Since no internet, I can't install PowerShell. I'll note failed attempt in the message and test log. I'll also run python -m py_compile app_dash.py to check for syntax errors.

root@a3bd66c837b3:/workspace/SSS096# python -m py_compile app_dash.py
I'm including a timestamp and Chinese comment, confirming it's in the code with git diff.

root@a3bd66c837b3:/workspace/SSS096# git diff --staged
root@a3bd66c837b3:/workspace/SSS096# git add app_dash.py
root@a3bd66c837b3:/workspace/SSS096# git status --short
M  app_dash.py
?? __pycache__/
I need to remove pycache from git status since it shouldn't be committed.

root@a3bd66c837b3:/workspace/SSS096# git clean -fd __pycache__
Removing __pycache__/
root@a3bd66c837b3:/workspace/SSS096# git status --short
M  app_dash.py
Ah, M means modified but not staged. If no space, itâ€™s staged already.

diff --git a/app_dash.py b/app_dash.py
index 6685ece..f2cb263 100644
--- a/app_dash.py
+++ b/app_dash.py
@@ -1237,7 +1237,12 @@ def run_backtest(n_clicks, auto_run, ticker, start_date, end_date, discount, coo

                     # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
                     try:
-                        from SSS_EnsembleTab import risk_valve_backtest, CostParams
+                        from SSS_EnsembleTab import (
+                            risk_valve_backtest,
+                            CostParams,
+                            _mdd_from_daily_equity,
+                            _sell_returns_pct_from_ledger,
+                        )

                         # æˆæœ¬åƒæ•¸
                         trade_cost = (strat_params.get("trade_cost", {})
@@ -1282,6 +1287,7 @@ def run_backtest(n_clicks, auto_run, ticker, start_date, end_date, discount, coo
                             result['trades'] = pack_df(result_cap['trade_ledger_valve'])
                             result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
                             result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
+                            result['trade_df'] = pack_df(result_cap['trade_ledger_valve'])

                         if 'daily_state_valve' in result_cap:
                             result['daily_state'] = pack_df(result_cap['daily_state_valve'])
@@ -1302,6 +1308,56 @@ def run_backtest(n_clicks, auto_run, ticker, start_date, end_date, discount, coo
                         for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
                             if k in result:
                                 result.pop(k, None)
+
+                        # 2025-08-20 é‡ç®—æŒ‡æ¨™ä»¥ä¿ç•™ç¸¾æ•ˆè³‡è¨Š #app_dash.py
+                        ledger_valve = result_cap.get('trade_ledger_valve', pd.DataFrame())
+                        ds_valve = result_cap.get('daily_state_valve', pd.DataFrame())
+                        if not ledger_valve.empty and not ds_valve.empty and 'equity' in ds_valve:
+                            r = _sell_returns_pct_from_ledger(ledger_valve)
+                            eq = ds_valve['equity']
+                            total_ret = eq.iloc[-1] / eq.iloc[0] - 1
+                            years = max((eq.index[-1] - eq.index[0]).days / 365.25, 1)
+                            ann_ret = (1 + total_ret) ** (1 / years) - 1
+                            mdd = _mdd_from_daily_equity(eq)
+                            dd = eq / eq.cummax() - 1
+                            blocks = (~(dd < 0)).cumsum()
+                            dd_dur = int((dd.groupby(blocks).cumcount() + 1).where(dd < 0).max() or 0)
+                            num_trades = len(r)
+                            win_rate = (r > 0).sum() / num_trades if num_trades > 0 else 0
+                            avg_win = r[r > 0].mean() if win_rate > 0 else np.nan
+                            avg_loss = r[r < 0].mean() if win_rate < 1 else np.nan
+                            payoff = abs(avg_win / avg_loss) if avg_loss != 0 and not np.isnan(avg_win) else np.nan
+                            daily_r = eq.pct_change().dropna()
+                            sharpe = (daily_r.mean() * np.sqrt(252)) / daily_r.std() if daily_r.std() != 0 else np.nan
+                            downside = daily_r[daily_r < 0]
+                            sortino = (daily_r.mean() * np.sqrt(252)) / downside.std() if downside.std() != 0 else np.nan
+                            ann_vol = daily_r.std() * np.sqrt(252) if len(daily_r) > 0 else np.nan
+                            prof = r[r > 0].sum()
+                            loss = abs(r[r < 0].sum())
+                            pf = prof / loss if loss != 0 else np.nan
+                            win_flag = r > 0
+                            grp = (win_flag != win_flag.shift()).cumsum()
+                            consec = win_flag.groupby(grp).cumcount() + 1
+                            max_wins = int(consec[win_flag].max() if True in win_flag.values else 0)
+                            max_losses = int(consec[~win_flag].max() if False in win_flag.values else 0)
+                            result['metrics'] = {
+                                'total_return': float(total_ret),
+                                'annual_return': float(ann_ret),
+                                'max_drawdown': float(mdd),
+                                'max_drawdown_duration': dd_dur,
+                                'calmar_ratio': float(ann_ret / abs(mdd)) if mdd < 0 else np.nan,
+                                'num_trades': int(num_trades),
+                                'win_rate': float(win_rate),
+                                'avg_win': float(avg_win) if not np.isnan(avg_win) else np.nan,
+                                'avg_loss': float(avg_loss) if not np.isnan(avg_loss) else np.nan,
+                                'payoff_ratio': float(payoff) if not np.isnan(payoff) else np.nan,
+                                'sharpe_ratio': float(sharpe) if not np.isnan(sharpe) else np.nan,
+                                'sortino_ratio': float(sortino) if not np.isnan(sortino) else np.nan,
+                                'max_consecutive_wins': max_wins,
+                                'max_consecutive_losses': max_losses,
+                                'annualized_volatility': float(ann_vol) if not np.isnan(ann_vol) else np.nan,
+                                'profit_factor': float(pf) if not np.isnan(pf) else np.nan,
+                            }

                         # 3) çµ¦ UI ä¸€å€‹æ——æ¨™èˆ‡åƒæ•¸ï¼Œä¾¿æ–¼é¡¯ç¤ºã€Œå·²å¥—ç”¨ã€
                         result['_risk_valve_applied'] = True
The diff includes necessary comment and modifications. Timestamp and path comment look good!

root@a3bd66c837b3:/workspace/SSS096# git commit -m "fix risk valve metrics" && git status --short
[work c4d695b] fix risk valve metrics
 1 file changed, 57 insertions(+), 1 deletion(-)
root@a3bd66c837b3:/workspace/SSS096# git status --short
I'll grab the line numbers in app_dash.py by running nl -ba for citation.

root@a3bd66c837b3:/workspace/SSS096# nl -ba app_dash.py | sed -n '1237,1320p'
  1237
  1238                      # è‹¥ä½ æ²¿ç”¨ç¾æœ‰çš„ risk_valve_backtestï¼Œçµ¦ cap_level=1.0 è¡¨ç¤ºã€Œw å·²ç¶“æ˜¯ç›®æ¨™åºåˆ—ã€
  1239                      try:
  1240                          from SSS_EnsembleTab import (
  1241                              risk_valve_backtest,
  1242                              CostParams,
  1243                              _mdd_from_daily_equity,
  1244                              _sell_returns_pct_from_ledger,
  1245                          )
  1246
  1247                          # æˆæœ¬åƒæ•¸
  1248                          trade_cost = (strat_params.get("trade_cost", {})
  1249                                        if isinstance(strat_params, dict) else {})
  1250                          cost = CostParams(
  1251                              buy_fee_bp=float(trade_cost.get("buy_fee_bp", 4.27)),
  1252                              sell_fee_bp=float(trade_cost.get("sell_fee_bp", 4.27)),
  1253                              sell_tax_bp=float(trade_cost.get("sell_tax_bp", 30.0)),
  1254                          )
  1255
  1256                          # åŸºæº–ï¼ˆæœ‰é«˜ä½åƒ¹å°±å¸¶ä¸Šï¼‰
  1257                          bench = pd.DataFrame(index=pd.to_datetime(df_raw.index))
  1258                          if 'close' in df_raw.columns:
  1259                              bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["close"], errors="coerce")
  1260                          elif 'æ”¶ç›¤åƒ¹' in df_raw.columns:
  1261                              bench["æ”¶ç›¤åƒ¹"] = pd.to_numeric(df_raw["æ”¶ç›¤åƒ¹"], errors="coerce")
  1262                          if 'high' in df_raw.columns and 'low' in df_raw.columns:
  1263                              bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["high"], errors="coerce")
  1264                              bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["low"], errors="coerce")
  1265                          elif 'æœ€é«˜åƒ¹' in df_raw.columns and 'æœ€ä½åƒ¹' in df_raw.columns:
  1266                              bench["æœ€é«˜åƒ¹"] = pd.to_numeric(df_raw["æœ€é«˜åƒ¹"], errors="coerce")
  1267                              bench["æœ€ä½åƒ¹"] = pd.to_numeric(df_raw["æœ€ä½åƒ¹"], errors="coerce")
  1268
  1269                          result_cap = risk_valve_backtest(
  1270                              open_px=open_px,
  1271                              w=ds["w"].astype(float).reindex(open_px.index).fillna(0.0),
  1272                              cost=cost,
  1273                              benchmark_df=bench,
  1274                              mode="cap",
  1275                              cap_level=1.0,
  1276                          )
  1277                      except Exception as e:
  1278                          logger.warning(f"[{strat}] ç„¡æ³•å°å…¥ risk_valve_backtest: {e}")
  1279                          result_cap = None
  1280
  1281                      if result_cap:
  1282                          # === å®‰å…¨è¦†å¯«ï¼šæ¸…æ‰èˆŠéµä¸¦è£œé½Šæ–°éµ ===
  1283                          logger.info(f"[UI_CHECK] å³å°‡è¦†å¯«ï¼šnew_trades={len(result_cap.get('trade_ledger_valve', pd.DataF
rame()))} rows, new_ds={len(result_cap.get('daily_state_valve', pd.DataFrame()))} rows")
  1284
  1285                          # 1) è¦†å¯«çµæœ â€”â€” ä¸€å¾‹ç”¨ pack_df/pack_series
  1286                          if 'trade_ledger_valve' in result_cap:
  1287                              result['trades'] = pack_df(result_cap['trade_ledger_valve'])
  1288                              result['trade_ledger'] = pack_df(result_cap['trade_ledger_valve'])
  1289                              result['trade_ledger_std'] = pack_df(result_cap['trade_ledger_valve'])
  1290                              result['trade_df'] = pack_df(result_cap['trade_ledger_valve'])
  1291
  1292                          if 'daily_state_valve' in result_cap:
  1293                              result['daily_state'] = pack_df(result_cap['daily_state_valve'])
  1294                              result['daily_state_std'] = pack_df(result_cap['daily_state_valve'])
  1295
  1296                          if 'weights_valve' in result_cap:
  1297                              result['weight_curve'] = pack_series(result_cap['weights_valve'])
  1298
  1299                          # æ¬Šç›Šæ›²ç·šï¼šè‹¥æ˜¯ Series
  1300                          if 'daily_state_valve' in result_cap and 'equity' in result_cap['daily_state_valve']:
  1301                              try:
  1302                                  result['equity_curve'] = pack_series(result_cap['daily_state_valve']['equity'])
  1303                              except Exception:
  1304                                  # è‹¥ä½ å­˜çš„æ˜¯ DataFrame
  1305                                  result['equity_curve'] = pack_df(result_cap['daily_state_valve']['equity'].to_frame('equ
ity'))
  1306
  1307                          # 2) **é—œéµ**ï¼šæŠŠ UI å¯èƒ½æ‹¿ä¾†ç”¨çš„èˆŠå¿«å–æ¸…æ‰ï¼Œå¼·è¿« UI èµ°æ–°è³‡æ–™
  1308                          for k in ['trades_ui', 'trade_df', 'trade_ledger_std', 'metrics']:
  1309                              if k in result:
  1310                                  result.pop(k, None)
  1311
  1312                          # 2025-08-20 é‡ç®—æŒ‡æ¨™ä»¥ä¿ç•™ç¸¾æ•ˆè³‡è¨Š #app_dash.py
  1313                          ledger_valve = result_cap.get('trade_ledger_valve', pd.DataFrame())
  1314                          ds_valve = result_cap.get('daily_state_valve', pd.DataFrame())
  1315                          if not ledger_valve.empty and not ds_valve.empty and 'equity' in ds_valve:
  1316                              r = _sell_returns_pct_from_ledger(ledger_valve)
  1317                              eq = ds_valve['equity']
  1318                              total_ret = eq.iloc[-1] / eq.iloc[0] - 1
  1319                              years = max((eq.index[-1] - eq.index[0]).days / 365.25, 1)
  1320                              ann_ret = (1 + total_ret) ** (1 / years) - 1
root@a3bd66c837b3:/workspace/SSS096# nl -ba app_dash.py | sed -n '1320,1360p'
  1320                              ann_ret = (1 + total_ret) ** (1 / years) - 1
  1321                              mdd = _mdd_from_daily_equity(eq)
  1322                              dd = eq / eq.cummax() - 1
  1323                              blocks = (~(dd < 0)).cumsum()
  1324                              dd_dur = int((dd.groupby(blocks).cumcount() + 1).where(dd < 0).max() or 0)
  1325                              num_trades = len(r)
  1326                              win_rate = (r > 0).sum() / num_trades if num_trades > 0 else 0
  1327                              avg_win = r[r > 0].mean() if win_rate > 0 else np.nan
  1328                              avg_loss = r[r < 0].mean() if win_rate < 1 else np.nan
  1329                              payoff = abs(avg_win / avg_loss) if avg_loss != 0 and not np.isnan(avg_win) else np.nan
  1330                              daily_r = eq.pct_change().dropna()
  1331                              sharpe = (daily_r.mean() * np.sqrt(252)) / daily_r.std() if daily_r.std() != 0 else np.nan
  1332                              downside = daily_r[daily_r < 0]
  1333                              sortino = (daily_r.mean() * np.sqrt(252)) / downside.std() if downside.std() != 0 else np.na
n
  1334                              ann_vol = daily_r.std() * np.sqrt(252) if len(daily_r) > 0 else np.nan
  1335                              prof = r[r > 0].sum()
  1336                              loss = abs(r[r < 0].sum())
  1337                              pf = prof / loss if loss != 0 else np.nan
  1338                              win_flag = r > 0
  1339                              grp = (win_flag != win_flag.shift()).cumsum()
  1340                              consec = win_flag.groupby(grp).cumcount() + 1
  1341                              max_wins = int(consec[win_flag].max() if True in win_flag.values else 0)
  1342                              max_losses = int(consec[~win_flag].max() if False in win_flag.values else 0)
  1343                              result['metrics'] = {
  1344                                  'total_return': float(total_ret),
  1345                                  'annual_return': float(ann_ret),
  1346                                  'max_drawdown': float(mdd),
  1347                                  'max_drawdown_duration': dd_dur,
  1348                                  'calmar_ratio': float(ann_ret / abs(mdd)) if mdd < 0 else np.nan,
  1349                                  'num_trades': int(num_trades),
  1350                                  'win_rate': float(win_rate),
  1351                                  'avg_win': float(avg_win) if not np.isnan(avg_win) else np.nan,
  1352                                  'avg_loss': float(avg_loss) if not np.isnan(avg_loss) else np.nan,
  1353                                  'payoff_ratio': float(payoff) if not np.isnan(payoff) else np.nan,
  1354                                  'sharpe_ratio': float(sharpe) if not np.isnan(sharpe) else np.nan,
  1355                                  'sortino_ratio': float(sortino) if not np.isnan(sortino) else np.nan,
  1356                                  'max_consecutive_wins': max_wins,
  1357                                  'max_consecutive_losses': max_losses,
  1358                                  'annualized_volatility': float(ann_vol) if not np.isnan(ann_vol) else np.nan,
  1359                                  'profit_factor': float(pf) if not np.isnan(pf) else np.nan,
  1360                              }
I'll finalize the PR message describing the fix, testing, and references to logs.

I'll prepare the final message in Chinese, summarizing code changes and including necessary citations.

I'll summarize: metrics recalculated using ledger and equity; testing successful except powershell installation failure.