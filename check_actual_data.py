# check_actual_data.py
# æª¢æŸ¥å¯¦éš›çš„æ•¸æ“šçµæ§‹å’Œå…§å®¹
# å‰µå»ºæ™‚é–“ï¼š2025-08-21 02:05:00
# è·¯å¾‘ï¼š#æ ¹ç›®éŒ„/ä»£ç¢¼

import pandas as pd
import numpy as np
import json
from pathlib import Path
import sys
import os

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.getcwd())

def check_enhanced_trades_cache_structure():
    """æª¢æŸ¥ enhanced-trades-cache çš„æ•¸æ“šçµæ§‹"""
    print("ğŸ” æª¢æŸ¥ enhanced-trades-cache æ•¸æ“šçµæ§‹...")
    print("=" * 60)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸é—œçš„æ•¸æ“šæ–‡ä»¶
    data_files = [
        "enhanced_trades_cache_sample.json",
        "enhanced_trades_cache.json",
        "enhanced_trades_cache_*.json"
    ]
    
    print("ğŸ“ æŸ¥æ‰¾ enhanced-trades-cache ç›¸é—œæ–‡ä»¶:")
    for pattern in data_files:
        if "*" in pattern:
            import glob
            files = glob.glob(pattern)
            for file in files:
                print(f"   ğŸ“„ {file}")
        else:
            if Path(pattern).exists():
                print(f"   ğŸ“„ {pattern}")
            else:
                print(f"   âŒ {pattern} (ä¸å­˜åœ¨)")
    
    # æª¢æŸ¥ app_dash.py ä¸­ enhanced-trades-cache çš„ä½¿ç”¨
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("\nğŸ” æª¢æŸ¥ enhanced-trades-cache åœ¨ä»£ç¢¼ä¸­çš„ä½¿ç”¨:")
    
    # æŸ¥æ‰¾ enhanced-trades-cache çš„ä½¿ç”¨æ¨¡å¼
    cache_patterns = [
        'enhanced-trades-cache',
        'enhanced_trades_cache',
        'enhanced-trades-cache.*data'
    ]
    
    for pattern in cache_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    # æŸ¥æ‰¾ enhanced-trades-cache çš„æ•¸æ“šçµæ§‹
    print("\nğŸ” æª¢æŸ¥ enhanced-trades-cache çš„æ•¸æ“šçµæ§‹:")
    
    # æŸ¥æ‰¾æ•¸æ“šè¼‰å…¥é‚è¼¯
    data_loading_patterns = [
        'cache.get("df_raw")',
        'cache.get("daily_state")',
        'cache.get("strategy")',
        'cache.get("trades")',
        'cache.get("weight_curve")'
    ]
    
    for pattern in data_loading_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    return True

def check_backtest_store_structure():
    """æª¢æŸ¥ backtest-store çš„æ•¸æ“šçµæ§‹"""
    print("\nğŸ” æª¢æŸ¥ backtest-store æ•¸æ“šçµæ§‹...")
    print("=" * 60)
    
    # æª¢æŸ¥ app_dash.py ä¸­ backtest-store çš„ä½¿ç”¨
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("ğŸ“ æª¢æŸ¥ backtest-store åœ¨ä»£ç¢¼ä¸­çš„ä½¿ç”¨:")
    
    # æŸ¥æ‰¾ backtest-store çš„ä½¿ç”¨æ¨¡å¼
    store_patterns = [
        'backtest-store',
        'backtest_store',
        'backtest-store.*data'
    ]
    
    for pattern in store_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    # æŸ¥æ‰¾ backtest-store çš„æ•¸æ“šçµæ§‹
    print("\nğŸ” æª¢æŸ¥ backtest-store çš„æ•¸æ“šçµæ§‹:")
    
    # æŸ¥æ‰¾æ•¸æ“šè¼‰å…¥é‚è¼¯
    store_data_patterns = [
        'backtest_data.get("df_raw")',
        'backtest_data.get("results")',
        'backtest_data.get("daily_state")',
        'backtest_data.get("trades")',
        'backtest_data.get("weight_curve")'
    ]
    
    for pattern in store_data_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    # æŸ¥æ‰¾ results çš„çµæ§‹
    print("\nğŸ” æª¢æŸ¥ results çš„çµæ§‹:")
    
    results_patterns = [
        'results.get(',
        'results\[',
        'results\[strategy_name\]',
        'result.get('
    ]
    
    for pattern in results_patterns:
        count = content.count(pattern)
        print(f"   {pattern}: {count} æ¬¡")
    
    return True

def check_data_flow():
    """æª¢æŸ¥æ•¸æ“šæµ"""
    print("\nğŸ” æª¢æŸ¥æ•¸æ“šæµ...")
    print("=" * 60)
    
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("ğŸ“Š æ•¸æ“šæµæª¢æŸ¥:")
    
    # æª¢æŸ¥æ•¸æ“šä¾†æº
    data_sources = {
        "å…¨å±€å¥—ç”¨æ•¸æ“šæº": "backtest_data.get(\"df_raw\")",
        "å¿«å–æ•¸æ“šæº": "cache.get(\"df_raw\")",
        "ç­–ç•¥çµæœ": "results.get(strategy_name)",
        "æ—¥åº¦ç‹€æ…‹": "daily_state_std",
        "äº¤æ˜“è¨˜éŒ„": "trade_ledger_valve"
    }
    
    for source, pattern in data_sources.items():
        count = content.count(pattern)
        print(f"   {source}: {pattern} - {count} æ¬¡")
    
    # æª¢æŸ¥æ•¸æ“šè½‰æ›
    print("\nğŸ”„ æ•¸æ“šè½‰æ›æª¢æŸ¥:")
    
    data_transforms = {
        "df_from_pack": "df_from_pack(",
        "pack_df": "pack_df(",
        "pack_series": "pack_series(",
        "pd.to_numeric": "pd.to_numeric(",
        "pd.to_datetime": "pd.to_datetime("
    }
    
    for transform, pattern in data_transforms.items():
        count = content.count(pattern)
        print(f"   {transform}: {pattern} - {count} æ¬¡")
    
    return True

def check_risk_valve_consistency():
    """æª¢æŸ¥é¢¨éšªé–¥é–€çš„ä¸€è‡´æ€§"""
    print("\nğŸ” æª¢æŸ¥é¢¨éšªé–¥é–€ä¸€è‡´æ€§...")
    print("=" * 60)
    
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("ğŸ“Š é¢¨éšªé–¥é–€åƒæ•¸æª¢æŸ¥:")
    
    # æª¢æŸ¥å…¨å±€å¥—ç”¨çš„é¢¨éšªé–¥é–€åƒæ•¸
    global_valve_params = {
        "use_slopes": "use_slopes=True",
        "slope_method": 'slope_method="polyfit"',
        "atr_cmp": 'atr_cmp="gt"',
        "atr_win": "atr_win=20",
        "atr_ref_win": "atr_ref_win=60",
        "atr_ratio_mult": "atr_ratio_mult"
    }
    
    for param, pattern in global_valve_params.items():
        count = content.count(pattern)
        print(f"   å…¨å±€å¥—ç”¨ {param}: {pattern} - {count} æ¬¡")
    
    # æª¢æŸ¥å¢å¼·åˆ†æçš„é¢¨éšªé–¥é–€åƒæ•¸
    enhanced_valve_params = {
        "use_slopes": "use_slopes: True",
        "slope_method": 'slope_method: "polyfit"',
        "atr_cmp": 'atr_cmp: "gt"',
        "atr_win": "atr_win: 20",
        "atr_ref_win": "atr_ref_win: 60",
        "atr_ratio_mult": "atr_ratio_mult"
    }
    
    for param, pattern in enhanced_valve_params.items():
        count = content.count(pattern)
        print(f"   å¢å¼·åˆ†æ {param}: {pattern} - {count} æ¬¡")
    
    return True

def generate_data_structure_report():
    """ç”Ÿæˆæ•¸æ“šçµæ§‹å ±å‘Š"""
    print("\nğŸ“‹ ç”Ÿæˆæ•¸æ“šçµæ§‹å ±å‘Š...")
    
    report = {
        "check_date": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
        "check_type": "æ•¸æ“šçµæ§‹è©³ç´°æª¢æŸ¥",
        "files_checked": ["app_dash.py"],
        "data_sources": {},
        "data_flow": {},
        "risk_valve_consistency": {}
    }
    
    # æª¢æŸ¥æ•¸æ“šä¾†æº
    with open("app_dash.py", 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ•¸æ“šä¾†æºçµ±è¨ˆ
    data_sources = {
        "enhanced_trades_cache": content.count("enhanced-trades-cache"),
        "backtest_store": content.count("backtest-store"),
        "df_raw_from_cache": content.count('cache.get("df_raw")'),
        "df_raw_from_store": content.count('backtest_data.get("df_raw")'),
        "daily_state_from_cache": content.count('cache.get("daily_state")'),
        "daily_state_from_store": content.count('backtest_data.get("daily_state")')
    }
    
    report["data_sources"] = data_sources
    
    # æ•¸æ“šè½‰æ›çµ±è¨ˆ
    data_transforms = {
        "df_from_pack": content.count("df_from_pack("),
        "pack_df": content.count("pack_df("),
        "pack_series": content.count("pack_series(")
    }
    
    report["data_flow"] = data_transforms
    
    # é¢¨éšªé–¥é–€ä¸€è‡´æ€§
    valve_consistency = {
        "global_use_slopes": content.count("use_slopes=True"),
        "enhanced_use_slopes": content.count("use_slopes: True"),
        "global_slope_method": content.count('slope_method="polyfit"'),
        "enhanced_slope_method": content.count('slope_method: "polyfit"'),
        "global_atr_cmp": content.count('atr_cmp="gt"'),
        "enhanced_atr_cmp": content.count('atr_cmp: "gt"')
    }
    
    report["risk_valve_consistency"] = valve_consistency
    
    # ä¿å­˜å ±å‘Š
    report_file = f"data_structure_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ•¸æ“šçµæ§‹å ±å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æª¢æŸ¥å¯¦éš›æ•¸æ“šçµæ§‹...")
    print("=" * 60)
    
    # åŸ·è¡Œå„é …æª¢æŸ¥
    check_enhanced_trades_cache_structure()
    check_backtest_store_structure()
    check_data_flow()
    check_risk_valve_consistency()
    
    # ç”Ÿæˆå ±å‘Š
    report = generate_data_structure_report()
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ¯ æ•¸æ“šçµæ§‹æª¢æŸ¥å®Œæˆï¼")
    
    print("\nğŸ“Š æ•¸æ“šä¾†æºçµ±è¨ˆ:")
    for source, count in report["data_sources"].items():
        print(f"   {source}: {count} æ¬¡")
    
    print("\nğŸ”„ æ•¸æ“šè½‰æ›çµ±è¨ˆ:")
    for transform, count in report["data_flow"].items():
        print(f"   {transform}: {count} æ¬¡")
    
    print("\nğŸ”§ é¢¨éšªé–¥é–€ä¸€è‡´æ€§:")
    for param, count in report["risk_valve_consistency"].items():
        print(f"   {param}: {count} æ¬¡")
    
    print("\nğŸ’¡ é—œéµç™¼ç¾:")
    print("   â€¢ enhanced-trades-cache å­˜å„²ç­–ç•¥ç‰¹å®šçš„äº¤æ˜“æ•¸æ“š")
    print("   â€¢ backtest-store å­˜å„²å…¨å±€çš„å›æ¸¬çµæœå’Œæ•¸æ“š")
    print("   â€¢ å…©è€…é€šé backtest_data åƒæ•¸é€£æ¥")
    print("   â€¢ æ•¸æ“šæºè‡ªå‹•åˆ‡æ›æ©Ÿåˆ¶å·²å¯¦ç¾")
    
    print("\nâœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()
